{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial_2_deep_reinforcement_learning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"i-LKD78Z3kjS"},"source":["# Tutorial 2 Thinking with Deep Learning: Week 8 Part 2\n","# Deep Reinforcement Learning \n","\n","__Instructor:__ James Evans\n","\n","__Notebook Author:__ Bhargav Srinivasa Desikan\n","\n","__Teaching Assistants & Course Organizers:__ Likun Cao & Partha Kadambi \n","\n","\n","In this notebook we will continue from our basic reinforcement learning onto deep reinforcement learning."]},{"cell_type":"markdown","metadata":{"id":"FjrlpIrwSj1J"},"source":["# Actor Critic Method\n","\n","We will be drawing heavily on the [Keras documentation tutorial](https://keras.io/examples/rl/actor_critic_cartpole/) by [Apoorv Nandan](https://twitter.com/NandanApoorv). As noted in the last notebook, the Actor Critic approach merges policy gradient learning with value learning, but here using modern libraries that improve the automatic differentiation required for RL at scale."]},{"cell_type":"markdown","metadata":{"id":"qTY_er5bnce6"},"source":["## Introduction\n","\n","This script shows an implementation of Actor Critic method on the now *very familiar* CartPole-V0 environment.\n","\n","### Actor Critic Method\n","\n","As an agent takes actions and moves through an environment, it learns to map\n","the observed state of the environment to two possible outputs:\n","\n","1. Recommended action: A probability value for each action in the action space.\n","   The part of the agent responsible for this output is called the **actor**.\n","2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n","   future. The part of the agent responsible for this output is the **critic**.\n","\n","Agent and Critic learn to perform their tasks, such that the recommended actions\n","from the actor maximize the rewards.\n","\n","### CartPole-V0\n","\n","A pole is attached to a cart placed on a frictionless track. The agent has to apply\n","force to move the cart. It is rewarded for every time step the pole\n","remains upright. The agent, therefore, must learn to keep the pole from falling over.\n","\n","### References\n","\n","- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n","- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n"]},{"cell_type":"markdown","metadata":{"id":"XfVZH4tknce7"},"source":["## Setup\n"]},{"cell_type":"code","source":["!pip install pygame"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtWkTC5fWH6a","executionInfo":{"status":"ok","timestamp":1651689320938,"user_tz":300,"elapsed":8880,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"f7ec8ce1-03f3-4c93-8f71-a35aa9ba4336"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pygame\n","  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n","\u001b[K     |████████████████████████████████| 21.8 MB 81.2 MB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.1.2\n"]}]},{"cell_type":"code","metadata":{"id":"BLFT7CRence8","executionInfo":{"status":"ok","timestamp":1651689324514,"user_tz":300,"elapsed":3591,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Configuration parameters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","max_steps_per_episode = 10000\n","env = gym.make(\"CartPole-v0\")  # Create the environment\n","env.seed(seed)\n","eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Csf3ZNcnce9"},"source":["## Implement Actor Critic network\n","\n","This network learns two functions:\n","\n","1. Actor: This takes as input the state of our environment and returns a\n","probability value for each action in its action space.\n","2. Critic: This takes as input the state of our environment and returns\n","an estimate of total rewards in future.\n","\n","In our implementation, they share the initial layer.\n"]},{"cell_type":"code","metadata":{"id":"wvw4LoP1nce-","executionInfo":{"status":"ok","timestamp":1651689324769,"user_tz":300,"elapsed":270,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["num_inputs = 4\n","num_actions = 2\n","num_hidden = 128\n","\n","inputs = layers.Input(shape=(num_inputs,))\n","common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n","action = layers.Dense(num_actions, activation=\"softmax\")(common)\n","critic = layers.Dense(1)(common)\n","\n","model = keras.Model(inputs=inputs, outputs=[action, critic])\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52TvsRKJnce_"},"source":["## Train\n"]},{"cell_type":"code","metadata":{"id":"qTLL99Gxnce_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651689493120,"user_tz":300,"elapsed":168359,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"88b6282a-fedf-4065-b046-3023b81a31f1"},"source":["optimizer = keras.optimizers.Adam(learning_rate=0.01)\n","huber_loss = keras.losses.Huber()\n","action_probs_history = []\n","critic_value_history = []\n","rewards_history = []\n","running_reward = 0\n","episode_count = 0\n","\n","while True:  # Run until solved\n","    state = env.reset()\n","    episode_reward = 0\n","    with tf.GradientTape() as tape:\n","        for timestep in range(1, max_steps_per_episode):\n","            # env.render(); Adding this line would show the attempts\n","            # of the agent in a pop up window.\n","\n","            state = tf.convert_to_tensor(state)\n","            state = tf.expand_dims(state, 0)\n","\n","            # Predict action probabilities and estimated future rewards\n","            # from environment state\n","            action_probs, critic_value = model(state)\n","            critic_value_history.append(critic_value[0, 0])\n","\n","            # Sample action from action probability distribution\n","            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n","            action_probs_history.append(tf.math.log(action_probs[0, action]))\n","\n","            # Apply the sampled action in our environment\n","            state, reward, done, _ = env.step(action)\n","            rewards_history.append(reward)\n","            episode_reward += reward\n","\n","            if done:\n","                break\n","\n","        # Update running reward to check condition for solving\n","        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n","\n","        # Calculate expected value from rewards\n","        # - At each timestep what was the total reward received after that timestep\n","        # - Rewards in the past are discounted by multiplying them with gamma\n","        # - These are the labels for our critic\n","        returns = []\n","        discounted_sum = 0\n","        for r in rewards_history[::-1]:\n","            discounted_sum = r + gamma * discounted_sum\n","            returns.insert(0, discounted_sum)\n","\n","        # Normalize\n","        returns = np.array(returns)\n","        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n","        returns = returns.tolist()\n","\n","        # Calculating loss values to update our network\n","        history = zip(action_probs_history, critic_value_history, returns)\n","        actor_losses = []\n","        critic_losses = []\n","        for log_prob, value, ret in history:\n","            # At this point in history, the critic estimated that we would get a\n","            # total reward = `value` in the future. We took an action with log probability\n","            # of `log_prob` and ended up recieving a total reward = `ret`.\n","            # The actor must be updated so that it predicts an action that leads to\n","            # high rewards (compared to critic's estimate) with high probability.\n","            diff = ret - value\n","            actor_losses.append(-log_prob * diff)  # actor loss\n","\n","            # The critic must be updated so that it predicts a better estimate of\n","            # the future rewards.\n","            critic_losses.append(\n","                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n","            )\n","\n","        # Backpropagation\n","        loss_value = sum(actor_losses) + sum(critic_losses)\n","        grads = tape.gradient(loss_value, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        # Clear the loss and reward history\n","        action_probs_history.clear()\n","        critic_value_history.clear()\n","        rewards_history.clear()\n","\n","    # Log details\n","    episode_count += 1\n","    if episode_count % 10 == 0:\n","        template = \"running reward: {:.2f} at episode {}\"\n","        print(template.format(running_reward, episode_count))\n","\n","    if running_reward > 195:  # Condition to consider the task solved\n","        print(\"Solved at episode {}!\".format(episode_count))\n","        break\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["running reward: 10.35 at episode 10\n","running reward: 22.61 at episode 20\n","running reward: 49.69 at episode 30\n","running reward: 82.33 at episode 40\n","running reward: 97.73 at episode 50\n","running reward: 101.74 at episode 60\n","running reward: 110.24 at episode 70\n","running reward: 125.27 at episode 80\n","running reward: 147.37 at episode 90\n","running reward: 126.59 at episode 100\n","running reward: 92.89 at episode 110\n","running reward: 101.03 at episode 120\n","running reward: 139.94 at episode 130\n","running reward: 156.66 at episode 140\n","running reward: 142.84 at episode 150\n","running reward: 153.20 at episode 160\n","running reward: 171.98 at episode 170\n","running reward: 183.22 at episode 180\n","running reward: 189.95 at episode 190\n","running reward: 193.99 at episode 200\n","Solved at episode 204!\n"]}]},{"cell_type":"markdown","metadata":{"id":"oxHZ0kQXncfC"},"source":["## Visualizations\n","In early stages of training:\n","![Imgur](https://i.imgur.com/5gCs5kH.gif)\n","\n","In later stages of training:\n","![Imgur](https://i.imgur.com/5ziiZUD.gif)\n"]},{"cell_type":"markdown","metadata":{"id":"-gq5A07XTAFe"},"source":["# Deep Q-Learning\n","\n","The paper that started it all: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)\n","\n","We will be starting the [Keras tutorial](https://keras.io/examples/rl/deep_q_network_breakout/) by [Jacob Chapman](https://twitter.com/jacoblchapman) and [Mathias Lechner](https://twitter.com/MLech20)."]},{"cell_type":"markdown","metadata":{"id":"M9cggy4pnyPh"},"source":["## Deep Q-Learning for Atari Breakout\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sQJH_8hInyP1"},"source":["## Introduction\n","\n","This script shows an implementation of Deep Q-Learning on the\n","`BreakoutNoFrameskip-v4` environment.\n","\n","### Deep Q-Learning\n","\n","As detailed in the last homework, an agent takes actions and moves through an environment, learning to map\n","the observed state of the environment to an action. An agent will choose an action\n","in a given state based on a \"Q-value\", which is a weighted reward based on the\n","expected highest long-term reward. A Q-Learning Agent learns to perform its\n","task such that the recommended action maximizes the potential future rewards.\n","This method is considered an \"Off-Policy\" method,\n","meaning its Q values are updated assuming that the best action was chosen, even\n","if the best action was not chosen.\n","\n","### Atari Breakout\n","\n","In this environment, a board moves along the bottom of the screen returning a ball that\n","will destroy blocks at the top of the screen.\n","The aim of the game is to remove all blocks and breakout of the\n","level. The agent must learn to control the board by moving left and right, returning the\n","ball and removing all the blocks without the ball passing the board.\n","\n","### Note\n","\n","The Deepmind paper trained for \"a total of 50 million frames (that is, around 38 days of\n","game experience in total)\". However this script will give good results at around 10\n","million frames which are processed in less than 24 hours on a modern machine.\n","\n","### References\n","\n","- [Q-Learning](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)\n","- [Deep Q-Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)"]},{"cell_type":"markdown","metadata":{"id":"b9B-3ieAnyP4"},"source":["## Setup"]},{"cell_type":"code","source":["!git clone https://github.com/openai/baselines.git\n","import os\n","os.chdir('/content/baselines')\n","!pip install -e .\n","\n","#after running this installation, restart the kernel."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGgG_8tUWO6G","executionInfo":{"status":"ok","timestamp":1651689738115,"user_tz":300,"elapsed":5517,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"0c46d979-8d51-4ed9-fe26-304a96f5e751"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'baselines' already exists and is not an empty directory.\n","Obtaining file:///content/baselines\n","Requirement already satisfied: gym<0.16.0,>=0.15.4 in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (0.15.7)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (4.64.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.1.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.2.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (7.1.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (4.1.2.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.15.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n","Installing collected packages: baselines\n","  Attempting uninstall: baselines\n","    Found existing installation: baselines 0.1.6\n","    Can't uninstall 'baselines'. No files were found to uninstall.\n","  Running setup.py develop for baselines\n","Successfully installed baselines-0.1.6\n"]}]},{"cell_type":"code","source":["#before running this block, create a new folder /content/Roms\n","!pip install patool\n","import patoolib\n","patoolib.extract_archive(\"/content/Roms.rar\", outdir=\"/content/Roms\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"7ypX6eyZarET","executionInfo":{"status":"ok","timestamp":1651690440796,"user_tz":300,"elapsed":5357,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"180962df-11e7-4d2e-8290-f61842bdf495"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: patool in /usr/local/lib/python3.7/dist-packages (1.12)\n","patool: Extracting /content/Roms.rar ...\n","patool: running /usr/bin/unrar x -- /content/Roms.rar\n","patool:     with cwd='/content/Roms'\n","patool: ... /content/Roms.rar extracted to `/content/Roms'.\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/Roms'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!python -m atari_py.import_roms /content/Roms/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFZE3piCaCej","executionInfo":{"status":"ok","timestamp":1651690446972,"user_tz":300,"elapsed":1377,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"377a8a85-4d8c-4870-f5f2-d622bf10ea1f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n","copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n","copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n","copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n","copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n","copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n","copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n","copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n","copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n","copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n","copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n","copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n","copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n","copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n","copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n","copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n","copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n","copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n","copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n","copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n","copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n","copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n","copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n","copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n","copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n","copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n","copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n","copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n","copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n","copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n","copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n","copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n","copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n","copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n","copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n","copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n","copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n","copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n","copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n","copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n","copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n","copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n","copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n","copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n","copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n","copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n","copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n","copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n","copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n","copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n","copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n","copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n","copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n","copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n","copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n","copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n","copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n","copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n","copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n","copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n","copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n","copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n","copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n","copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n","copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n","copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n","copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n","copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n","copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n","copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n","copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n","copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n","copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n","copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n","copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n","copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n"]}]},{"cell_type":"code","metadata":{"id":"cSdDsKsRnyP5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651690453028,"user_tz":300,"elapsed":954,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"30e5ee15-ae21-4bfa-8005-544d4ef647cd"},"source":["from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",")  # Rate at which to reduce chance of random action being taken\n","batch_size = 32  # Size of batch taken from replay buffer\n","max_steps_per_episode = 10000\n","\n","# Use the Baseline Atari environment because of Deepmind helper functions\n","env = make_atari(\"BreakoutNoFrameskip-v4\")\n","# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n","env = wrap_deepmind(env, frame_stack=True, scale=True)\n","env.seed(seed)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]},{"output_type":"execute_result","data":{"text/plain":["[42, 742738649]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"gAFqV-VrnyP6"},"source":["## Implement the Deep Q-Network\n","\n","This network learns an approximation of the Q-table, which is a mapping between\n","the states and actions that an agent will take. For every state we'll have four\n","actions, that can be taken. The environment provides the state, and the action\n","is chosen by selecting the larger of the four Q-values predicted in the output layer."]},{"cell_type":"code","metadata":{"id":"EFZK1SssnyP6","executionInfo":{"status":"ok","timestamp":1651690494875,"user_tz":300,"elapsed":349,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["num_actions = 4\n","\n","\n","def create_q_model():\n","    # Network defined by the Deepmind paper\n","    inputs = layers.Input(shape=(84, 84, 4,))\n","\n","    # Convolutions on the frames on the screen\n","    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n","    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n","    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n","\n","    layer4 = layers.Flatten()(layer3)\n","\n","    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n","    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n","\n","    return keras.Model(inputs=inputs, outputs=action)\n","\n","\n","# The first model makes the predictions for Q-values which are used to\n","# make a action.\n","model = create_q_model()\n","# Build a target model for the prediction of future rewards.\n","# The weights of a target model get updated every 10000 steps thus when the\n","# loss between the Q-values is calculated the target Q-value is stable.\n","model_target = create_q_model()\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRk0L5E7nyP7"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"DwpKMQzQnyP9","colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"status":"error","timestamp":1651691041745,"user_tz":300,"elapsed":546209,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"50901a9d-a8e6-4eed-f927-af75523309d9"},"source":["# In the Deepmind paper they use RMSProp however then Adam optimizer\n","# improves training time\n","optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n","\n","# Experience replay buffers\n","action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 50000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 1000000.0\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 100000\n","# Train the model after 4 actions\n","update_after_actions = 4\n","# How often to update the target network\n","update_target_network = 10000\n","# Using huber loss for stability\n","loss_function = keras.losses.Huber()\n","\n","while True:  # Run until solved\n","    state = np.array(env.reset())\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        # env.render(); Adding this line would show the attempts\n","        # of the agent in a pop up window.\n","        frame_count += 1\n","\n","        # Use epsilon-greedy for exploration\n","        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n","            # Take random action\n","            action = np.random.choice(num_actions)\n","        else:\n","            # Predict action Q-values\n","            # From environment state\n","            state_tensor = tf.convert_to_tensor(state)\n","            state_tensor = tf.expand_dims(state_tensor, 0)\n","            action_probs = model(state_tensor, training=False)\n","            # Take best action\n","            action = tf.argmax(action_probs[0]).numpy()\n","\n","        # Decay probability of taking random action\n","        epsilon -= epsilon_interval / epsilon_greedy_frames\n","        epsilon = max(epsilon, epsilon_min)\n","\n","        # Apply the sampled action in our environment\n","        state_next, reward, done, _ = env.step(action)\n","        state_next = np.array(state_next)\n","\n","        episode_reward += reward\n","\n","        # Save actions and states in replay buffer\n","        action_history.append(action)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        done_history.append(done)\n","        rewards_history.append(reward)\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","\n","            # Get indices of samples for replay buffers\n","            indices = np.random.choice(range(len(done_history)), size=batch_size)\n","\n","            # Using list comprehension to sample from replay buffer\n","            state_sample = np.array([state_history[i] for i in indices])\n","            state_next_sample = np.array([state_next_history[i] for i in indices])\n","            rewards_sample = [rewards_history[i] for i in indices]\n","            action_sample = [action_history[i] for i in indices]\n","            done_sample = tf.convert_to_tensor(\n","                [float(done_history[i]) for i in indices]\n","            )\n","\n","            # Build the updated Q-values for the sampled future states\n","            # Use the target model for stability\n","            future_rewards = model_target.predict(state_next_sample)\n","            # Q value = reward + discount factor * expected future reward\n","            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n","                future_rewards, axis=1\n","            )\n","\n","            # If final frame set the last value to -1\n","            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n","\n","            # Create a mask so we only calculate loss on the updated Q-values\n","            masks = tf.one_hot(action_sample, num_actions)\n","\n","            with tf.GradientTape() as tape:\n","                # Train the model on the states and updated Q-values\n","                q_values = model(state_sample)\n","\n","                # Apply the masks to the Q-values to get the Q-value for action taken\n","                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n","                # Calculate loss between new Q-value and old Q-value\n","                loss = loss_function(updated_q_values, q_action)\n","\n","            # Backpropagation\n","            grads = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        if frame_count % update_target_network == 0:\n","            # update the the target network with new weights\n","            model_target.set_weights(model.get_weights())\n","            # Log details\n","            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n","            print(template.format(running_reward, episode_count, frame_count))\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    # Update running reward to check condition for solving\n","    episode_reward_history.append(episode_reward)\n","    if len(episode_reward_history) > 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    episode_count += 1\n","\n","    if running_reward > 40:  # Condition to consider the task solved\n","        print(\"Solved at episode {}!\".format(episode_count))\n","        break"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["running reward: 0.43 at episode 271, frame count 10000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-420436d30366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m# Train the model on the states and updated Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;31m# Apply the masks to the Q-values to get the Q-value for action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m    451\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 452\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mconvolution_op\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         name=self.__class__.__name__)\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1153\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1285\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1288\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2763\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2764\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2765\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"ZJu8dFH5nyP_"},"source":["## Visualizations\n","Before any training:\n","![Imgur](https://i.imgur.com/rRxXF4H.gif)\n","\n","In early stages of training:\n","![Imgur](https://i.imgur.com/X8ghdpL.gif)\n","\n","In later stages of training:\n","![Imgur](https://i.imgur.com/Z1K6qBQ.gif)"]},{"cell_type":"code","metadata":{"id":"SY6OVn94TB5S","executionInfo":{"status":"aborted","timestamp":1651691041637,"user_tz":300,"elapsed":362,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DP8A_aKlB9yP"},"source":["## Other resources\n","\n","We also highly recommend this tutorial from module 12 of the [Deep Learning](https://sites.wustl.edu/jeffheaton/t81-558/) course at  [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx), by  [Jeff Heaton](https://sites.wustl.edu/jeffheaton/):\n","\n","- [Keras reinforce Deep Q-Learning](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_03_keras_reinforce.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"2O-0Kt6lTHaq"},"source":["# Double Deep Q Networks\n","\n","Recall our discussion of Double Q-Learning in the last tutorial: using two Q tables to avoid the problematic feedback that result from making a bad decision with one.\n","\n","The paper: [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)\n","\n","We will be following the [PyTorch documentation tutorial](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) by [Yuansong Feng](https://github.com/YuansongFeng), [Suraj Subramanian](https://github.com/suraj813),  [Howard Wang](https://github.com/hw26) and [Steven Guo](https://github.com/GuoYuzhang).\n"]},{"cell_type":"markdown","metadata":{"id":"JQKWw6Kzn6YS"},"source":["\n","## Train a Mario-playing RL Agent\n","\n"]},{"cell_type":"code","source":["!pip install nes_py\n","!pip install gym_super_mario_bros"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M86eW60RdfDI","executionInfo":{"status":"ok","timestamp":1651691219598,"user_tz":300,"elapsed":13501,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"b03be5b4-46ec-4041-cb85-14aeb6d6740d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nes_py\n","  Downloading nes_py-8.1.8.tar.gz (76 kB)\n","\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 39.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 61 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 76 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.5.0)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (4.64.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes_py) (0.16.0)\n","Building wheels for collected packages: nes-py\n","  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=441068 sha256=174af7ef5d39ea0812e23fd864434b29078ff995723f165ee718d40d4d3e4ef9\n","  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n","Successfully built nes-py\n","Installing collected packages: nes-py\n","Successfully installed nes-py-8.1.8\n","Collecting gym_super_mario_bros\n","  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 14.3 MB/s \n","\u001b[?25hRequirement already satisfied: nes-py>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from gym_super_mario_bros) (8.1.8)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.21.6)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (4.64.0)\n","Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.5.0)\n","Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (0.17.3)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym_super_mario_bros) (0.16.0)\n","Installing collected packages: gym-super-mario-bros\n","Successfully installed gym-super-mario-bros-7.3.2\n"]}]},{"cell_type":"code","metadata":{"id":"y6Ag5S5jn6YT","executionInfo":{"status":"ok","timestamp":1651691223569,"user_tz":300,"elapsed":3996,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# !pip install gym-super-mario-bros==7.3.0\n","\n","import torch\n","from torch import nn\n","from torchvision import transforms as T\n","from PIL import Image\n","import numpy as np\n","from pathlib import Path\n","from collections import deque\n","import random, datetime, os, copy\n","\n","# Gym is an OpenAI toolkit for RL\n","import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","\n","# NES Emulator for OpenAI Gym\n","from nes_py.wrappers import JoypadSpace\n","\n","# Super Mario environment for OpenAI Gym\n","import gym_super_mario_bros"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NozIm41in6YU"},"source":["RL Definitions\n","\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n","\n","**Environment** The world that an agent interacts with and learns from.\n","\n","**Action** $a$ : How the Agent responds to the Environment. The\n","set of all possible Actions is called *action-space*.\n","\n","**State** $s$ : The current characteristic of the Environment. The\n","set of all possible States the Environment can be in is called\n","*state-space*.\n","\n","**Reward** $r$ : Reward is the key feedback from Environment to\n","Agent. It is what drives the Agent to learn and to change its future\n","action. An aggregation of rewards over multiple time steps is called\n","**Return**.\n","\n","**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected\n","return if you start in state $s$, take an arbitrary action\n","$a$, and then for each future time step take the action that\n","maximizes returns. $Q$ can be said to stand for the “quality” of\n","the action in a state. We try to approximate this function.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aB4G5BS2n6YV"},"source":["### Environment\n","\n","### Initialize Environment\n","------------------------\n","\n","In Mario, the environment consists of tubes, mushrooms and other\n","components.\n","\n","When Mario makes an action, the environment responds with the changed\n","(next) state, reward and other info.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"fRhp1kmNn6YV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651691224455,"user_tz":300,"elapsed":898,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"aa0b5488-0c21-4f1e-8a93-0430397e79ea"},"source":["# Initialize Super Mario environment\n","env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n","\n","# Limit the action-space to\n","#   0. walk right\n","#   1. jump right\n","env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n","\n","env.reset()\n","next_state, reward, done, info = env.step(action=0)\n","print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(240, 256, 3),\n"," 0,\n"," False,\n"," {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"]}]},{"cell_type":"markdown","metadata":{"id":"sqpQv8TRn6YW"},"source":["### Preprocess Environment\n","\n","Environment data is returned to the agent in ``next_state``. As you saw\n","above, each state is represented by a ``[3, 240, 256]`` size array.\n","Often that is more information than our agent needs; for instance,\n","Mario’s actions do not depend on the color of the pipes or the sky!\n","\n","We use **Wrappers** to preprocess environment data before sending it to\n","the agent.\n","\n","``GrayScaleObservation`` is a common wrapper to transform an RGB image\n","to grayscale; doing so reduces the size of the state representation\n","without losing useful information. Now the size of each state:\n","``[1, 240, 256]``\n","\n","``ResizeObservation`` downsamples each observation into a square image.\n","New size: ``[1, 84, 84]``\n","\n","``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and\n","implements the ``step()`` function. Because consecutive frames don’t\n","vary much, we can skip n-intermediate frames without losing much\n","information. The n-th frame aggregates rewards accumulated over each\n","skipped frame.\n","\n","``FrameStack`` is a wrapper that allows us to squash consecutive frames\n","of the environment into a single observation point to feed to our\n","learning model. This way, we can identify if Mario was landing or\n","jumping based on the direction of his movement in the previous several\n","frames.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"mZ5mf--wn6YX","executionInfo":{"status":"ok","timestamp":1651691224456,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, skip):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"Repeat action, and sum reward\"\"\"\n","        total_reward = 0.0\n","        done = False\n","        for i in range(self._skip):\n","            # Accumulate reward and repeat the same action\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def permute_orientation(self, observation):\n","        # permute [H, W, C] array to [C, H, W] tensor\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        return observation\n","\n","    def observation(self, observation):\n","        observation = self.permute_orientation(observation)\n","        transform = T.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int):\n","            self.shape = (shape, shape)\n","        else:\n","            self.shape = tuple(shape)\n","\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        transforms = T.Compose(\n","            [T.Resize(self.shape), T.Normalize(0, 255)]\n","        )\n","        observation = transforms(observation).squeeze(0)\n","        return observation\n","\n","\n","# Apply Wrappers to environment\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","env = FrameStack(env, num_stack=4)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-Is5g5On6YY"},"source":["After applying the above wrappers to the environment, the final wrapped\n","state consists of 4 gray-scaled consecutive frames stacked together, as\n","shown above in the image on the left. Each time Mario makes an action,\n","the environment responds with a state of this structure. The structure\n","is represented by a 3-D array of size ``[4, 84, 84]``.\n","\n",".. figure:: /_static/img/mario_env.png\n","   :alt: picture\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5iP8QhJMn6Ya"},"source":["### Agent\n","\n","\n","We create a class ``Mario`` to represent our agent in the game. Mario\n","should be able to:\n","\n","-  **Act** according to the optimal action policy based on the current\n","   state (of the environment).\n","\n","-  **Remember** experiences. Experience = (current state, current\n","   action, reward, next state). Mario *caches* and later *recalls* his\n","   experiences to update his action policy.\n","\n","-  **Learn** a better action policy over time\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"85Z1eshpn6Yd","executionInfo":{"status":"ok","timestamp":1651691224457,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario:\n","    def __init__():\n","        pass\n","\n","    def act(self, state):\n","        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n","        pass\n","\n","    def cache(self, experience):\n","        \"\"\"Add the experience to memory\"\"\"\n","        pass\n","\n","    def recall(self):\n","        \"\"\"Sample experiences from memory\"\"\"\n","        pass\n","\n","    def learn(self):\n","        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n","        pass"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1qhb7ZYkn6Yd"},"source":["In the following sections, we will populate Mario’s parameters and\n","define his functions.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_OfdEnMAn6Ye"},"source":["### Act\n","\n","For any given state, an agent can choose to do the most optimal action\n","(**exploit**) or a random action (**explore**).\n","\n","Mario randomly explores with a chance of ``self.exploration_rate``; when\n","he chooses to exploit, he relies on ``MarioNet`` (implemented in\n","``Learn`` section) to provide the optimal action.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"i9sIIvwVn6Ye","executionInfo":{"status":"ok","timestamp":1651691224457,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario:\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.save_dir = save_dir\n","\n","        self.use_cuda = torch.cuda.is_available()\n","\n","        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n","        self.net = MarioNet(self.state_dim, self.action_dim).float()\n","        if self.use_cuda:\n","            self.net = self.net.to(device=\"cuda\")\n","\n","        self.exploration_rate = 1\n","        self.exploration_rate_decay = 0.99999975\n","        self.exploration_rate_min = 0.1\n","        self.curr_step = 0\n","\n","        self.save_every = 5e5  # no. of experiences between saving Mario Net\n","\n","    def act(self, state):\n","        \"\"\"\n","    Given a state, choose an epsilon-greedy action and update value of step.\n","\n","    Inputs:\n","    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n","    Outputs:\n","    action_idx (int): An integer representing which action Mario will perform\n","    \"\"\"\n","        # EXPLORE\n","        if np.random.rand() < self.exploration_rate:\n","            action_idx = np.random.randint(self.action_dim)\n","\n","        # EXPLOIT\n","        else:\n","            state = state.__array__()\n","            if self.use_cuda:\n","                state = torch.tensor(state).cuda()\n","            else:\n","                state = torch.tensor(state)\n","            state = state.unsqueeze(0)\n","            action_values = self.net(state, model=\"online\")\n","            action_idx = torch.argmax(action_values, axis=1).item()\n","\n","        # decrease exploration_rate\n","        self.exploration_rate *= self.exploration_rate_decay\n","        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","\n","        # increment step\n","        self.curr_step += 1\n","        return action_idx"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChfeTIVun6Yf"},"source":["### Cache and Recall\n","\n","\n","These two functions serve as Mario’s “memory”.\n","\n","``cache()``: Each time Mario performs an action, he stores the\n","``experience`` in his memory. His experience includes the current\n","*state*, *action* performed, *reward* from the action, the *next state*,\n","and whether the game is *done*.\n","\n","``recall()``: Mario randomly samples a batch of experiences from his\n","memory, and uses that to learn the game.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"KCohcVH5n6Yf","executionInfo":{"status":"ok","timestamp":1651691224458,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario(Mario):  # subclassing for continuity\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.memory = deque(maxlen=100000)\n","        self.batch_size = 32\n","\n","    def cache(self, state, next_state, action, reward, done):\n","        \"\"\"\n","        Store the experience to self.memory (replay buffer)\n","\n","        Inputs:\n","        state (LazyFrame),\n","        next_state (LazyFrame),\n","        action (int),\n","        reward (float),\n","        done(bool))\n","        \"\"\"\n","        state = state.__array__()\n","        next_state = next_state.__array__()\n","\n","        if self.use_cuda:\n","            state = torch.tensor(state).cuda()\n","            next_state = torch.tensor(next_state).cuda()\n","            action = torch.tensor([action]).cuda()\n","            reward = torch.tensor([reward]).cuda()\n","            done = torch.tensor([done]).cuda()\n","        else:\n","            state = torch.tensor(state)\n","            next_state = torch.tensor(next_state)\n","            action = torch.tensor([action])\n","            reward = torch.tensor([reward])\n","            done = torch.tensor([done])\n","\n","        self.memory.append((state, next_state, action, reward, done,))\n","\n","    def recall(self):\n","        \"\"\"\n","        Retrieve a batch of experiences from memory\n","        \"\"\"\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJiwrVVCn6Yg"},"source":["### Learn\n","\n","Mario uses the `DDQN algorithm <https://arxiv.org/pdf/1509.06461>`__\n","under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n","$Q_{target}$ - that independently approximate the optimal\n","action-value function.\n","\n","In our implementation, we share feature generator ``features`` across\n","$Q_{online}$ and $Q_{target}$, but maintain separate FC\n","classifiers for each. $\\theta_{target}$ (the parameters of\n","$Q_{target}$) is frozen to prevent updation by backprop. Instead,\n","it is periodically synced with $\\theta_{online}$ (more on this\n","later).\n","\n"]},{"cell_type":"code","metadata":{"id":"xDOYRDZ9n6Yg","executionInfo":{"status":"ok","timestamp":1651691224458,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class MarioNet(nn.Module):\n","    \"\"\"mini cnn structure\n","  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n","  \"\"\"\n","\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        c, h, w = input_dim\n","\n","        if h != 84:\n","            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n","        if w != 84:\n","            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n","\n","        self.online = nn.Sequential(\n","            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, output_dim),\n","        )\n","\n","        self.target = copy.deepcopy(self.online)\n","\n","        # Q_target parameters are frozen.\n","        for p in self.target.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, input, model):\n","        if model == \"online\":\n","            return self.online(input)\n","        elif model == \"target\":\n","            return self.target(input)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A6RSBDmpn6Yh"},"source":["#### TD Estimate & TD Target\n","\n","Two values are involved in learning:\n","\n","**TD Estimate** - the predicted optimal $Q^*$ for a given state\n","$s$\n","\n","\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n","\n","**TD Target** - aggregation of current reward and the estimated\n","$Q^*$ in the next state $s'$\n","\n","\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n","\n","\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n","\n","Because we don’t know what next action $a'$ will be, we use the\n","action $a'$ maximizes $Q_{online}$ in the next state\n","$s'$.\n","\n","Notice we use the\n","`@torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__\n","decorator on ``td_target()`` to disable gradient calculations here\n","(because we don’t need to backpropagate on $\\theta_{target}$).\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"NSuIzDu8n6Yh","executionInfo":{"status":"ok","timestamp":1651691224459,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.gamma = 0.9\n","\n","    def td_estimate(self, state, action):\n","        current_Q = self.net(state, model=\"online\")[\n","            np.arange(0, self.batch_size), action\n","        ]  # Q_online(s,a)\n","        return current_Q\n","\n","    @torch.no_grad()\n","    def td_target(self, reward, next_state, done):\n","        next_state_Q = self.net(next_state, model=\"online\")\n","        best_action = torch.argmax(next_state_Q, axis=1)\n","        next_Q = self.net(next_state, model=\"target\")[\n","            np.arange(0, self.batch_size), best_action\n","        ]\n","        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3FiI_oun6Yh"},"source":["#### Updating the model\n","\n","As Mario samples inputs from his replay buffer, we compute $TD_t$\n","and $TD_e$ and backpropagate this loss down $Q_{online}$ to\n","update its parameters $\\theta_{online}$ ($\\alpha$ is the\n","learning rate ``lr`` passed to the ``optimizer``)\n","\n","\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n","\n","$\\theta_{target}$ does not update through backpropagation.\n","Instead, we periodically copy $\\theta_{online}$ to\n","$\\theta_{target}$\n","\n","\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"7nQ948bAn6Yh","executionInfo":{"status":"ok","timestamp":1651691224459,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n","        self.loss_fn = torch.nn.SmoothL1Loss()\n","\n","    def update_Q_online(self, td_estimate, td_target):\n","        loss = self.loss_fn(td_estimate, td_target)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss.item()\n","\n","    def sync_Q_target(self):\n","        self.net.target.load_state_dict(self.net.online.state_dict())"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ft8sXJIdn6Yi"},"source":["Save checkpoint"]},{"cell_type":"code","metadata":{"id":"41mGaS3On6Yi","executionInfo":{"status":"ok","timestamp":1651691224460,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario(Mario):\n","    def save(self):\n","        save_path = (\n","            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n","        )\n","        torch.save(\n","            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n","            save_path,\n","        )\n","        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNjNAVnyn6Yi"},"source":["Putting it all together"]},{"cell_type":"code","metadata":{"id":"YGBQHiaon6Yi","executionInfo":{"status":"ok","timestamp":1651691224460,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.burnin = 1e4  # min. experiences before training\n","        self.learn_every = 3  # no. of experiences between updates to Q_online\n","        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n","\n","    def learn(self):\n","        if self.curr_step % self.sync_every == 0:\n","            self.sync_Q_target()\n","\n","        if self.curr_step % self.save_every == 0:\n","            self.save()\n","\n","        if self.curr_step < self.burnin:\n","            return None, None\n","\n","        if self.curr_step % self.learn_every != 0:\n","            return None, None\n","\n","        # Sample from memory\n","        state, next_state, action, reward, done = self.recall()\n","\n","        # Get TD Estimate\n","        td_est = self.td_estimate(state, action)\n","\n","        # Get TD Target\n","        td_tgt = self.td_target(reward, next_state, done)\n","\n","        # Backpropagate loss through Q_online\n","        loss = self.update_Q_online(td_est, td_tgt)\n","\n","        return (td_est.mean().item(), loss)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1cGURWEn6Yi"},"source":["### Logging\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pNNdXB8Un6Yj","executionInfo":{"status":"ok","timestamp":1651691224461,"user_tz":300,"elapsed":22,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import numpy as np\n","import time, datetime\n","import matplotlib.pyplot as plt\n","\n","\n","class MetricLogger:\n","    def __init__(self, save_dir):\n","        self.save_log = save_dir / \"log\"\n","        with open(self.save_log, \"w\") as f:\n","            f.write(\n","                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n","                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n","                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n","            )\n","        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n","        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n","        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n","        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n","\n","        # History metrics\n","        self.ep_rewards = []\n","        self.ep_lengths = []\n","        self.ep_avg_losses = []\n","        self.ep_avg_qs = []\n","\n","        # Moving averages, added for every call to record()\n","        self.moving_avg_ep_rewards = []\n","        self.moving_avg_ep_lengths = []\n","        self.moving_avg_ep_avg_losses = []\n","        self.moving_avg_ep_avg_qs = []\n","\n","        # Current episode metric\n","        self.init_episode()\n","\n","        # Timing\n","        self.record_time = time.time()\n","\n","    def log_step(self, reward, loss, q):\n","        self.curr_ep_reward += reward\n","        self.curr_ep_length += 1\n","        if loss:\n","            self.curr_ep_loss += loss\n","            self.curr_ep_q += q\n","            self.curr_ep_loss_length += 1\n","\n","    def log_episode(self):\n","        \"Mark end of episode\"\n","        self.ep_rewards.append(self.curr_ep_reward)\n","        self.ep_lengths.append(self.curr_ep_length)\n","        if self.curr_ep_loss_length == 0:\n","            ep_avg_loss = 0\n","            ep_avg_q = 0\n","        else:\n","            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n","            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n","        self.ep_avg_losses.append(ep_avg_loss)\n","        self.ep_avg_qs.append(ep_avg_q)\n","\n","        self.init_episode()\n","\n","    def init_episode(self):\n","        self.curr_ep_reward = 0.0\n","        self.curr_ep_length = 0\n","        self.curr_ep_loss = 0.0\n","        self.curr_ep_q = 0.0\n","        self.curr_ep_loss_length = 0\n","\n","    def record(self, episode, epsilon, step):\n","        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n","        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n","        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n","        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n","        self.moving_avg_ep_rewards.append(mean_ep_reward)\n","        self.moving_avg_ep_lengths.append(mean_ep_length)\n","        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n","        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n","\n","        last_record_time = self.record_time\n","        self.record_time = time.time()\n","        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n","\n","        print(\n","            f\"Episode {episode} - \"\n","            f\"Step {step} - \"\n","            f\"Epsilon {epsilon} - \"\n","            f\"Mean Reward {mean_ep_reward} - \"\n","            f\"Mean Length {mean_ep_length} - \"\n","            f\"Mean Loss {mean_ep_loss} - \"\n","            f\"Mean Q Value {mean_ep_q} - \"\n","            f\"Time Delta {time_since_last_record} - \"\n","            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n","        )\n","\n","        with open(self.save_log, \"a\") as f:\n","            f.write(\n","                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n","                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n","                f\"{time_since_last_record:15.3f}\"\n","                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n","            )\n","\n","        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n","            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n","            plt.savefig(getattr(self, f\"{metric}_plot\"))\n","            plt.clf()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"577NnIc9n6Yj"},"source":["#### Let’s play!\n","\n","In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\n","his world, we suggest running the loop for at least 40,000 episodes!\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ZMEqo6frn6Yj","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1651691260246,"user_tz":300,"elapsed":35806,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"a0fb5949-102a-4cbb-be70-54a76913ad5d"},"source":["use_cuda = torch.cuda.is_available()\n","print(f\"Using CUDA: {use_cuda}\")\n","print()\n","\n","save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n","save_dir.mkdir(parents=True)\n","\n","mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n","\n","logger = MetricLogger(save_dir)\n","\n","episodes = 10\n","for e in range(episodes):\n","\n","    state = env.reset()\n","\n","    # Play the game!\n","    while True:\n","\n","        # Run agent on the state\n","        action = mario.act(state)\n","\n","        # Agent performs action\n","        next_state, reward, done, info = env.step(action)\n","\n","        # Remember\n","        mario.cache(state, next_state, action, reward, done)\n","\n","        # Learn\n","        q, loss = mario.learn()\n","\n","        # Logging\n","        logger.log_step(reward, loss, q)\n","\n","        # Update state\n","        state = next_state\n","\n","        # Check if end of game\n","        if done or info[\"flag_get\"]:\n","            break\n","\n","    logger.log_episode()\n","\n","    if e % 20 == 0:\n","        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA: True\n","\n","Episode 0 - Step 122 - Epsilon 0.9999695004613035 - Mean Reward 589.0 - Mean Length 122.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.526 - Time 2022-05-04T19:07:17\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"_27yzfpkn6Yk"},"source":["#### Conclusion\n","\n","In this setion, we saw how we can use PyTorch to train a game-playing RL AI. You can use the same methods to train an AI to play any of the games at the [OpenAI gym](https://gym.openai.com/). If you liked these examples, follow the following\n","[github](https://github.com/yuansongFeng/MadMario/>)!\n","\n"]},{"cell_type":"code","metadata":{"id":"InkFqtLtTKGJ","executionInfo":{"status":"ok","timestamp":1651691260247,"user_tz":300,"elapsed":27,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmuAfEmkSspm"},"source":["# Deep Deterministic Policy Gradient\n","\n","We will be drawing heavily upon the [Keras DDPG tutorial](https://keras.io/examples/rl/ddpg_pendulum/) by [amifunny](https://github.com/amifunny)."]},{"cell_type":"markdown","metadata":{"id":"7Pgpgte1PECV"},"source":["## Introduction\n","\n","**Deep Deterministic Policy Gradient (DDPG)** is a model-free off-policy algorithm for\n","learning continous actions.\n","\n","It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network).\n","It uses Experience Replay and slow-learning target networks from DQN, and it is based on\n","DPG,\n","which can operate over continuous action spaces.\n","\n","This tutorial closely follow this paper -\n","[Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971.pdf)\n","\n","## Problem\n","\n","We are trying to solve the classic **Inverted Pendulum** control problem.\n","In this setting, we can take only two actions: swing left or swing right.\n","\n","What make this problem challenging for Q-Learning Algorithms is that actions\n","are **continuous** instead of being **discrete**. That is, instead of using two\n","discrete actions like `-1` or `+1`, we have to select from infinite actions\n","ranging from `-2` to `+2`.\n","\n","## Quick theory\n","\n","Just like the Actor-Critic method, we have two networks:\n","\n","1. Actor - It proposes an action given a state.\n","2. Critic - It predicts if the action is good (positive value) or bad (negative value)\n","given a state and an action.\n","\n","DDPG uses two more techniques not present in the original DQN:\n","\n","**First, it uses two Target networks.**\n","\n","**Why?** Because it add stability to training. In short, we are learning from estimated\n","targets and Target networks are updated slowly, keeping our estimated targets\n","stable.\n","\n","Conceptually, this is like saying, \"I have an idea of how to play this well,\n","I'm going to try it out for a bit until I find something better\",\n","as opposed to saying \"I'm going to re-learn how to play this entire game after every\n","move\".\n","See this [StackOverflow answer](https://stackoverflow.com/a/54238556/13475679).\n","\n","**Second, it uses Experience Replay.**\n","\n","We store list of tuples `(state, action, reward, next_state)`, and instead of\n","learning only from recent experience, we learn from sampling all of our experience\n","accumulated so far.\n","\n","Now, let's see how is it implemented."]},{"cell_type":"code","metadata":{"id":"DCjZNhRXPECW","executionInfo":{"status":"ok","timestamp":1651691262381,"user_tz":300,"elapsed":2150,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import gym\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwGs5OZVPECX"},"source":["We use [OpenAIGym](http://gym.openai.com/docs) to create the environment.\n","We will use the `upper_bound` parameter to scale our actions later."]},{"cell_type":"code","metadata":{"id":"byu6RZE2PECX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651691262382,"user_tz":300,"elapsed":18,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"5b3f35b2-66fa-4999-917f-635eb183f3ec"},"source":["problem = \"Pendulum-v0\"\n","#if you have a problem here, change to \"Pendulum-v1\"\n","env = gym.make(problem)\n","\n","num_states = env.observation_space.shape[0]\n","print(\"Size of State Space ->  {}\".format(num_states))\n","num_actions = env.action_space.shape[0]\n","print(\"Size of Action Space ->  {}\".format(num_actions))\n","\n","upper_bound = env.action_space.high[0]\n","lower_bound = env.action_space.low[0]\n","\n","print(\"Max Value of Action ->  {}\".format(upper_bound))\n","print(\"Min Value of Action ->  {}\".format(lower_bound))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of State Space ->  3\n","Size of Action Space ->  1\n","Max Value of Action ->  2.0\n","Min Value of Action ->  -2.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"dxN_SyvdPECY"},"source":["To implement better exploration by the Actor network, we use noisy perturbations,\n","specifically\n","an **Ornstein-Uhlenbeck process** for generating noise, a stationary Gauss–Markov process, which means that it is a Gaussian process, a Markov process, and is temporally homogeneous (it is the only process that satisfies these properties). It tends to drift towards its mean function (a mean-reverting process).\n","It samples noise from a correlated normal distribution."]},{"cell_type":"code","metadata":{"id":"6tiux3A0PECY","executionInfo":{"status":"ok","timestamp":1651691262383,"user_tz":300,"elapsed":12,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class OUActionNoise:\n","    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n","        self.theta = theta\n","        self.mean = mean\n","        self.std_dev = std_deviation\n","        self.dt = dt\n","        self.x_initial = x_initial\n","        self.reset()\n","\n","    def __call__(self):\n","        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n","        x = (\n","            self.x_prev\n","            + self.theta * (self.mean - self.x_prev) * self.dt\n","            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n","        )\n","        # Store x into x_prev\n","        # Makes next noise dependent on current one\n","        self.x_prev = x\n","        return x\n","\n","    def reset(self):\n","        if self.x_initial is not None:\n","            self.x_prev = self.x_initial\n","        else:\n","            self.x_prev = np.zeros_like(self.mean)\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VM5wHPJ2PECZ"},"source":["The `Buffer` class implements Experience Replay.\n","\n","---\n","![Algorithm](https://i.imgur.com/mS6iGyJ.jpg)\n","---\n","\n","\n","**Critic loss** - Mean Squared Error of `y - Q(s, a)`\n","where `y` is the expected return as seen by the Target network,\n","and `Q(s, a)` is action value predicted by the Critic network. `y` is a moving target\n","that the critic model tries to achieve; we make this target\n","stable by updating the Target model slowly.\n","\n","**Actor loss** - This is computed using the mean of the value given by the Critic network\n","for the actions taken by the Actor network. We seek to maximize this quantity.\n","\n","Hence we update the Actor network so that it produces actions that get\n","the maximum predicted value as seen by the Critic, for a given state."]},{"cell_type":"code","metadata":{"id":"EoTgQShZPECZ","executionInfo":{"status":"ok","timestamp":1651691262384,"user_tz":300,"elapsed":12,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["\n","class Buffer:\n","    def __init__(self, buffer_capacity=100000, batch_size=64):\n","        # Number of \"experiences\" to store at max\n","        self.buffer_capacity = buffer_capacity\n","        # Num of tuples to train on.\n","        self.batch_size = batch_size\n","\n","        # Its tells us num of times record() was called.\n","        self.buffer_counter = 0\n","\n","        # Instead of list of tuples as the exp.replay concept go\n","        # We use different np.arrays for each tuple element\n","        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n","        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n","        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n","        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n","\n","    # Takes (s,a,r,s') obervation tuple as input\n","    def record(self, obs_tuple):\n","        # Set index to zero if buffer_capacity is exceeded,\n","        # replacing old records\n","        index = self.buffer_counter % self.buffer_capacity\n","\n","        self.state_buffer[index] = obs_tuple[0]\n","        self.action_buffer[index] = obs_tuple[1]\n","        self.reward_buffer[index] = obs_tuple[2]\n","        self.next_state_buffer[index] = obs_tuple[3]\n","\n","        self.buffer_counter += 1\n","\n","    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n","    # TensorFlow to build a static graph out of the logic and computations in our function.\n","    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n","    @tf.function\n","    def update(\n","        self, state_batch, action_batch, reward_batch, next_state_batch,\n","    ):\n","        # Training and updating Actor & Critic networks.\n","        # See Pseudo Code.\n","        with tf.GradientTape() as tape:\n","            target_actions = target_actor(next_state_batch, training=True)\n","            y = reward_batch + gamma * target_critic(\n","                [next_state_batch, target_actions], training=True\n","            )\n","            critic_value = critic_model([state_batch, action_batch], training=True)\n","            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n","\n","        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n","        critic_optimizer.apply_gradients(\n","            zip(critic_grad, critic_model.trainable_variables)\n","        )\n","\n","        with tf.GradientTape() as tape:\n","            actions = actor_model(state_batch, training=True)\n","            critic_value = critic_model([state_batch, actions], training=True)\n","            # Used `-value` as we want to maximize the value given\n","            # by the critic for our actions\n","            actor_loss = -tf.math.reduce_mean(critic_value)\n","\n","        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n","        actor_optimizer.apply_gradients(\n","            zip(actor_grad, actor_model.trainable_variables)\n","        )\n","\n","    # We compute the loss and update parameters\n","    def learn(self):\n","        # Get sampling range\n","        record_range = min(self.buffer_counter, self.buffer_capacity)\n","        # Randomly sample indices\n","        batch_indices = np.random.choice(record_range, self.batch_size)\n","\n","        # Convert to tensors\n","        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n","        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n","        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n","        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n","        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n","\n","        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n","\n","\n","# This update target parameters slowly\n","# Based on rate `tau`, which is much less than one.\n","@tf.function\n","def update_target(target_weights, weights, tau):\n","    for (a, b) in zip(target_weights, weights):\n","        a.assign(b * tau + a * (1 - tau))\n"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QrkTauicPECb"},"source":["## Actor and Critic Networks\n","\n","Here we define the Actor and Critic networks. These are basic Dense models\n","with `ReLU` activation.\n","\n","Note: We need the initialization for last layer of the Actor to be between\n","`-0.003` and `0.003` as this prevents us from getting `1` or `-1` output values in\n","the initial stages, which would squash our gradients to zero,\n","as we use the `tanh` activation."]},{"cell_type":"code","metadata":{"id":"EXpLjRFvPECb","executionInfo":{"status":"ok","timestamp":1651691262384,"user_tz":300,"elapsed":12,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["\n","def get_actor():\n","    # Initialize weights between -3e-3 and 3-e3\n","    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n","\n","    inputs = layers.Input(shape=(num_states,))\n","    out = layers.Dense(256, activation=\"relu\")(inputs)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n","\n","    # Our upper bound is 2.0 for Pendulum.\n","    outputs = outputs * upper_bound\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n","\n","def get_critic():\n","    # State as input\n","    state_input = layers.Input(shape=(num_states))\n","    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n","    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n","\n","    # Action as input\n","    action_input = layers.Input(shape=(num_actions))\n","    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n","\n","    # Both are passed through seperate layer before concatenating\n","    concat = layers.Concatenate()([state_out, action_out])\n","\n","    out = layers.Dense(256, activation=\"relu\")(concat)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1)(out)\n","\n","    # Outputs single value for give state-action\n","    model = tf.keras.Model([state_input, action_input], outputs)\n","\n","    return model\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Atd8cLoPECc"},"source":["`policy()` returns an action sampled from our Actor network plus some noise for\n","exploration."]},{"cell_type":"code","metadata":{"id":"ve2rmBuGPECc","executionInfo":{"status":"ok","timestamp":1651691262385,"user_tz":300,"elapsed":12,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["\n","def policy(state, noise_object):\n","    sampled_actions = tf.squeeze(actor_model(state))\n","    noise = noise_object()\n","    # Adding noise to action\n","    sampled_actions = sampled_actions.numpy() + noise\n","\n","    # We make sure action is within bounds\n","    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n","\n","    return [np.squeeze(legal_action)]\n"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHPfoYkFPECc"},"source":["## Training hyperparameters"]},{"cell_type":"code","metadata":{"id":"d5I5GwXiPECc","executionInfo":{"status":"ok","timestamp":1651691266754,"user_tz":300,"elapsed":4380,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["std_dev = 0.2\n","ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n","\n","actor_model = get_actor()\n","critic_model = get_critic()\n","\n","target_actor = get_actor()\n","target_critic = get_critic()\n","\n","# Making the weights equal initially\n","target_actor.set_weights(actor_model.get_weights())\n","target_critic.set_weights(critic_model.get_weights())\n","\n","# Learning rate for actor-critic models\n","critic_lr = 0.002\n","actor_lr = 0.001\n","\n","critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n","actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n","\n","total_episodes = 100\n","# Discount factor for future rewards\n","gamma = 0.99\n","# Used to update target networks\n","tau = 0.005\n","\n","buffer = Buffer(50000, 64)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjHHCGdEPECd"},"source":["Now we implement our main training loop, and iterate over episodes.\n","We sample actions using `policy()` and train with `learn()` at each time step,\n","along with updating the Target networks at a rate `tau`."]},{"cell_type":"code","metadata":{"id":"aA-YAjq5PECd","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1651691431202,"user_tz":300,"elapsed":164462,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"d6524b51-e1ba-4052-96df-5fe31c672a4d"},"source":["# To store reward history of each episode\n","ep_reward_list = []\n","# To store average reward history of last few episodes\n","avg_reward_list = []\n","\n","# Takes about 4 min to train\n","for ep in range(total_episodes):\n","\n","    prev_state = env.reset()\n","    episodic_reward = 0\n","\n","    while True:\n","        # Uncomment this to see the Actor in action\n","        # But not in a python notebook.\n","        # env.render()\n","\n","        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n","\n","        action = policy(tf_prev_state, ou_noise)\n","        # Recieve state and reward from environment.\n","        state, reward, done, info = env.step(action)\n","\n","        buffer.record((prev_state, action, reward, state))\n","        episodic_reward += reward\n","\n","        buffer.learn()\n","        update_target(target_actor.variables, actor_model.variables, tau)\n","        update_target(target_critic.variables, critic_model.variables, tau)\n","\n","        # End this episode when `done` is True\n","        if done:\n","            break\n","\n","        prev_state = state\n","\n","    ep_reward_list.append(episodic_reward)\n","\n","    # Mean of last 40 episodes\n","    avg_reward = np.mean(ep_reward_list[-40:])\n","    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n","    avg_reward_list.append(avg_reward)\n","\n","# Plotting graph\n","# Episodes versus Avg. Rewards\n","plt.plot(avg_reward_list)\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Avg. Epsiodic Reward\")\n","plt.show()"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode * 0 * Avg Reward is ==> -1437.2255538985144\n","Episode * 1 * Avg Reward is ==> -1497.5456619834927\n","Episode * 2 * Avg Reward is ==> -1552.883046848611\n","Episode * 3 * Avg Reward is ==> -1574.363048402965\n","Episode * 4 * Avg Reward is ==> -1541.4652047484713\n","Episode * 5 * Avg Reward is ==> -1531.2234895627744\n","Episode * 6 * Avg Reward is ==> -1513.5329812891935\n","Episode * 7 * Avg Reward is ==> -1492.1341133973697\n","Episode * 8 * Avg Reward is ==> -1454.193068462935\n","Episode * 9 * Avg Reward is ==> -1412.0962083672148\n","Episode * 10 * Avg Reward is ==> -1365.8996256234734\n","Episode * 11 * Avg Reward is ==> -1333.3900692940638\n","Episode * 12 * Avg Reward is ==> -1289.9322416123287\n","Episode * 13 * Avg Reward is ==> -1244.44081425273\n","Episode * 14 * Avg Reward is ==> -1242.1647853285535\n","Episode * 15 * Avg Reward is ==> -1239.9709569301754\n","Episode * 16 * Avg Reward is ==> -1197.3152646550561\n","Episode * 17 * Avg Reward is ==> -1145.2525101466101\n","Episode * 18 * Avg Reward is ==> -1091.8403089290275\n","Episode * 19 * Avg Reward is ==> -1043.5125972475666\n","Episode * 20 * Avg Reward is ==> -1005.4788071182563\n","Episode * 21 * Avg Reward is ==> -965.4484128748888\n","Episode * 22 * Avg Reward is ==> -929.0836263497124\n","Episode * 23 * Avg Reward is ==> -895.3121366173269\n","Episode * 24 * Avg Reward is ==> -864.4758906226464\n","Episode * 25 * Avg Reward is ==> -835.7512708607891\n","Episode * 26 * Avg Reward is ==> -804.852714685582\n","Episode * 27 * Avg Reward is ==> -784.3593591445851\n","Episode * 28 * Avg Reward is ==> -765.531540782309\n","Episode * 29 * Avg Reward is ==> -744.1987201398189\n","Episode * 30 * Avg Reward is ==> -731.8558616334417\n","Episode * 31 * Avg Reward is ==> -709.1245240044987\n","Episode * 32 * Avg Reward is ==> -691.3846611136481\n","Episode * 33 * Avg Reward is ==> -674.7067905917809\n","Episode * 34 * Avg Reward is ==> -662.4264321714459\n","Episode * 35 * Avg Reward is ==> -654.2330326448678\n","Episode * 36 * Avg Reward is ==> -643.1673718655047\n","Episode * 37 * Avg Reward is ==> -626.3180632499206\n","Episode * 38 * Avg Reward is ==> -616.4400815636818\n","Episode * 39 * Avg Reward is ==> -604.1907866067603\n","Episode * 40 * Avg Reward is ==> -571.478644836011\n","Episode * 41 * Avg Reward is ==> -535.4769404327196\n","Episode * 42 * Avg Reward is ==> -505.5932115382276\n","Episode * 43 * Avg Reward is ==> -470.7247911265904\n","Episode * 44 * Avg Reward is ==> -441.47731391132277\n","Episode * 45 * Avg Reward is ==> -416.61489826467886\n","Episode * 46 * Avg Reward is ==> -390.20022934979323\n","Episode * 47 * Avg Reward is ==> -369.3195125020839\n","Episode * 48 * Avg Reward is ==> -349.9718413764661\n","Episode * 49 * Avg Reward is ==> -327.3298881846016\n","Episode * 50 * Avg Reward is ==> -313.46100202884696\n","Episode * 51 * Avg Reward is ==> -292.1614157590879\n","Episode * 52 * Avg Reward is ==> -278.9984650540597\n","Episode * 53 * Avg Reward is ==> -268.77679623295046\n","Episode * 54 * Avg Reward is ==> -244.41558860735398\n","Episode * 55 * Avg Reward is ==> -219.91129922987662\n","Episode * 56 * Avg Reward is ==> -219.2812997356691\n","Episode * 57 * Avg Reward is ==> -227.99480086806494\n","Episode * 58 * Avg Reward is ==> -231.05756270807242\n","Episode * 59 * Avg Reward is ==> -233.8828586083079\n","Episode * 60 * Avg Reward is ==> -236.42523595339867\n","Episode * 61 * Avg Reward is ==> -236.41364818628458\n","Episode * 62 * Avg Reward is ==> -236.1377458798168\n","Episode * 63 * Avg Reward is ==> -245.71815888936175\n","Episode * 64 * Avg Reward is ==> -245.66680260581478\n","Episode * 65 * Avg Reward is ==> -245.74841233618477\n","Episode * 66 * Avg Reward is ==> -245.7182317286053\n","Episode * 67 * Avg Reward is ==> -242.9406142272407\n","Episode * 68 * Avg Reward is ==> -237.02030688369618\n","Episode * 69 * Avg Reward is ==> -236.81571661503068\n","Episode * 70 * Avg Reward is ==> -233.5552322613409\n","Episode * 71 * Avg Reward is ==> -236.29565331295566\n","Episode * 72 * Avg Reward is ==> -236.32969118915838\n","Episode * 73 * Avg Reward is ==> -236.41139003689824\n","Episode * 74 * Avg Reward is ==> -233.41739841604067\n","Episode * 75 * Avg Reward is ==> -227.3033058709203\n","Episode * 76 * Avg Reward is ==> -226.91452240235822\n","Episode * 77 * Avg Reward is ==> -229.74067977270028\n","Episode * 78 * Avg Reward is ==> -229.57847690777407\n","Episode * 79 * Avg Reward is ==> -226.45673490102905\n","Episode * 80 * Avg Reward is ==> -229.32352254448932\n","Episode * 81 * Avg Reward is ==> -229.40108306559233\n","Episode * 82 * Avg Reward is ==> -220.7761413654145\n","Episode * 83 * Avg Reward is ==> -217.96913971690043\n","Episode * 84 * Avg Reward is ==> -215.11337590028316\n","Episode * 85 * Avg Reward is ==> -206.06599211448275\n","Episode * 86 * Avg Reward is ==> -197.43249674704535\n","Episode * 87 * Avg Reward is ==> -184.86388070593338\n","Episode * 88 * Avg Reward is ==> -178.51829978518265\n","Episode * 89 * Avg Reward is ==> -175.43754652700426\n","Episode * 90 * Avg Reward is ==> -166.7702264242756\n","Episode * 91 * Avg Reward is ==> -172.34957454102764\n","Episode * 92 * Avg Reward is ==> -166.39146130563455\n","Episode * 93 * Avg Reward is ==> -166.41496558606258\n","Episode * 94 * Avg Reward is ==> -169.77966132539035\n","Episode * 95 * Avg Reward is ==> -167.1922106209875\n","Episode * 96 * Avg Reward is ==> -158.0439793117949\n","Episode * 97 * Avg Reward is ==> -145.8951357292391\n","Episode * 98 * Avg Reward is ==> -142.581494720373\n","Episode * 99 * Avg Reward is ==> -139.68238195856694\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"IniZBqkGPECd"},"source":["If training proceeds correctly, the average episodic reward will increase with time.\n","\n","Feel free to try different learning rates, `tau` values, and architectures for the\n","Actor and Critic networks.\n","\n","The Inverted Pendulum problem has low complexity, but DDPG works well on many other\n","problems.\n","\n","Another excellent environment to try this on is `LunarLandingContinuous-v2`, but it will take\n","more episodes to obtain good results."]},{"cell_type":"code","metadata":{"id":"vTSMI1qDPECd","executionInfo":{"status":"ok","timestamp":1651691431226,"user_tz":300,"elapsed":46,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# Save the weights\n","actor_model.save_weights(\"pendulum_actor.h5\")\n","critic_model.save_weights(\"pendulum_critic.h5\")\n","\n","target_actor.save_weights(\"pendulum_target_actor.h5\")\n","target_critic.save_weights(\"pendulum_target_critic.h5\")"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Te2HO0k-PECe"},"source":["Before Training:\n","\n","![before_img](https://i.imgur.com/ox6b9rC.gif)"]},{"cell_type":"markdown","metadata":{"id":"I3krDlRZPECe"},"source":["After 100 episodes:\n","\n","![after_img](https://i.imgur.com/eEH8Cz6.gif)"]},{"cell_type":"markdown","metadata":{"id":"GyByy_DsThur"},"source":["# Tensorflow Agents tutorial - RL on real world data with DDPG\n","\n","A popular way to navigate Reinforcement Learning is through [Tensorflow Agents](https://github.com/tensorflow/agents). The example we will see next uses these agents to solve a problem of paying off mortgage and saving for retirement. Before you dive in, we recommend peeking at these tutorials on Tensorflow Agents: \n","\n","- [Hvaas-Labs RL tutorial](https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb#scrollTo=82khyceYqTGI)\n","\n","- [Tensorflow Agents official tutorial](https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb)\n","\n","- [Jeff Heaton's Deep Learning class module 12, tutorial 4](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_04_atari.ipynb#scrollTo=jbY4yrjTEyc9)\n","\n","The code below is also from a Jeff Heaton tutorial.\n"]},{"cell_type":"markdown","metadata":{"id":"1u9QVVsShC9X"},"source":["\n","Creating an environment is the first step to applying TF-Agent-based reinforcement learning to a problem with your design.  In this part, we will see how to create your environment and apply it to an agent that allows actions to be floating-point values, rather than the discrete actions employed by the Deep Q-Networks (DQN) we used earlier. This new type of agent is called a Deep Deterministic Policy Gradients (DDPG) network. From an application standpoint, the primary difference between DDPG and DQN is that DQN only supports discrete actions, whereas DDPG supports continuous actions; however, there are other essential differences that we will cover later.\n","\n","The environment that we will demonstrate simulates paying off a mortgage and saving for retirement. This simulation allows the agent to allocate their income between several types of account, buying luxury items, and paying off their mortgage. The goal is to maximize net worth.  Because we wish to provide the agent with the ability to distribute their income among several accounts, we provide continuous (floating point) actions that determine this distribution of the agent's salary.\n","\n","We begin, similarly to previous TF-Agent examples, by importing needed packages.\n","\n","\n"]},{"cell_type":"code","source":["! pip install pyvirtualdisplay\n","! pip install tf_agents\n","! sudo apt-get install xvfb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrNUXLZNgNwo","executionInfo":{"status":"ok","timestamp":1651691825731,"user_tz":300,"elapsed":13446,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"9098a4e1-ebbf-4e96-ef20-d241ee395097"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting tf_agents\n","  Downloading tf_agents-0.12.1-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 14.3 MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf_agents) (7.1.2)\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[K     |████████████████████████████████| 18.3 MB 117 kB/s \n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.3.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.0.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.14.0)\n","Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (0.16.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (0.5.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (3.17.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (4.2.0)\n","Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (0.17.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.15.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf_agents) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf_agents) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf_agents) (0.16.0)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf_agents) (0.5.3)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf_agents) (4.4.2)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf_agents) (0.1.7)\n","Installing collected packages: pygame, tf-agents\n","Successfully installed pygame-2.1.0 tf-agents-0.12.1\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'sudo apt autoremove' to remove them.\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,271 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n","Fetched 784 kB in 1s (1,215 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package xvfb.\n","(Reading database ... 155203 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"id":"sMitx5qSgJk1","executionInfo":{"status":"ok","timestamp":1651691826247,"user_tz":300,"elapsed":528,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import base64\n","import imageio\n","import IPython\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL.Image\n","import pyvirtualdisplay\n","import math\n","import numpy as np\n","\n","import tensorflow as tf\n","\n","from tf_agents.agents.ddpg import actor_network\n","from tf_agents.agents.ddpg import critic_network\n","from tf_agents.agents.ddpg import ddpg_agent\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.drivers import dynamic_step_driver\n","from tf_agents.environments import suite_gym\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.eval import metric_utils\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.networks import q_network\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","from tf_agents.trajectories import policy_step\n","from tf_agents.utils import common\n","\n","import gym\n","from gym import spaces\n","from gym.utils import seeding\n","from gym.envs.registration import register\n","import PIL.ImageDraw\n","import PIL.Image\n","from PIL import ImageFont"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UhkU7xcZkNS_"},"source":["Note, if you get the following error, restart and rerun the Google CoLab environment.  Sometimes a restart is needed after installing TF-Agents.\n","\n","```\n","AttributeError: module 'google.protobuf.descriptor' has no \n","    attribute '_internal_create_key'\n","```"]},{"cell_type":"markdown","metadata":{"id":"WjnHqwyZoKJu"},"source":["We create a virtual display so that we can view the simulation in a Jupyter notebook."]},{"cell_type":"code","metadata":{"id":"J6HsdS5GbSjd","executionInfo":{"status":"ok","timestamp":1651691826726,"user_tz":300,"elapsed":486,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# Set up a virtual display for rendering OpenAI gym environments.\n","vdisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKfeSlp05rFW"},"source":["### Create an Environment of your Own\n","\n","An environment is a simulator that your agent runs in. An environment must have a current state. Some of this state is visible to the agent. However, the environment also hides some aspects of the state from the agent.  Likewise, the agent takes actions that will affect the state of the environment.  There may also be internal actions that occur outside the control of the agent.  For example, in the finance simulator demonstrated in this section, the agent does not control the investment returns or rate of inflation. Instead, the agent must react to these external actions and state components.\n","\n","The environment class that you create must contain these elements:\n","\n","* Be a child class of **gym.Env**\n","* Implement a **seed** function that sets a seed that governs the simulation's random aspects. For this environment, the seed oversees the random fluctuations in inflation and rates of return.\n","* Implement a **reset** function that resets the state for a new episode.\n","* Implement a **render** function that renders one frame of the simulation.  The rendering is only for display and does not affect reinforcement learning.\n","* Implement a **step** function that performs one step of your simulation.\n","\n","The class presented below implements a financial planning simulation.  The agent must save for retirement and should attempt to amass the greatest possible net worth.  The simulation includes the following key elements:\n","\n","* Random starting salary between 40K (USD) and 60K (USD).\n","* Home loan for a house with a random purchase price that is between 1.5 and 4 times the starting salary.\n","* Home loan is a standard amortized 30-year loan with a fixed monthly payment.\n","* Paying higher than the home's monthly payment pays the loan down quicker. Paying below the monthly payment results in late fees and eventually foreclosure.\n","* Ability to allocate income between luxury purchases, home payments (above or below payment amount), as well as a taxable and tax-advantaged savings account.\n","\n","The state is composed of the following floating point values:\n","\n","* **age** - The agent's current age in months (steps) \n","* **salary** - The agent's starting salary, increases relative to inflation. \n","* **home_value** - The value of the agent's home, increases relative to inflation.\n","* **home_loan** - How much the agent still owes on their home.\n","* **req_home_pmt** - The minimum required home payment.\n","* **acct_tax_adv** - The balance of the tax advantaged retirement account.\n","* **acct_tax**  - The balance of the taxable retuirement account.\n","\n","The action space is composed of the following floating-point values (between 0 and 1):\n","\n","* **home_loan** - The amount to apply to a home loan.\n","* **savings_tax_adv** - The amount to deposit in a tax-advantaged savings account.\n","* **savings taxable** - The amount to deposit in a taxable savings account.\n","* **luxury** - The amount to spend on luxury items/services.\n","\n","The actions are weights that the program converts to a percentage of the total.  For example, the home loan percentage is the home loan action value divided by all actions (including a home loan).  The following code implements the environment and provides implementation details in the comments."]},{"cell_type":"code","metadata":{"id":"1aDjer9xsdLc","executionInfo":{"status":"ok","timestamp":1651691827426,"user_tz":300,"elapsed":495,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["class SimpleGameOfLifeEnv(gym.Env):\n","    metadata = {\n","        'render.modes': ['human', 'rgb_array'],\n","        'video.frames_per_second': 1\n","    }\n","\n","    STATE_ELEMENTS = 7\n","    STATES = ['age', 'salary', 'home_value', 'home_loan', 'req_home_pmt', \n","              'acct_tax_adv', 'acct_tax', \"expenses\", \"actual_home_pmt\", \n","              \"tax_deposit\", \n","              \"tax_adv_deposit\", \"net_worth\"]\n","    STATE_AGE = 0\n","    STATE_SALARY = 1\n","    STATE_HOME_VALUE = 2\n","    STATE_HOME_LOAN = 3\n","    STATE_HOME_REQ_PAYMENT = 4\n","    STATE_SAVE_TAX_ADV = 5\n","    STATE_SAVE_TAXABLE = 6\n","\n","    MEG = 1.0e6\n","\n","    ACTION_ELEMENTS = 4\n","    ACTION_HOME_LOAN = 0\n","    ACTION_SAVE_TAX_ADV = 1\n","    ACTION_SAVE_TAXABLE = 2\n","    ACTION_LUXURY = 3\n","\n","    INFLATION = (0.015)/12.0\n","    INTEREST = (0.05)/12.0\n","    TAX_RATE = (.142)/12.0\n","    EXPENSES = 0.6\n","    INVEST_RETURN = 0.065/12.0\n","    SALARY_LOW = 40000.0\n","    SALARY_HIGH = 60000.0\n","    START_AGE = 18\n","    RETIRE_AGE = 80\n","\n","    def __init__(self, goal_velocity=0):\n","      self.verbose = False\n","      self.viewer = None\n","\n","      self.action_space = spaces.Box(\n","          low=0.0,\n","          high=1.0,\n","          shape=(SimpleGameOfLifeEnv.ACTION_ELEMENTS,),\n","          dtype=np.float32\n","      )\n","      self.observation_space = spaces.Box(\n","          low=0,\n","          high=2,\n","          shape=(SimpleGameOfLifeEnv.STATE_ELEMENTS,),\n","          dtype=np.float32\n","      )\n","\n","      self.seed()\n","      self.reset()\n","\n","      self.state_log = []\n","\n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def _calc_net_worth(self):\n","      home_value = self.state[SimpleGameOfLifeEnv.STATE_HOME_VALUE]\n","      principal = self.state[SimpleGameOfLifeEnv.STATE_HOME_LOAN]\n","      worth = home_value - principal\n","      worth += self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAX_ADV]\n","      worth += self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAXABLE]\n","      return worth\n","\n","    def _eval_action(self, action, payment):\n","      # Calculate actions\n","      act_home_payment = action[SimpleGameOfLifeEnv.ACTION_HOME_LOAN]\n","      act_tax_adv_pay = action[SimpleGameOfLifeEnv.ACTION_SAVE_TAX_ADV]\n","      act_taxable = action[SimpleGameOfLifeEnv.ACTION_SAVE_TAXABLE]\n","      act_luxury = action[SimpleGameOfLifeEnv.ACTION_LUXURY]\n","      if payment <=0:\n","        act_home_payment = 0\n","      total_act = act_home_payment + act_tax_adv_pay + act_taxable + \\\n","            act_luxury + self.expenses\n","\n","      if total_act <1e-2:\n","        pct_home_payment = 0\n","        pct_tax_adv_pay = 0\n","        pct_taxable = 0\n","        pct_luxury = 0\n","      else:\n","        pct_home_payment = act_home_payment / total_act\n","        pct_tax_adv_pay = act_tax_adv_pay / total_act\n","        pct_taxable = act_taxable / total_act\n","        pct_luxury = act_luxury / total_act\n","\n","      return pct_home_payment, pct_tax_adv_pay, pct_taxable, pct_luxury\n","\n","    def step(self, action):\n","      self.last_action = action\n","      age = self.state[SimpleGameOfLifeEnv.STATE_AGE]\n","      salary = self.state[SimpleGameOfLifeEnv.STATE_SALARY]\n","      home_value = self.state[SimpleGameOfLifeEnv.STATE_HOME_VALUE]\n","      principal = self.state[SimpleGameOfLifeEnv.STATE_HOME_LOAN]\n","      payment = self.state[SimpleGameOfLifeEnv.STATE_HOME_REQ_PAYMENT]\n","      net1 = self._calc_net_worth()\n","      remaining_salary = salary\n","\n","      # Calculate actions\n","      pct_home_payment, pct_tax_adv_pay, pct_taxable, pct_luxury = \\\n","        self._eval_action(action,payment)\n","\n","      # Expenses\n","      current_expenses = salary * self.expenses\n","      remaining_salary -= current_expenses\n","      if self.verbose:\n","        print(f\"Expenses: {current_expenses}\")\n","        print(f\"Remaining Salary: {remaining_salary}\")\n","\n","      # Tax advantaged deposit action\n","      my_tax_adv_deposit = min(salary * pct_tax_adv_pay, remaining_salary)\n","      my_tax_adv_deposit = min(my_tax_adv_deposit, \\\n","        self.year_tax_adv_deposit_left) # Govt CAP\n","      self.year_tax_adv_deposit_left -= my_tax_adv_deposit\n","      remaining_salary -= my_tax_adv_deposit\n","      tax_adv_deposit = my_tax_adv_deposit * 1.05 # Company match\n","      self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAX_ADV] += \\\n","        int(tax_adv_deposit)\n","\n","      if self.verbose:\n","        print(f\"IRA Deposit: {tax_adv_deposit}\")\n","        print(f\"Remaining Salary: {remaining_salary}\")\n","\n","      # Tax\n","      remaining_salary -= remaining_salary * SimpleGameOfLifeEnv.TAX_RATE\n","      if self.verbose:\n","        print(f\"Tax Salary: {remaining_salary}\")\n","\n","      # Home payment\n","      actual_payment = min(salary * pct_home_payment, remaining_salary)\n","\n","      if principal>0:\n","        ipart = principal * SimpleGameOfLifeEnv.INTEREST\n","        ppart = actual_payment - ipart\n","        principal = int(principal-ppart)\n","        if principal<=0:\n","          principal = 0\n","          self.state[SimpleGameOfLifeEnv.STATE_HOME_REQ_PAYMENT] = 0\n","        elif actual_payment < payment:\n","          self.late_count += 1\n","          if self.late_count>15:\n","            sell = (home_value-principal)/2\n","            sell -= 20000\n","            sell = max(sell,0)\n","            self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAXABLE] += sell\n","            principal = 0\n","            home_value = 0\n","            self.expenses += .3\n","            self.state[SimpleGameOfLifeEnv.STATE_HOME_REQ_PAYMENT] = 0\n","            if self.verbose:\n","              print(f\"Foreclosure!!\")\n","          else:\n","            late_fee = payment * 0.1\n","            principal += late_fee\n","            if self.verbose:\n","              print(f\"Late Fee: {late_fee}\")\n","\n","\n","        self.state[SimpleGameOfLifeEnv.STATE_HOME_LOAN] = principal\n","        remaining_salary -= actual_payment\n","\n","      if self.verbose:\n","        print(f\"Home Payment: {actual_payment}\")\n","        print(f\"Remaining Salary: {remaining_salary}\")\n","\n","      # Taxable savings\n","      actual_savings = remaining_salary * pct_taxable\n","      self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAXABLE] += actual_savings\n","      remaining_salary -= actual_savings\n","\n","      if self.verbose:\n","        print(f\"Tax Save: {actual_savings}\")\n","        print(f\"Remaining Salary (goes to Luxury): {remaining_salary}\")\n","\n","      # Investment income\n","      return_taxable = self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAXABLE] * \\\n","          self.invest_return\n","      return_tax_adv = self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAX_ADV] * \\\n","          self.invest_return\n","\n","      return_taxable *= 1-SimpleGameOfLifeEnv.TAX_RATE\n","      self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAXABLE] += return_taxable\n","      self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAX_ADV] += return_tax_adv\n","\n","      # Yearly events\n","      if age>0 and age % 12 == 0:\n","        self.perform_yearly()\n","\n","      # Monthly events\n","      self.state[SimpleGameOfLifeEnv.STATE_AGE] += 1\n","      \n","      # Time to retire (by age?)\n","      done = self.state[SimpleGameOfLifeEnv.STATE_AGE] > \\\n","        (SimpleGameOfLifeEnv.RETIRE_AGE*12)\n","\n","      # Calculate reward \n","      net2 = self._calc_net_worth()\n","      reward = net2 - net1\n","\n","      # Track progress\n","      if self.verbose:\n","        print(f\"Networth: {nw}\")\n","        print(f\"*** End Step {self.step_num}: State={self.state}, \\\n","          Reward={reward}\")\n","      self.state_log.append(self.state + [current_expenses, actual_payment, \n","      actual_savings, my_tax_adv_deposit, net2])\n","      self.step_num += 1\n","\n","      # Normalize state and finish up\n","      norm_state = [x/SimpleGameOfLifeEnv.MEG for x in self.state]\n","      return norm_state, reward/SimpleGameOfLifeEnv.MEG, done, {}\n","\n","    def perform_yearly(self):\n","      salary = self.state[SimpleGameOfLifeEnv.STATE_SALARY]\n","      home_value = self.state[SimpleGameOfLifeEnv.STATE_HOME_VALUE]\n","      \n","      self.inflation = SimpleGameOfLifeEnv.INTEREST + \\\n","          self.np_random.normal(loc=0,scale=1e-2)\n","      self.invest_return = SimpleGameOfLifeEnv.INVEST_RETURN + \\\n","          self.np_random.normal(loc=0,scale=1e-2)\n","\n","      self.year_tax_adv_deposit_left = 19000\n","      self.state[SimpleGameOfLifeEnv.STATE_SALARY] = \\\n","        int(salary * (1+self.inflation))\n","\n","      self.state[SimpleGameOfLifeEnv.STATE_HOME_VALUE] \\\n","        = int(home_value * (1+self.inflation))\n","\n","    def reset(self):\n","      self.expenses = SimpleGameOfLifeEnv.EXPENSES\n","      self.late_count = 0\n","      self.step_num = 0\n","      self.last_action = [0] * SimpleGameOfLifeEnv.ACTION_ELEMENTS\n","      self.state = [0] * SimpleGameOfLifeEnv.STATE_ELEMENTS\n","      self.state_log = []\n","      salary = float(self.np_random.randint(low=\\\n","                SimpleGameOfLifeEnv.SALARY_LOW,\\\n","                high=SimpleGameOfLifeEnv.SALARY_HIGH))\n","      house_mult = self.np_random.uniform(low=1.5,high=4)\n","      value = round(salary*house_mult)\n","      p = (value*0.9)\n","      i = SimpleGameOfLifeEnv.INTEREST\n","      n = 30 * 12\n","      m = float(int(p *  ( i * (1 + i)**n ) / ( (1 + i)**n - 1)))\n","      self.state[SimpleGameOfLifeEnv.STATE_AGE] = \\\n","        SimpleGameOfLifeEnv.START_AGE * 12\n","      self.state[SimpleGameOfLifeEnv.STATE_SALARY] = salary / 12.0\n","      self.state[SimpleGameOfLifeEnv.STATE_HOME_VALUE] = value\n","      self.state[SimpleGameOfLifeEnv.STATE_HOME_LOAN] = p\n","      self.state[SimpleGameOfLifeEnv.STATE_HOME_REQ_PAYMENT] = m\n","      self.year_tax_adv_deposit_left = 19000\n","      self.perform_yearly()\n","      return np.array(self.state)\n","\n","    def render(self, mode='human'):\n","        screen_width = 600\n","        screen_height = 400\n","\n","        img = PIL.Image.new('RGB', (600, 400))\n","        d = PIL.ImageDraw.Draw(img)\n","        font = ImageFont.load_default()\n","        y = 0\n","        _, height = d.textsize(\"W\", font)\n","  \n","        age = self.state[SimpleGameOfLifeEnv.STATE_AGE]\n","        salary = self.state[SimpleGameOfLifeEnv.STATE_SALARY]*12\n","        home_value = self.state[SimpleGameOfLifeEnv.STATE_HOME_VALUE]\n","        home_loan = self.state[SimpleGameOfLifeEnv.STATE_HOME_LOAN]\n","        home_payment = self.state[SimpleGameOfLifeEnv.STATE_HOME_REQ_PAYMENT]\n","        balance_tax_adv = self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAX_ADV]\n","        balance_taxable = self.state[SimpleGameOfLifeEnv.STATE_SAVE_TAXABLE]\n","        net_worth = self._calc_net_worth()\n","\n","        d.text((0, y), f\"Age: {age/12}\", fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Salary: {salary:,}\", fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Home Value: {home_value:,}\", \\\n","                fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Home Loan: {home_loan:,}\", \\\n","               fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Home Payment: {home_payment:,}\", \\\n","               fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Balance Tax Adv: {balance_tax_adv:,}\", \\\n","               fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Balance Taxable: {balance_taxable:,}\", \\\n","               fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Net Worth: {net_worth:,}\", fill=(0, 255, 0))\n","        y+=height*2\n","\n","        payment = self.state[SimpleGameOfLifeEnv.STATE_HOME_REQ_PAYMENT]\n","        pct_home_payment, pct_tax_adv_pay, pct_taxable, pct_luxury = \\\n","          self._eval_action(self.last_action,payment)\n","        d.text((0, y), f\"Percent Home Payment: {pct_home_payment}\", \\\n","               fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Percent Tax Adv: {pct_tax_adv_pay}\", \n","               fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Percent Taxable: {pct_taxable}\", fill=(0, 255, 0))\n","        y+=height\n","        d.text((0, y), f\"Percent Luxury: {pct_luxury}\", fill=(0, 255, 0))\n","\n","        return np.array(img)\n","\n","    def close(self):\n","        pass"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lGg0y8Hw-ug"},"source":["You must register the environment class with TF-Agents before your program can use it."]},{"cell_type":"code","metadata":{"id":"EHT8kKwB3aK3","executionInfo":{"status":"ok","timestamp":1651691827427,"user_tz":300,"elapsed":5,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["register(\n","    id='simple-game-of-life-v0',\n","    entry_point=f'{__name__}:SimpleGameOfLifeEnv',\n",")"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sv4jBX5U6kjW"},"source":["### Testing the Environment\n","\n","This financial planning environment is complex.  It required some degree of testing to perfect it.  Even at the current state of this simulator, it is far from a complete financial simulator.  The primary objective of this simulator is to demonstrate creating your own environment for a non-video game project.\n","\n","We used the following code to help test this simulator. We ran the simulator with fixed actions and then loaded the state into a Pandas data frame for easy viewing."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScDdkjHo2mcb","outputId":"8fbee500-40e5-48dd-9876-03fe8a1e9873","executionInfo":{"status":"ok","timestamp":1651691828496,"user_tz":300,"elapsed":1073,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["env_name = 'simple-game-of-life-v0'\n","env = gym.make(env_name)\n","\n","env.reset()\n","done = False\n","\n","i = 0\n","env.verbose = False\n","while not done:\n","    i += 1\n","    #if i>36: break\n","    # ACTION_HOME_LOAN = 0, ACTION_SAVE_TAX_ADV = 1, ACTION_SAVE_TAXABLE = 2\n","    # ACTION_LUXURY = 3\n","    state, reward, done, _ = env.step([1,1,0,0])\n","    env.render()\n","    \n","env.close()\n","print(env._calc_net_worth())"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["8950823.799884506\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"HHgPyoT6vH_r","outputId":"40a8f001-8789-48a7-d203-fb1b8b4510f4","executionInfo":{"status":"ok","timestamp":1651691828499,"user_tz":300,"elapsed":38,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import pandas as pd\n","\n","df = pd.DataFrame(env.state_log, columns=SimpleGameOfLifeEnv.STATES)\n","df = df.round(0)\n","df['age'] = df['age']/12\n","df['age'] = df['age'].round(2)\n","for col in df.columns:\n","  df[col] = df[col].apply(lambda x : \"{:,}\".format(x))\n","\n","pd.set_option('display.max_columns', 7)\n","pd.set_option('display.max_rows', 12)\n","display(df)"],"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["       age salary home_value  ... tax_deposit tax_adv_deposit    net_worth\n","0    18.08  4,737    188,067  ...         0.0         1,843.0     15,040.0\n","1    18.17  4,737    188,067  ...         0.0         1,822.0     16,226.0\n","2    18.25  4,737    188,067  ...         0.0         1,822.0     17,419.0\n","3    18.33  4,737    188,067  ...         0.0         1,822.0     18,621.0\n","4    18.42  4,737    188,067  ...         0.0         1,822.0     19,832.0\n","..     ...    ...        ...  ...         ...             ...          ...\n","740  79.75  5,790    231,109  ...         0.0           579.0  8,581,958.0\n","741  79.83  5,790    231,109  ...         0.0           579.0  8,672,511.0\n","742  79.92  5,790    231,109  ...         0.0           579.0  8,764,039.0\n","743   80.0  5,790    231,109  ...         0.0           579.0  8,856,553.0\n","744  80.08  5,809    231,869  ...         0.0           579.0  8,950,824.0\n","\n","[745 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-5a9b3702-62e0-4844-ad0a-a929d9c56de8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>salary</th>\n","      <th>home_value</th>\n","      <th>...</th>\n","      <th>tax_deposit</th>\n","      <th>tax_adv_deposit</th>\n","      <th>net_worth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18.08</td>\n","      <td>4,737</td>\n","      <td>188,067</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1,843.0</td>\n","      <td>15,040.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>18.17</td>\n","      <td>4,737</td>\n","      <td>188,067</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1,822.0</td>\n","      <td>16,226.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18.25</td>\n","      <td>4,737</td>\n","      <td>188,067</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1,822.0</td>\n","      <td>17,419.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>18.33</td>\n","      <td>4,737</td>\n","      <td>188,067</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1,822.0</td>\n","      <td>18,621.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>18.42</td>\n","      <td>4,737</td>\n","      <td>188,067</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1,822.0</td>\n","      <td>19,832.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>740</th>\n","      <td>79.75</td>\n","      <td>5,790</td>\n","      <td>231,109</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>579.0</td>\n","      <td>8,581,958.0</td>\n","    </tr>\n","    <tr>\n","      <th>741</th>\n","      <td>79.83</td>\n","      <td>5,790</td>\n","      <td>231,109</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>579.0</td>\n","      <td>8,672,511.0</td>\n","    </tr>\n","    <tr>\n","      <th>742</th>\n","      <td>79.92</td>\n","      <td>5,790</td>\n","      <td>231,109</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>579.0</td>\n","      <td>8,764,039.0</td>\n","    </tr>\n","    <tr>\n","      <th>743</th>\n","      <td>80.0</td>\n","      <td>5,790</td>\n","      <td>231,109</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>579.0</td>\n","      <td>8,856,553.0</td>\n","    </tr>\n","    <tr>\n","      <th>744</th>\n","      <td>80.08</td>\n","      <td>5,809</td>\n","      <td>231,869</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>579.0</td>\n","      <td>8,950,824.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>745 rows × 12 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a9b3702-62e0-4844-ad0a-a929d9c56de8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5a9b3702-62e0-4844-ad0a-a929d9c56de8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5a9b3702-62e0-4844-ad0a-a929d9c56de8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"3Az8Q3kSuMrX"},"source":["1810888.5833333335"]},{"cell_type":"markdown","metadata":{"id":"LmC0NDhdLIKY"},"source":["## Hyperparameters\n","\n","We tuned the following hyperparamaters to get a reasonable result from training the agent.  Further optimization would be beneficial."]},{"cell_type":"code","metadata":{"id":"HC1kNrOsLSIZ","executionInfo":{"status":"ok","timestamp":1651691828505,"user_tz":300,"elapsed":43,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# How long should training run?\n","num_iterations = 50000 \n","# How many initial random steps, before training start, to\n","# collect initial data.\n","initial_collect_steps = 1000   \n","# How many steps should we run each iteration to collect \n","# data from.\n","collect_steps_per_iteration = 50\n","# How much data should we store for training examples.\n","replay_buffer_max_length = 100000\n","\n","batch_size = 64  \n","#learning_rate = 1e-4 \n","# How often should the program provide an update.\n","log_interval = 2500  \n","\n","# How many episodes should the program use for each evaluation.\n","num_eval_episodes = 100\n","# How often should an evaluation occur.\n","eval_interval = 5000  "],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMsJC3DEgI0x"},"source":["## Instantiate the Environment\n","\n","We are now ready to make use of our environment.  Because we registered the environment with TF-Agents the program can load the environment by its name \"simple-game-of-life-v\".  You can also try the continuous version of the mountain car environment by switching the name given below.  The continuous mountain car works similar to the discrete mountain car seen previously.  However, the continuous mountain car allows the degree of forward or backward force to be specified."]},{"cell_type":"code","metadata":{"id":"pYEz-S9gEv2-","executionInfo":{"status":"ok","timestamp":1651691828506,"user_tz":300,"elapsed":43,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["env_name = 'simple-game-of-life-v0'\n","#env_name = 'MountainCarContinuous-v0'\n","env = suite_gym.load(env_name)"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIHYVBkuvPNw"},"source":["We can now have a quick look at the first state rendered. Here we can see the random salary and home values are chosen for an agent.  The learned policy must be able to take into consideration different starting salaries and home values and find an appropriate strategy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"id":"RlO7WIQHu_7D","outputId":"75dfbe80-be1c-4934-be62-59b9e5f19f38","executionInfo":{"status":"ok","timestamp":1651691828508,"user_tz":300,"elapsed":44,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["env.reset()\n","PIL.Image.fromarray(env.render())"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=600x400 at 0x7F21FE757290>"],"image/png":"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\n"},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"4JSc9GviWUBK"},"source":["As before, the program instantiates two environments: one for training and one for evaluation. "]},{"cell_type":"code","metadata":{"id":"N7brXNIGWXjC","executionInfo":{"status":"ok","timestamp":1651691828509,"user_tz":300,"elapsed":43,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["train_py_env = suite_gym.load(env_name)\n","eval_py_env = suite_gym.load(env_name)\n","\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9lW_OZYFR8A"},"source":["Recall that DQN does not support continuous actions--the DQN algorithm maps each action as an output activation. Each of these activations predicts the likely future reward for taking each action. Generally, the DQN agent will perform the action with the highest reward. The algorithm knows the future rewards for each particular action.  However, because a continuous number represented in a computer has an effectively infinite number of possible values, it is not possible to calculate a future reward estimate for all of them.\n","\n","To provide a continuous action space we will use the Deep Deterministic Policy Gradients (DDPG) algorithm. [[Cite:lillicrap2015continuous]](https://arxiv.org/abs/1509.02971) This technique uses two neural networks.  The first neural network, called an actor, acts as the agent and predicts the expected reward for a given value of the action.  The second neural network, called a critic, is trained to predict the accuracy of the actor-network.  Training two neural networks in parallel that operate adversarially to one another is a popular technique.  Earlier in this course, we saw that Generative Adversarial Networks (GAN) made use of a similar method. The figure below shows the structure of the DDPG network we will use. \n","\n","**DDPG: Actor Critic Model**\n","![Deep Q-Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/actor-critic.png \"Actor Critic Model\")\n","\n","The environment provides the same input ($x(t)$) for each time step to both the actor and critic networks. The temporal difference error ($r(t)$) reports the difference between the estimated reward at any given state or time step and the actual reward. \n","\n","The following code creates the actor and critic neural networks."]},{"cell_type":"code","metadata":{"id":"eO_TVGGR9NSY","executionInfo":{"status":"ok","timestamp":1651691829298,"user_tz":300,"elapsed":247,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["actor_fc_layers=(400, 300)\n","critic_obs_fc_layers=(400,)\n","critic_action_fc_layers=None\n","critic_joint_fc_layers=(300,)\n","ou_stddev=0.2\n","ou_damping=0.15\n","target_update_tau=0.05\n","target_update_period=5\n","dqda_clipping=None\n","td_errors_loss_fn=tf.compat.v1.losses.huber_loss\n","gamma=0.995\n","reward_scale_factor=1.0\n","gradient_clipping=None\n","\n","actor_learning_rate=1e-4\n","critic_learning_rate=1e-3\n","debug_summaries=False\n","summarize_grads_and_vars=False\n","\n","global_step = tf.compat.v1.train.get_or_create_global_step()\n","\n","actor_net = actor_network.ActorNetwork(\n","        train_env.time_step_spec().observation,\n","        train_env.action_spec(),\n","        fc_layer_params=actor_fc_layers,\n","    )\n","\n","critic_net_input_specs = (train_env.time_step_spec().observation,\n","                          train_env.action_spec())\n","\n","critic_net = critic_network.CriticNetwork(\n","    critic_net_input_specs,\n","    observation_fc_layer_params=critic_obs_fc_layers,\n","    action_fc_layer_params=critic_action_fc_layers,\n","    joint_fc_layer_params=critic_joint_fc_layers,\n",")\n","\n","tf_agent = ddpg_agent.DdpgAgent(\n","    train_env.time_step_spec(),\n","    train_env.action_spec(),\n","    actor_network=actor_net,\n","    critic_network=critic_net,\n","    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n","        learning_rate=actor_learning_rate),\n","    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n","        learning_rate=critic_learning_rate),\n","    ou_stddev=ou_stddev,\n","    ou_damping=ou_damping,\n","    target_update_tau=target_update_tau,\n","    target_update_period=target_update_period,\n","    dqda_clipping=dqda_clipping,\n","    td_errors_loss_fn=td_errors_loss_fn,\n","    gamma=gamma,\n","    reward_scale_factor=reward_scale_factor,\n","    gradient_clipping=gradient_clipping,\n","    debug_summaries=debug_summaries,\n","    summarize_grads_and_vars=summarize_grads_and_vars,\n","    train_step_counter=global_step)\n","tf_agent.initialize()\n","\n"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"94rCXQtbUbXv"},"source":["## Metrics and Evaluation\n","\n","Just as in previous examples, we will compute the average return over several episodes to evaluate performance.\n"]},{"cell_type":"code","metadata":{"id":"bitzHo5_UbXy","executionInfo":{"status":"ok","timestamp":1651691829299,"user_tz":300,"elapsed":11,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["def compute_avg_return(environment, policy, num_episodes=10):\n","\n","  total_return = 0.0\n","  for _ in range(num_episodes):\n","\n","    time_step = environment.reset()\n","    episode_return = 0.0\n","\n","    while not time_step.is_last():\n","      action_step = policy.action(time_step)\n","      time_step = environment.step(action_step.action)\n","      episode_return += time_step.reward\n","    total_return += episode_return\n","\n","  avg_return = total_return / num_episodes\n","  return avg_return.numpy()[0]\n","\n","\n","# See also the metrics module for standard implementations of \n","# different metrics.\n","# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVD5nQ9ZGo8_"},"source":["## Data Collection\n","\n","Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."]},{"cell_type":"code","metadata":{"id":"wr1KSAEGG4h9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651691830524,"user_tz":300,"elapsed":1236,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"227f11cd-6170-47ec-df7a-c3f3a5d260ae"},"source":["def collect_step(environment, policy, buffer):\n","  time_step = environment.current_time_step()\n","  action_step = policy.action(time_step)\n","  next_time_step = environment.step(action_step.action)\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","  # Add trajectory to the replay buffer\n","  buffer.add_batch(traj)\n","\n","def collect_data(env, policy, buffer, steps):\n","  for _ in range(steps):\n","    collect_step(env, policy, buffer)\n","\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","    data_spec=tf_agent.collect_data_spec,\n","    batch_size=train_env.batch_size,\n","    max_length=replay_buffer_max_length)\n","\n","collect_data(train_env, random_policy, replay_buffer, steps=100)\n","\n","# Dataset generates trajectories with shape [Bx2x...]\n","dataset = replay_buffer.as_dataset(\n","    num_parallel_calls=3, \n","    sample_batch_size=batch_size, \n","    num_steps=2).prefetch(3)\n"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n"]}]},{"cell_type":"markdown","metadata":{"id":"hBc9lj9VWWtZ"},"source":["## Training the agent\n","\n","We are now ready to train the agent. This process can take many hours, depending on how many episodes you wish to run through. As training occurs, this code will update on both the loss and average return. As training becomes more successful, the average return should increase. The losses reported reflecting the average loss for individual training batches."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"0pTbJ3PeyF-u","outputId":"3e9ca658-2e58-4cfc-9f13-61991c7f4753","executionInfo":{"status":"error","timestamp":1651692447634,"user_tz":300,"elapsed":617114,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["iterator = iter(dataset)\n","\n","# (Optional) Optimize by wrapping some of the code in a graph using \n","# TF function.\n","tf_agent.train = common.function(tf_agent.train)\n","\n","# Reset the train step\n","tf_agent.train_step_counter.assign(0)\n","\n","# Evaluate the agent's policy once before training.\n","avg_return = compute_avg_return(eval_env, tf_agent.policy, \\\n","                                num_eval_episodes)\n","returns = [avg_return]\n","\n","for _ in range(num_iterations):\n","\n","  # Collect a few steps using collect_policy and save to the replay buffer.\n","  for _ in range(collect_steps_per_iteration):\n","    collect_step(train_env, tf_agent.collect_policy, replay_buffer)\n","\n","  # Sample a batch of data from the buffer and update the agent's network.\n","  experience, unused_info = next(iterator)\n","  train_loss = tf_agent.train(experience).loss\n","\n","  step = tf_agent.train_step_counter.numpy()\n","\n","  if step % log_interval == 0:\n","    print('step = {0}: loss = {1}'.format(step, train_loss))\n","\n","  if step % eval_interval == 0:\n","    avg_return = compute_avg_return(eval_env, tf_agent.policy, \\\n","                                    num_eval_episodes)\n","    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n","    returns.append(avg_return)"],"execution_count":39,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-47b4dc5b373c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_steps_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcollect_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-b012cacc41b9>\u001b[0m in \u001b[0;36mcollect_step\u001b[0;34m(environment, policy, buffer)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_time_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_time_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/environments/tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m           name='step_py_func')\n\u001b[0m\u001b[1;32m    320\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step_from_numpy_function_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[0;34m(func, inp, Tout, name)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m   \"\"\"\n\u001b[0;32m--> 750\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[0;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0;31m# Mimic the automatic unwrapping in graph-mode py_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0;31m# Mimic the automatic unwrapping in graph-mode py_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \"\"\"\n\u001b[1;32m    267\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 268\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"68jNcA_TiJDq"},"source":["## Visualization\n","\n","The notebook can plot the average return over training iterations. The average return should increase as the program performs more training iterations."]},{"cell_type":"markdown","metadata":{"id":"aO-LWCdbbOIC"},"source":["### Plots\n","\n","Use **matplotlib.pyplot** to chart how the policy improved during training.\n","\n","One iteration of **Cartpole-v0** consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts show the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase each time monotonically.)"]},{"cell_type":"code","metadata":{"id":"NxtL1mbOYCVO"},"source":["iterations = range(0, num_iterations + 1, eval_interval)\n","plt.plot(iterations, returns)\n","plt.ylabel('Average Return')\n","plt.xlabel('Iterations')\n","plt.ylim(top=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M7-XpPP99Cy7"},"source":["### Videos\n","\n","We use the following functions to produce video in Jupyter notebook."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541,"output_embedded_package_id":"16_Z9a3BKwMm7Q6HddCy6ooSW4pNc4YT6"},"id":"ULaGr8pvOKbl","outputId":"9d98d758-fa20-4618-ba5b-fe29d237972d","executionInfo":{"status":"ok","timestamp":1651692533467,"user_tz":300,"elapsed":54155,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["def embed_mp4(filename):\n","  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n","  video = open(filename,'rb').read()\n","  b64 = base64.b64encode(video)\n","  tag = '''\n","  <video width=\"640\" height=\"480\" controls>\n","    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n","  Your browser does not support the video tag.\n","  </video>'''.format(b64.decode())\n","\n","  return IPython.display.HTML(tag)\n","\n","def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n","  filename = filename + \".mp4\"\n","  with imageio.get_writer(filename, fps=fps) as video:\n","    for _ in range(num_episodes):\n","      time_step = eval_env.reset()\n","      video.append_data(eval_py_env.render())\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = eval_env.step(action_step.action)\n","        video.append_data(eval_py_env.render())\n","  return embed_mp4(filename)\n","\n","create_policy_eval_video(tf_agent.policy, \"trained-agent\")"],"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"ik4SPxYEmJ2w"},"source":["The agent's net worth is consistently rising!"]},{"cell_type":"markdown","metadata":{"id":"5-PO2zYLwEme"},"source":["# Reinforcement Learning for Finance\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yv3IDvrobU37"},"source":["## Deep Reinforcement Learning for Stock Trading from Scratch: Portfolio Allocation\n","\n","Tutorials to use OpenAI DRL to perform portfolio allocation | Drawn from a Presentation at NeurIPS 2020: Deep RL Workshop\n","\n","* This blog is based on the paper: FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance, presented at NeurIPS 2020: Deep RL Workshop.\n","* Check out medium blog for detailed explanations: \n","* Please report any issues to the Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n","* **Pytorch Version** \n","\n"]},{"cell_type":"markdown","metadata":{"id":"12v1i0jVkg48"},"source":["### Part 1. Problem Definition"]},{"cell_type":"markdown","metadata":{"id":"L63HKnWvkirx"},"source":["This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n","\n","The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n","\n","\n","* Action: The action space describes the allowed actions that the agent interacts with the\n","environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n","selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n","an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n","10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n","\n","* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n","values at state s′ and s, respectively\n","\n","* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n","our trading agent observes many different features to better learn in an interactive environment.\n","\n","* Environment: Dow 30 consituents\n","\n","\n","The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"]},{"cell_type":"markdown","metadata":{"id":"g_emqQCCklVt"},"source":["### Part 2. Getting Started- Load Python Packages"]},{"cell_type":"markdown","metadata":{"id":"cVCcCalAknGn"},"source":["### 2.1. Install all the packages through FinRL library\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pT8a0fvhA_TW","outputId":"9c3613bd-9be3-480b-acbe-30c347bb8cd4","executionInfo":{"status":"ok","timestamp":1651693502401,"user_tz":300,"elapsed":82281,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["## install finrl library\n","!pip install -U finrl"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting finrl\n","  Downloading finrl-0.3.4-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 4.3 MB/s \n","\u001b[?25hCollecting pyfolio\n","  Downloading pyfolio-0.9.2.tar.gz (91 kB)\n","\u001b[K     |████████████████████████████████| 91 kB 9.2 MB/s \n","\u001b[?25hCollecting ccxt>=1.66.32\n","  Downloading ccxt-1.81.45-py2.py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 56.4 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from finrl) (3.2.2)\n","Collecting ray[tune]\n","  Downloading ray-1.12.0-cp37-cp37m-manylinux2014_x86_64.whl (53.2 MB)\n","\u001b[K     |████████████████████████████████| 53.2 MB 1.2 MB/s \n","\u001b[?25hCollecting pre-commit\n","  Downloading pre_commit-2.18.1-py2.py3-none-any.whl (197 kB)\n","\u001b[K     |████████████████████████████████| 197 kB 81.3 MB/s \n","\u001b[?25hCollecting wrds\n","  Downloading wrds-3.1.1-py3-none-any.whl (12 kB)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from finrl) (3.6.4)\n","Collecting alpaca-trade-api\n","  Downloading alpaca_trade_api-2.1.0-py3-none-any.whl (33 kB)\n","Collecting lz4\n","  Downloading lz4-4.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 46.0 MB/s \n","\u001b[?25hCollecting elegantrl\n","  Downloading elegantrl-0.3.3-py3-none-any.whl (234 kB)\n","\u001b[K     |████████████████████████████████| 234 kB 77.8 MB/s \n","\u001b[?25hCollecting jqdatasdk\n","  Downloading jqdatasdk-1.8.10-py3-none-any.whl (153 kB)\n","\u001b[K     |████████████████████████████████| 153 kB 77.6 MB/s \n","\u001b[?25hCollecting stockstats\n","  Downloading stockstats-0.4.1-py2.py3-none-any.whl (19 kB)\n","Requirement already satisfied: setuptools>=41.4.0 in /usr/local/lib/python3.7/dist-packages (from finrl) (57.4.0)\n","Requirement already satisfied: wheel>=0.33.6 in /usr/local/lib/python3.7/dist-packages (from finrl) (0.37.1)\n","Collecting yfinance\n","  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 64.7 MB/s \n","\u001b[?25hCollecting stable-baselines3[extra]\n","  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n","\u001b[K     |████████████████████████████████| 177 kB 71.9 MB/s \n","\u001b[?25hCollecting exchange-calendars\n","  Downloading exchange_calendars-3.6.1.tar.gz (150 kB)\n","\u001b[K     |████████████████████████████████| 150 kB 81.0 MB/s \n","\u001b[?25hCollecting gputil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from finrl) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from finrl) (1.0.2)\n","Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from finrl) (0.17.3)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from finrl) (1.3.5)\n","Collecting yarl==1.7.2\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 74.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.66.32->finrl) (2.23.0)\n","Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.66.32->finrl) (2021.10.8)\n","Collecting setuptools>=41.4.0\n","  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 61.3 MB/s \n","\u001b[?25hCollecting aiodns>=1.1.1\n","  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n","Collecting aiohttp>=3.8\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 55.9 MB/s \n","\u001b[?25hCollecting cryptography>=2.6.1\n","  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 56.6 MB/s \n","\u001b[?25hCollecting multidict>=4.0\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt>=1.66.32->finrl) (4.2.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt>=1.66.32->finrl) (2.10)\n","Collecting pycares>=4.0.0\n","  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n","\u001b[K     |████████████████████████████████| 291 kB 78.8 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.66.32->finrl) (21.4.0)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.66.32->finrl) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 83.0 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.6.1->ccxt>=1.66.32->finrl) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.66.32->finrl) (2.21)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->finrl) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->finrl) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->finrl) (1.5.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->finrl) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->finrl) (2.8.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->finrl) (0.16.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->finrl) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt>=1.66.32->finrl) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->ccxt>=1.66.32->finrl) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->finrl) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->finrl) (3.1.0)\n","Collecting PyYAML==6.0\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 66.4 MB/s \n","\u001b[?25hCollecting deprecation==2.1.0\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Collecting websocket-client<2,>=0.56.0\n","  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n","\u001b[?25hCollecting websockets<11,>=9.0\n","  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 66.7 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.7/dist-packages (from alpaca-trade-api->finrl) (1.0.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation==2.1.0->alpaca-trade-api->finrl) (21.3)\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 81.6 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from elegantrl->finrl) (1.11.0+cu113)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from elegantrl->finrl) (4.1.2.30)\n","Collecting pybullet\n","  Downloading pybullet-3.2.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n","\u001b[K     |████████████████████████████████| 91.7 MB 112 kB/s \n","\u001b[?25hCollecting pyluach\n","  Downloading pyluach-1.4.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from exchange-calendars->finrl) (0.11.2)\n","Requirement already satisfied: korean_lunar_calendar in /usr/local/lib/python3.7/dist-packages (from exchange-calendars->finrl) (0.2.1)\n","Collecting pymysql>=0.7.6\n","  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.7/dist-packages (from jqdatasdk->finrl) (1.4.36)\n","Collecting thriftpy2>=0.3.9\n","  Downloading thriftpy2-0.4.14.tar.gz (361 kB)\n","\u001b[K     |████████████████████████████████| 361 kB 58.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk->finrl) (4.11.3)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk->finrl) (1.1.2)\n","Collecting ply<4.0,>=3.4\n","  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 5.6 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy>=1.2.8->jqdatasdk->finrl) (3.8.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->finrl) (3.0.8)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->finrl) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->finrl) (0.11.0)\n","Collecting toml\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Collecting nodeenv>=0.11.1\n","  Downloading nodeenv-1.6.0-py2.py3-none-any.whl (21 kB)\n","Collecting identify>=1.0.0\n","  Downloading identify-2.5.0-py2.py3-none-any.whl (98 kB)\n","\u001b[K     |████████████████████████████████| 98 kB 8.2 MB/s \n","\u001b[?25hCollecting virtualenv>=20.0.8\n","  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 55.8 MB/s \n","\u001b[?25hCollecting cfgv>=2.0.0\n","  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit->finrl) (3.6.0)\n","Collecting distlib<1,>=0.3.1\n","  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n","\u001b[K     |████████████████████████████████| 461 kB 75.7 MB/s \n","\u001b[?25hCollecting platformdirs<3,>=2\n","  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n","Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.7/dist-packages (from pyfolio->finrl) (5.5.0)\n","Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pyfolio->finrl) (0.11.2)\n","Collecting empyrical>=0.5.0\n","  Downloading empyrical-0.5.5.tar.gz (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.7/dist-packages (from empyrical>=0.5.0->pyfolio->finrl) (0.9.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (5.1.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=3.2.3->pyfolio->finrl) (4.4.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio->finrl) (4.2.6)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=3.2.3->pyfolio->finrl) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=3.2.3->pyfolio->finrl) (0.7.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->finrl) (1.11.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->finrl) (1.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->finrl) (8.12.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->finrl) (0.7.1)\n","Collecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 41.1 MB/s \n","\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]->finrl) (4.3.3)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]->finrl) (7.1.2)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]->finrl) (3.17.3)\n","Collecting aiohttp-cors\n","  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n","Collecting opencensus\n","  Downloading opencensus-0.9.0-py2.py3-none-any.whl (128 kB)\n","\u001b[K     |████████████████████████████████| 128 kB 83.5 MB/s \n","\u001b[?25hCollecting colorful\n","  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n","\u001b[K     |████████████████████████████████| 201 kB 63.0 MB/s \n","\u001b[?25hCollecting py-spy>=0.2.0\n","  Downloading py_spy-0.3.11-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 61.0 MB/s \n","\u001b[?25hCollecting gpustat>=1.0.0b1\n","  Downloading gpustat-1.0.0b1.tar.gz (82 kB)\n","\u001b[K     |████████████████████████████████| 82 kB 247 kB/s \n","\u001b[?25hRequirement already satisfied: smart-open in /usr/local/lib/python3.7/dist-packages (from ray[tune]->finrl) (6.0.0)\n","Collecting prometheus-client<0.14.0,>=0.7.1\n","  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[tune]->finrl) (7.352.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[tune]->finrl) (5.4.8)\n","Collecting blessed>=1.17.1\n","  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]->finrl) (5.7.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]->finrl) (0.18.1)\n","Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[tune]->finrl) (1.31.5)\n","Collecting opencensus-context>=0.1.2\n","  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n","Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[tune]->finrl) (1.35.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[tune]->finrl) (1.56.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[tune]->finrl) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[tune]->finrl) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[tune]->finrl) (4.2.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[tune]->finrl) (0.4.8)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]->finrl) (0.8.9)\n","Collecting gym>=0.17\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 57.8 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]->finrl) (2.8.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]->finrl) (7.1.2)\n","Collecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Collecting ale-py~=0.7.4\n","  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]->finrl) (4.64.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (1.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (1.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]->finrl) (3.2.0)\n","Collecting mock\n","  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n","Collecting psycopg2-binary\n","  Downloading psycopg2_binary-2.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 52.7 MB/s \n","\u001b[?25hCollecting lxml\n","  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n","\u001b[K     |████████████████████████████████| 6.4 MB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance->finrl) (0.0.10)\n","Collecting requests>=2.18.4\n","  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","Building wheels for collected packages: exchange-calendars, gputil, thriftpy2, pyfolio, empyrical, gpustat, gym, AutoROM.accept-rom-license\n","  Building wheel for exchange-calendars (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for exchange-calendars: filename=exchange_calendars-3.6.1-py3-none-any.whl size=180505 sha256=126a4eed6e6c641ee75d6f3359b8af6a279adddc0ffafd6de7368ed169732938\n","  Stored in directory: /root/.cache/pip/wheels/db/70/a2/5e1d8d0873feb8cb9808f3b55e8f270698b742db54a4b8ad2e\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=313a4ee4d3280c039449fa6f4fc28493bbd8828bb4943535519367432eb82a41\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","  Building wheel for thriftpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for thriftpy2: filename=thriftpy2-0.4.14-cp37-cp37m-linux_x86_64.whl size=944448 sha256=cddf2418696b5312616945c21a1845c182d55cff00d3928cb7260eef9fc44611\n","  Stored in directory: /root/.cache/pip/wheels/2a/f5/49/9c0d851aa64b58db72883cf9393cc824d536bdf13f5c83cff4\n","  Building wheel for pyfolio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyfolio: filename=pyfolio-0.9.2-py3-none-any.whl size=88682 sha256=9738bfd97c4541c91d16ea7e2aab78cd4400aa39ab02aaa9bbb05f914b828915\n","  Stored in directory: /root/.cache/pip/wheels/e4/96/9b/0dfff5453e702fd780a099b7c850521099c5ec0dfafae189f9\n","  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for empyrical: filename=empyrical-0.5.5-py3-none-any.whl size=39780 sha256=88b0105f779e8c9f696bf791ef42cb283ad224cacbc9e1e44a41e2e1396bb4c6\n","  Stored in directory: /root/.cache/pip/wheels/d9/91/4b/654fcff57477efcf149eaca236da2fce991526cbab431bf312\n","  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpustat: filename=gpustat-1.0.0b1-py3-none-any.whl size=15979 sha256=7f803ed38e32ef7f73be0c703346bdb5cca0e3e5086e1fae25221a20cf98d5a8\n","  Stored in directory: /root/.cache/pip/wheels/1a/16/e2/3e2437fba4c4b6a97a97bd96fce5d14e66cff5c4966fb1cc8c\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=e9529c24414050bfdbaaa0aab082fcd133d253347d69359c4858c68b8f519ca3\n","  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=1b2f19be61778eabaf41ddd0de49655cc4f2633fd449ef1115f45384d1d4b958\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built exchange-calendars gputil thriftpy2 pyfolio empyrical gpustat gym AutoROM.accept-rom-license\n","Installing collected packages: setuptools, requests, multidict, frozenlist, yarl, platformdirs, lxml, distlib, asynctest, async-timeout, aiosignal, virtualenv, PyYAML, pycares, ply, opencensus-context, gym, grpcio, blessed, AutoROM.accept-rom-license, autorom, aiohttp, websockets, websocket-client, toml, thriftpy2, tensorboardX, stable-baselines3, ray, pymysql, pyluach, pybullet, py-spy, psycopg2-binary, prometheus-client, opencensus, nodeenv, mock, identify, gpustat, empyrical, deprecation, cryptography, colorful, cfgv, box2d-py, ale-py, aiohttp-cors, aiodns, yfinance, wrds, stockstats, pyfolio, pre-commit, lz4, jqdatasdk, gputil, exchange-calendars, elegantrl, ccxt, alpaca-trade-api, finrl\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.2.6\n","    Uninstalling lxml-4.2.6:\n","      Successfully uninstalled lxml-4.2.6\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.44.0\n","    Uninstalling grpcio-1.44.0:\n","      Successfully uninstalled grpcio-1.44.0\n","  Attempting uninstall: prometheus-client\n","    Found existing installation: prometheus-client 0.14.1\n","    Uninstalling prometheus-client-0.14.1:\n","      Successfully uninstalled prometheus-client-0.14.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed AutoROM.accept-rom-license-0.4.2 PyYAML-6.0 aiodns-3.0.0 aiohttp-3.8.1 aiohttp-cors-0.7.0 aiosignal-1.2.0 ale-py-0.7.5 alpaca-trade-api-2.1.0 async-timeout-4.0.2 asynctest-0.13.0 autorom-0.4.2 blessed-1.19.1 box2d-py-2.3.8 ccxt-1.81.45 cfgv-3.3.1 colorful-0.5.4 cryptography-37.0.2 deprecation-2.1.0 distlib-0.3.4 elegantrl-0.3.3 empyrical-0.5.5 exchange-calendars-3.6.1 finrl-0.3.4 frozenlist-1.3.0 gpustat-1.0.0b1 gputil-1.4.0 grpcio-1.43.0 gym-0.21.0 identify-2.5.0 jqdatasdk-1.8.10 lxml-4.8.0 lz4-4.0.0 mock-4.0.3 multidict-6.0.2 nodeenv-1.6.0 opencensus-0.9.0 opencensus-context-0.1.2 platformdirs-2.5.2 ply-3.11 pre-commit-2.18.1 prometheus-client-0.13.1 psycopg2-binary-2.9.3 py-spy-0.3.11 pybullet-3.2.4 pycares-4.1.2 pyfolio-0.9.2 pyluach-1.4.1 pymysql-1.0.2 ray-1.12.0 requests-2.27.1 setuptools-62.1.0 stable-baselines3-1.5.0 stockstats-0.4.1 tensorboardX-2.5 thriftpy2-0.4.14 toml-0.10.2 virtualenv-20.14.1 websocket-client-1.3.2 websockets-10.3 wrds-3.1.1 yarl-1.7.2 yfinance-0.1.70\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{}}]},{"cell_type":"code","source":["#restart the kernel after running this block.\n","import pkg_resources\n","import pip\n","installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n","\n","!pip install yfinance\n","!pip install pandas\n","!pip install matplotlib\n","!pip install stockstats\n","!pip install gym\n","!pip install stable-baselines[mpi]\n","!pip install tensorflow==1.15.4\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wZxSITSnm8LP","executionInfo":{"status":"ok","timestamp":1651693765796,"user_tz":300,"elapsed":116105,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}},"outputId":"cc5af122-288f-4a47-a295-373d465a1cfa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.70)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n","Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n","Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.8.0)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n","Requirement already satisfied: stockstats in /usr/local/lib/python3.7/dist-packages (0.4.1)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from stockstats) (1.3.5)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->stockstats) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->stockstats) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->stockstats) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->stockstats) (1.15.0)\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.21.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (4.2.0)\n","Collecting stable-baselines[mpi]\n","  Using cached stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n","Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.3.0)\n","Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (0.21.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.1.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]) (1.3.5)\n","Collecting mpi4py\n","  Using cached mpi4py-3.1.3.tar.gz (2.5 MB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (4.11.3)\n","Requirement already satisfied: pyglet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n","Requirement already satisfied: ale-py~=0.7.1 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.7.5)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (5.7.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (3.8.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (3.0.8)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines[mpi]) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]) (2022.1)\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185284 sha256=3f02e19d01bd64bb7d41c581f84a6dd26a708bf528a3669bda23f40c529ee5b9\n","  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n","Successfully built mpi4py\n","Installing collected packages: stable-baselines, mpi4py\n","Successfully installed mpi4py-3.1.3 stable-baselines-2.10.2\n","Collecting tensorflow==1.15.4\n","  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n","\u001b[K     |████████████████████████████████| 110.5 MB 120.2 MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (3.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.15.0)\n","Collecting numpy<1.19.0,>=1.16.0\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[K     |████████████████████████████████| 20.1 MB 962 kB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.14.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (3.17.3)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 37.3 MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.37.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.1.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.2.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 52.1 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.43.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4) (3.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.3.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (62.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (1.0.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=c02ecf1d7b1462583f7085da5089e33dc4da6746e03d60374088c9427776aae4\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: numpy, tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.0\n","    Uninstalling tensorflow-2.8.0:\n","      Successfully uninstalled tensorflow-2.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n","jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.8 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"h2568cp5bU38"},"source":["\n","### 2.2. Check if the additional packages needed are present, if not install them. \n","* Yahoo Finance API\n","* pandas\n","* numpy\n","* matplotlib\n","* stockstats\n","* OpenAI gym\n","* stable-baselines\n","* tensorflow\n","* pyfolio"]},{"cell_type":"markdown","metadata":{"id":"bNmvYN9YbU4B"},"source":["### 2.3. Import Packages"]},{"cell_type":"code","metadata":{"id":"ntfTb0e2bU4C","executionInfo":{"status":"ok","timestamp":1651693790503,"user_tz":300,"elapsed":1349,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.use('Agg')\n","import datetime\n","\n","from finrl.config import config\n","from finrl.marketdata.yahoodownloader import YahooDownloader\n","from finrl.preprocessing.preprocessors import FeatureEngineer\n","from finrl.preprocessing.data import data_split\n","from finrl.env.env_portfolio import StockPortfolioEnv\n","\n","from finrl.model.models import DRLAgent\n","# from finrl.trade.backtest import backtest_stats, backtest_plot, get_daily_return, get_baseline,convert_daily_return_to_pyfolio_ts\n","\n","import sys\n","sys.path.append(\"../FinRL-Library\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OlIS2abxkwan"},"source":["### 2.4. Create Folders"]},{"cell_type":"code","metadata":{"id":"B8bBq7nsBCfF","executionInfo":{"status":"ok","timestamp":1651693790505,"user_tz":300,"elapsed":20,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import os\n","if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n","    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n","if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n","    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n","if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n","    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n","if not os.path.exists(\"./\" + config.RESULTS_DIR):\n","    os.makedirs(\"./\" + config.RESULTS_DIR)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slBria_QbU4F"},"source":["### Part 3. Download Data\n","Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n","* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n","* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPsuy6d9yRPp","outputId":"f5835659-9137-494d-cf7e-b527e3e4573d","executionInfo":{"status":"ok","timestamp":1651693790506,"user_tz":300,"elapsed":20,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["print(config.DOW_30_TICKER)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['AAPL', 'MSFT', 'JPM', 'V', 'RTX', 'PG', 'GS', 'NKE', 'DIS', 'AXP', 'HD', 'INTC', 'WMT', 'IBM', 'MRK', 'UNH', 'KO', 'CAT', 'TRV', 'JNJ', 'CVX', 'MCD', 'VZ', 'CSCO', 'XOM', 'BA', 'MMM', 'PFE', 'WBA', 'DD']\n"]}]},{"cell_type":"code","metadata":{"id":"WEwzMkFHbU4G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98a19b05-9155-4ed0-e846-1a1132ca3daa","executionInfo":{"status":"ok","timestamp":1651693805535,"user_tz":300,"elapsed":15047,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df = YahooDownloader(start_date = '2008-01-01',\n","                     end_date = '2021-01-01',\n","                     ticker_list = config.DOW_30_TICKER).fetch_data()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/finrl/marketdata/yahoodownloader.py:69: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","  data_df = data_df.drop(\"adjcp\", 1)\n"]},{"output_type":"stream","name":"stdout","text":["Shape of DataFrame:  (98167, 8)\n"]}]},{"cell_type":"code","metadata":{"id":"V1xC-LpbbU4f","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"2a5e1db7-3f7d-4661-b131-06b53788c5df","executionInfo":{"status":"ok","timestamp":1651693805555,"user_tz":300,"elapsed":45,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date       open       high        low      close      volume   tic  \\\n","0  2008-01-02   7.116786   7.152143   6.876786   5.958446  1079178800  AAPL   \n","1  2008-01-02  52.090000  52.320000  50.790001  40.477768     8053700   AXP   \n","2  2008-01-02  87.570000  87.839996  86.000000  63.481613     4303000    BA   \n","3  2008-01-02  72.559998  72.669998  70.050003  47.176800     6337800   CAT   \n","4  2008-01-02  27.000000  27.299999  26.209999  19.193113    64338900  CSCO   \n","\n","   day  \n","0    2  \n","1    2  \n","2    2  \n","3    2  \n","4    2  "],"text/html":["\n","  <div id=\"df-bda355d9-1b3d-42f8-a281-6481df5cc653\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2008-01-02</td>\n","      <td>7.116786</td>\n","      <td>7.152143</td>\n","      <td>6.876786</td>\n","      <td>5.958446</td>\n","      <td>1079178800</td>\n","      <td>AAPL</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2008-01-02</td>\n","      <td>52.090000</td>\n","      <td>52.320000</td>\n","      <td>50.790001</td>\n","      <td>40.477768</td>\n","      <td>8053700</td>\n","      <td>AXP</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2008-01-02</td>\n","      <td>87.570000</td>\n","      <td>87.839996</td>\n","      <td>86.000000</td>\n","      <td>63.481613</td>\n","      <td>4303000</td>\n","      <td>BA</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2008-01-02</td>\n","      <td>72.559998</td>\n","      <td>72.669998</td>\n","      <td>70.050003</td>\n","      <td>47.176800</td>\n","      <td>6337800</td>\n","      <td>CAT</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2008-01-02</td>\n","      <td>27.000000</td>\n","      <td>27.299999</td>\n","      <td>26.209999</td>\n","      <td>19.193113</td>\n","      <td>64338900</td>\n","      <td>CSCO</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bda355d9-1b3d-42f8-a281-6481df5cc653')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bda355d9-1b3d-42f8-a281-6481df5cc653 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bda355d9-1b3d-42f8-a281-6481df5cc653');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"mS1-nxRzbU4i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0ce6e12-2dff-4a1b-fc95-bed6a756fb40","executionInfo":{"status":"ok","timestamp":1651693805558,"user_tz":300,"elapsed":44,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(98167, 8)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"V9UwKwzRbU4l"},"source":["### Part 4: Preprocess Data\n","Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n","* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n","* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5h8RbeBHMDQ","outputId":"e1dcc48d-764c-4526-d39f-6397742e49a9","executionInfo":{"status":"ok","timestamp":1651693844161,"user_tz":300,"elapsed":38639,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["fe = FeatureEngineer(\n","                    use_technical_indicator=True,\n","                    use_turbulence=False,\n","                    user_defined_feature = False)\n","\n","df = fe.preprocess_data(df)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully added technical indicators\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zsIW1LAiqCb","outputId":"fe768e83-213e-4b1b-b372-ce9fa14f8db7","executionInfo":{"status":"ok","timestamp":1651693844162,"user_tz":300,"elapsed":70,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(98167, 16)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"lWB-RoTSbU4s","colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"a5c8805e-90b7-4190-92f3-47c42efc1b6c","executionInfo":{"status":"ok","timestamp":1651693844162,"user_tz":300,"elapsed":63,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date       open       high        low      close      volume   tic  \\\n","0  2008-01-02   7.116786   7.152143   6.876786   5.958446  1079178800  AAPL   \n","1  2008-01-02  52.090000  52.320000  50.790001  40.477768     8053700   AXP   \n","2  2008-01-02  87.570000  87.839996  86.000000  63.481613     4303000    BA   \n","3  2008-01-02  72.559998  72.669998  70.050003  47.176800     6337800   CAT   \n","4  2008-01-02  27.000000  27.299999  26.209999  19.193113    64338900  CSCO   \n","\n","   day      macd   boll_ub   boll_lb      rsi_30      cci_30  dx_30  \\\n","0    2  0.000000  5.963713  5.955930  100.000000  -66.666667  100.0   \n","1    2  0.000062  5.963713  5.955930  100.000000  -66.666667  100.0   \n","2    2 -0.014032  6.332461  5.284733    0.581177 -100.000000  100.0   \n","3    2 -0.022760  6.284159  5.144961    0.498192  -98.367616  100.0   \n","4    2 -0.034799  6.271556  4.966554    0.357986  -86.822816  100.0   \n","\n","   close_30_sma  close_60_sma  \n","0      5.958446      5.958446  \n","1      5.959821      5.959821  \n","2      5.808597      5.808597  \n","3      5.714560      5.714560  \n","4      5.619055      5.619055  "],"text/html":["\n","  <div id=\"df-89bb3a25-d93f-4f3b-bfc1-8491facc9696\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>boll_ub</th>\n","      <th>boll_lb</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>close_30_sma</th>\n","      <th>close_60_sma</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2008-01-02</td>\n","      <td>7.116786</td>\n","      <td>7.152143</td>\n","      <td>6.876786</td>\n","      <td>5.958446</td>\n","      <td>1079178800</td>\n","      <td>AAPL</td>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>5.963713</td>\n","      <td>5.955930</td>\n","      <td>100.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>5.958446</td>\n","      <td>5.958446</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2008-01-02</td>\n","      <td>52.090000</td>\n","      <td>52.320000</td>\n","      <td>50.790001</td>\n","      <td>40.477768</td>\n","      <td>8053700</td>\n","      <td>AXP</td>\n","      <td>2</td>\n","      <td>0.000062</td>\n","      <td>5.963713</td>\n","      <td>5.955930</td>\n","      <td>100.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>5.959821</td>\n","      <td>5.959821</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2008-01-02</td>\n","      <td>87.570000</td>\n","      <td>87.839996</td>\n","      <td>86.000000</td>\n","      <td>63.481613</td>\n","      <td>4303000</td>\n","      <td>BA</td>\n","      <td>2</td>\n","      <td>-0.014032</td>\n","      <td>6.332461</td>\n","      <td>5.284733</td>\n","      <td>0.581177</td>\n","      <td>-100.000000</td>\n","      <td>100.0</td>\n","      <td>5.808597</td>\n","      <td>5.808597</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2008-01-02</td>\n","      <td>72.559998</td>\n","      <td>72.669998</td>\n","      <td>70.050003</td>\n","      <td>47.176800</td>\n","      <td>6337800</td>\n","      <td>CAT</td>\n","      <td>2</td>\n","      <td>-0.022760</td>\n","      <td>6.284159</td>\n","      <td>5.144961</td>\n","      <td>0.498192</td>\n","      <td>-98.367616</td>\n","      <td>100.0</td>\n","      <td>5.714560</td>\n","      <td>5.714560</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2008-01-02</td>\n","      <td>27.000000</td>\n","      <td>27.299999</td>\n","      <td>26.209999</td>\n","      <td>19.193113</td>\n","      <td>64338900</td>\n","      <td>CSCO</td>\n","      <td>2</td>\n","      <td>-0.034799</td>\n","      <td>6.271556</td>\n","      <td>4.966554</td>\n","      <td>0.357986</td>\n","      <td>-86.822816</td>\n","      <td>100.0</td>\n","      <td>5.619055</td>\n","      <td>5.619055</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89bb3a25-d93f-4f3b-bfc1-8491facc9696')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-89bb3a25-d93f-4f3b-bfc1-8491facc9696 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-89bb3a25-d93f-4f3b-bfc1-8491facc9696');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"qz9K2vul6RmK"},"source":["#### Add covariance matrix as states"]},{"cell_type":"code","metadata":{"id":"IhizvNwcrg1n","executionInfo":{"status":"ok","timestamp":1651693887879,"user_tz":300,"elapsed":43777,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# add covariance matrix as states\n","df=df.sort_values(['date','tic'],ignore_index=True)\n","df.index = df.date.factorize()[0]\n","\n","cov_list = []\n","# look back is one year\n","lookback=252\n","for i in range(lookback,len(df.index.unique())):\n","  data_lookback = df.loc[i-lookback:i,:]\n","  price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n","  return_lookback = price_lookback.pct_change().dropna()\n","  covs = return_lookback.cov().values \n","  cov_list.append(covs)\n","  \n","df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list})\n","df = df.merge(df_cov, on='date')\n","df = df.sort_values(['date','tic']).reset_index(drop=True)\n","        "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUPwwa13uBQ-","outputId":"14df1e5c-8f33-48b8-c412-ae826d22374c","executionInfo":{"status":"ok","timestamp":1651693887880,"user_tz":300,"elapsed":61,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(90660, 17)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"wv3jR1zPrg4g","outputId":"118e103e-7723-4546-d987-ecb87a6191b4","executionInfo":{"status":"ok","timestamp":1651693887881,"user_tz":300,"elapsed":51,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.head()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date       open       high        low      close     volume   tic  \\\n","0  2008-12-31   3.070357   3.133571   3.047857   2.610106  607541200  AAPL   \n","1  2008-12-31  17.969999  18.750000  17.910000  14.908461    9625600   AXP   \n","2  2008-12-31  41.590000  43.049999  41.500000  32.005890    5443100    BA   \n","3  2008-12-31  43.700001  45.099998  43.700001  30.628830    6277400   CAT   \n","4  2008-12-31  16.180000  16.549999  16.120001  11.787783   37513700  CSCO   \n","\n","   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n","0    2  0.112284  53.286010  47.561928  48.009264   25.841097   5.858025   \n","1    2  0.225514  53.469343  47.791637  51.045376   80.049007   0.220406   \n","2    2  0.322032  53.717287  47.766476  51.269636   96.444484   2.969231   \n","3    2  0.327177  53.791300  47.768895  49.751677   71.687267   0.296878   \n","4    2  0.511683  54.318893  47.618567  53.664770  172.385044  17.964888   \n","\n","   close_30_sma  close_60_sma  \\\n","0     50.614906     50.970385   \n","1     50.690793     50.894581   \n","2     50.754755     50.823502   \n","3     50.778334     50.740349   \n","4     50.874006     50.731082   \n","\n","                                            cov_list  \n","0  [[0.0014139100106232375, 0.0011800737903184346...  \n","1  [[0.0014139100106232375, 0.0011800737903184346...  \n","2  [[0.0014139100106232375, 0.0011800737903184346...  \n","3  [[0.0014139100106232375, 0.0011800737903184346...  \n","4  [[0.0014139100106232375, 0.0011800737903184346...  "],"text/html":["\n","  <div id=\"df-9e342eb5-2ff3-4f9b-a3ba-c04d896409b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>boll_ub</th>\n","      <th>boll_lb</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>close_30_sma</th>\n","      <th>close_60_sma</th>\n","      <th>cov_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2008-12-31</td>\n","      <td>3.070357</td>\n","      <td>3.133571</td>\n","      <td>3.047857</td>\n","      <td>2.610106</td>\n","      <td>607541200</td>\n","      <td>AAPL</td>\n","      <td>2</td>\n","      <td>0.112284</td>\n","      <td>53.286010</td>\n","      <td>47.561928</td>\n","      <td>48.009264</td>\n","      <td>25.841097</td>\n","      <td>5.858025</td>\n","      <td>50.614906</td>\n","      <td>50.970385</td>\n","      <td>[[0.0014139100106232375, 0.0011800737903184346...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2008-12-31</td>\n","      <td>17.969999</td>\n","      <td>18.750000</td>\n","      <td>17.910000</td>\n","      <td>14.908461</td>\n","      <td>9625600</td>\n","      <td>AXP</td>\n","      <td>2</td>\n","      <td>0.225514</td>\n","      <td>53.469343</td>\n","      <td>47.791637</td>\n","      <td>51.045376</td>\n","      <td>80.049007</td>\n","      <td>0.220406</td>\n","      <td>50.690793</td>\n","      <td>50.894581</td>\n","      <td>[[0.0014139100106232375, 0.0011800737903184346...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2008-12-31</td>\n","      <td>41.590000</td>\n","      <td>43.049999</td>\n","      <td>41.500000</td>\n","      <td>32.005890</td>\n","      <td>5443100</td>\n","      <td>BA</td>\n","      <td>2</td>\n","      <td>0.322032</td>\n","      <td>53.717287</td>\n","      <td>47.766476</td>\n","      <td>51.269636</td>\n","      <td>96.444484</td>\n","      <td>2.969231</td>\n","      <td>50.754755</td>\n","      <td>50.823502</td>\n","      <td>[[0.0014139100106232375, 0.0011800737903184346...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2008-12-31</td>\n","      <td>43.700001</td>\n","      <td>45.099998</td>\n","      <td>43.700001</td>\n","      <td>30.628830</td>\n","      <td>6277400</td>\n","      <td>CAT</td>\n","      <td>2</td>\n","      <td>0.327177</td>\n","      <td>53.791300</td>\n","      <td>47.768895</td>\n","      <td>49.751677</td>\n","      <td>71.687267</td>\n","      <td>0.296878</td>\n","      <td>50.778334</td>\n","      <td>50.740349</td>\n","      <td>[[0.0014139100106232375, 0.0011800737903184346...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2008-12-31</td>\n","      <td>16.180000</td>\n","      <td>16.549999</td>\n","      <td>16.120001</td>\n","      <td>11.787783</td>\n","      <td>37513700</td>\n","      <td>CSCO</td>\n","      <td>2</td>\n","      <td>0.511683</td>\n","      <td>54.318893</td>\n","      <td>47.618567</td>\n","      <td>53.664770</td>\n","      <td>172.385044</td>\n","      <td>17.964888</td>\n","      <td>50.874006</td>\n","      <td>50.731082</td>\n","      <td>[[0.0014139100106232375, 0.0011800737903184346...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e342eb5-2ff3-4f9b-a3ba-c04d896409b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9e342eb5-2ff3-4f9b-a3ba-c04d896409b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9e342eb5-2ff3-4f9b-a3ba-c04d896409b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"UooHj1OgbU4v"},"source":["### Part 5. Design the Environment\n","Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n","\n","Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n","\n","The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."]},{"cell_type":"markdown","metadata":{"id":"MQnmN1qdk88I"},"source":["### Training data split: 2009-01-01 to 2018-12-31"]},{"cell_type":"code","metadata":{"id":"NrPxgv4eBQ_R","executionInfo":{"status":"ok","timestamp":1651693887884,"user_tz":300,"elapsed":53,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["train = data_split(df, '2009-01-01','2019-01-01')\n","#trade = data_split(df, '2020-01-01', config.END_DATE)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"vU2vXEll0hfk","outputId":"1b0f0bf1-90b1-41f7-d70e-43c35237c02d","executionInfo":{"status":"ok","timestamp":1651693888286,"user_tz":300,"elapsed":454,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["train.head()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date       open       high        low      close     volume   tic  \\\n","0  2009-01-02   3.067143   3.251429   3.041429   2.775245  746015200  AAPL   \n","0  2009-01-02  18.570000  19.520000  18.400000  15.535345   10955700   AXP   \n","0  2009-01-02  42.799999  45.560001  42.779999  33.941093    7010200    BA   \n","0  2009-01-02  44.910000  46.980000  44.709999  32.164722    7117200   CAT   \n","0  2009-01-02  16.410000  17.000000  16.250000  12.265082   40980600  CSCO   \n","\n","   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n","0    4  0.916408  58.037312  50.483559  58.279405  231.362655  28.830569   \n","0    4  1.104876  58.632919  50.354921  57.951212  206.221387  31.974391   \n","0    4  1.225587  59.109107  50.287863  57.599736  171.846935  31.974391   \n","0    4  1.288570  59.477773  50.242410  57.159556  138.088238  22.759063   \n","0    4  1.295163  59.737544  50.442498  56.448857  117.946012  22.759063   \n","\n","   close_30_sma  close_60_sma  \\\n","0     53.921121     52.268014   \n","0     54.109571     52.400182   \n","0     54.287686     52.521220   \n","0     54.486351     52.632342   \n","0     54.596317     52.735161   \n","\n","                                            cov_list  \n","0  [[0.0014277082780051412, 0.0011886545510691592...  \n","0  [[0.0014277082780051412, 0.0011886545510691592...  \n","0  [[0.0014277082780051412, 0.0011886545510691592...  \n","0  [[0.0014277082780051412, 0.0011886545510691592...  \n","0  [[0.0014277082780051412, 0.0011886545510691592...  "],"text/html":["\n","  <div id=\"df-c85a0f33-2745-4727-86c7-677c921c123d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>boll_ub</th>\n","      <th>boll_lb</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>close_30_sma</th>\n","      <th>close_60_sma</th>\n","      <th>cov_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2009-01-02</td>\n","      <td>3.067143</td>\n","      <td>3.251429</td>\n","      <td>3.041429</td>\n","      <td>2.775245</td>\n","      <td>746015200</td>\n","      <td>AAPL</td>\n","      <td>4</td>\n","      <td>0.916408</td>\n","      <td>58.037312</td>\n","      <td>50.483559</td>\n","      <td>58.279405</td>\n","      <td>231.362655</td>\n","      <td>28.830569</td>\n","      <td>53.921121</td>\n","      <td>52.268014</td>\n","      <td>[[0.0014277082780051412, 0.0011886545510691592...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2009-01-02</td>\n","      <td>18.570000</td>\n","      <td>19.520000</td>\n","      <td>18.400000</td>\n","      <td>15.535345</td>\n","      <td>10955700</td>\n","      <td>AXP</td>\n","      <td>4</td>\n","      <td>1.104876</td>\n","      <td>58.632919</td>\n","      <td>50.354921</td>\n","      <td>57.951212</td>\n","      <td>206.221387</td>\n","      <td>31.974391</td>\n","      <td>54.109571</td>\n","      <td>52.400182</td>\n","      <td>[[0.0014277082780051412, 0.0011886545510691592...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2009-01-02</td>\n","      <td>42.799999</td>\n","      <td>45.560001</td>\n","      <td>42.779999</td>\n","      <td>33.941093</td>\n","      <td>7010200</td>\n","      <td>BA</td>\n","      <td>4</td>\n","      <td>1.225587</td>\n","      <td>59.109107</td>\n","      <td>50.287863</td>\n","      <td>57.599736</td>\n","      <td>171.846935</td>\n","      <td>31.974391</td>\n","      <td>54.287686</td>\n","      <td>52.521220</td>\n","      <td>[[0.0014277082780051412, 0.0011886545510691592...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2009-01-02</td>\n","      <td>44.910000</td>\n","      <td>46.980000</td>\n","      <td>44.709999</td>\n","      <td>32.164722</td>\n","      <td>7117200</td>\n","      <td>CAT</td>\n","      <td>4</td>\n","      <td>1.288570</td>\n","      <td>59.477773</td>\n","      <td>50.242410</td>\n","      <td>57.159556</td>\n","      <td>138.088238</td>\n","      <td>22.759063</td>\n","      <td>54.486351</td>\n","      <td>52.632342</td>\n","      <td>[[0.0014277082780051412, 0.0011886545510691592...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2009-01-02</td>\n","      <td>16.410000</td>\n","      <td>17.000000</td>\n","      <td>16.250000</td>\n","      <td>12.265082</td>\n","      <td>40980600</td>\n","      <td>CSCO</td>\n","      <td>4</td>\n","      <td>1.295163</td>\n","      <td>59.737544</td>\n","      <td>50.442498</td>\n","      <td>56.448857</td>\n","      <td>117.946012</td>\n","      <td>22.759063</td>\n","      <td>54.596317</td>\n","      <td>52.735161</td>\n","      <td>[[0.0014277082780051412, 0.0011886545510691592...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c85a0f33-2745-4727-86c7-677c921c123d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c85a0f33-2745-4727-86c7-677c921c123d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c85a0f33-2745-4727-86c7-677c921c123d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"sxQTNjpblAMN"},"source":["### Environment for Portfolio Allocation\n"]},{"cell_type":"code","metadata":{"id":"xlfE-VERbU40","executionInfo":{"status":"ok","timestamp":1651693888302,"user_tz":300,"elapsed":57,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import numpy as np\n","import pandas as pd\n","from gym.utils import seeding\n","import gym\n","from gym import spaces\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","\n","\n","class StockPortfolioEnv(gym.Env):\n","    \"\"\"A single stock trading environment for OpenAI gym\n","\n","    Attributes\n","    ----------\n","        df: DataFrame\n","            input data\n","        stock_dim : int\n","            number of unique stocks\n","        hmax : int\n","            maximum number of shares to trade\n","        initial_amount : int\n","            start money\n","        transaction_cost_pct: float\n","            transaction cost percentage per trade\n","        reward_scaling: float\n","            scaling factor for reward, good for training\n","        state_space: int\n","            the dimension of input features\n","        action_space: int\n","            equals stock dimension\n","        tech_indicator_list: list\n","            a list of technical indicator names\n","        turbulence_threshold: int\n","            a threshold to control risk aversion\n","        day: int\n","            an increment number to control date\n","\n","    Methods\n","    -------\n","    _sell_stock()\n","        perform sell action based on the sign of the action\n","    _buy_stock()\n","        perform buy action based on the sign of the action\n","    step()\n","        at each step the agent will return actions, then \n","        we will calculate the reward, and return the next observation.\n","    reset()\n","        reset the environment\n","    render()\n","        use render to return other functions\n","    save_asset_memory()\n","        return account value at each time step\n","    save_action_memory()\n","        return actions/positions at each time step\n","        \n","\n","    \"\"\"\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self, \n","                df,\n","                stock_dim,\n","                hmax,\n","                initial_amount,\n","                transaction_cost_pct,\n","                reward_scaling,\n","                state_space,\n","                action_space,\n","                tech_indicator_list,\n","                turbulence_threshold=None,\n","                lookback=252,\n","                day = 0):\n","        #super(StockEnv, self).__init__()\n","        #money = 10 , scope = 1\n","        self.day = day\n","        self.lookback=lookback\n","        self.df = df\n","        self.stock_dim = stock_dim\n","        self.hmax = hmax\n","        self.initial_amount = initial_amount\n","        self.transaction_cost_pct =transaction_cost_pct\n","        self.reward_scaling = reward_scaling\n","        self.state_space = state_space\n","        self.action_space = action_space\n","        self.tech_indicator_list = tech_indicator_list\n","\n","        # action_space normalization and shape is self.stock_dim\n","        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,)) \n","        # Shape = (34, 30)\n","        # covariance matrix + technical indicators\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n","\n","        # load data from a pandas dataframe\n","        self.data = self.df.loc[self.day,:]\n","        self.covs = self.data['cov_list'].values[0]\n","        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n","        self.terminal = False     \n","        self.turbulence_threshold = turbulence_threshold        \n","        # initalize state: inital portfolio return + individual stock return + individual weights\n","        self.portfolio_value = self.initial_amount\n","\n","        # memorize portfolio value each step\n","        self.asset_memory = [self.initial_amount]\n","        # memorize portfolio return each step\n","        self.portfolio_return_memory = [0]\n","        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n","        self.date_memory=[self.data.date.unique()[0]]\n","\n","        \n","    def step(self, actions):\n","        # print(self.day)\n","        self.terminal = self.day >= len(self.df.index.unique())-1\n","        # print(actions)\n","\n","        if self.terminal:\n","            df = pd.DataFrame(self.portfolio_return_memory)\n","            df.columns = ['daily_return']\n","            plt.plot(df.daily_return.cumsum(),'r')\n","            plt.savefig('results/cumulative_reward.png')\n","            plt.close()\n","            \n","            plt.plot(self.portfolio_return_memory,'r')\n","            plt.savefig('results/rewards.png')\n","            plt.close()\n","\n","            print(\"=================================\")\n","            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))           \n","            print(\"end_total_asset:{}\".format(self.portfolio_value))\n","\n","            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n","            df_daily_return.columns = ['daily_return']\n","            if df_daily_return['daily_return'].std() !=0:\n","              sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n","                       df_daily_return['daily_return'].std()\n","              print(\"Sharpe: \",sharpe)\n","            print(\"=================================\")\n","            \n","            return self.state, self.reward, self.terminal,{}\n","\n","        else:\n","            #print(\"Model actions: \",actions)\n","            # actions are the portfolio weight\n","            # normalize to sum of 1\n","            #if (np.array(actions) - np.array(actions).min()).sum() != 0:\n","            #  norm_actions = (np.array(actions) - np.array(actions).min()) / (np.array(actions) - np.array(actions).min()).sum()\n","            #else:\n","            #  norm_actions = actions\n","            weights = self.softmax_normalization(actions) \n","            #print(\"Normalized actions: \", weights)\n","            self.actions_memory.append(weights)\n","            last_day_memory = self.data\n","\n","            #load next state\n","            self.day += 1\n","            self.data = self.df.loc[self.day,:]\n","            self.covs = self.data['cov_list'].values[0]\n","            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n","            #print(self.state)\n","            # calcualte portfolio return\n","            # individual stocks' return * weight\n","            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n","            # update portfolio value\n","            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n","            self.portfolio_value = new_portfolio_value\n","\n","            # save into memory\n","            self.portfolio_return_memory.append(portfolio_return)\n","            self.date_memory.append(self.data.date.unique()[0])            \n","            self.asset_memory.append(new_portfolio_value)\n","\n","            # the reward is the new portfolio value or end portfolo value\n","            self.reward = new_portfolio_value \n","            #print(\"Step reward: \", self.reward)\n","            #self.reward = self.reward*self.reward_scaling\n","\n","        return self.state, self.reward, self.terminal, {}\n","\n","    def reset(self):\n","        self.asset_memory = [self.initial_amount]\n","        self.day = 0\n","        self.data = self.df.loc[self.day,:]\n","        # load states\n","        self.covs = self.data['cov_list'].values[0]\n","        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n","        self.portfolio_value = self.initial_amount\n","        #self.cost = 0\n","        #self.trades = 0\n","        self.terminal = False \n","        self.portfolio_return_memory = [0]\n","        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n","        self.date_memory=[self.data.date.unique()[0]] \n","        return self.state\n","    \n","    def render(self, mode='human'):\n","        return self.state\n","        \n","    def softmax_normalization(self, actions):\n","        numerator = np.exp(actions)\n","        denominator = np.sum(np.exp(actions))\n","        softmax_output = numerator/denominator\n","        return softmax_output\n","\n","    \n","    def save_asset_memory(self):\n","        date_list = self.date_memory\n","        portfolio_return = self.portfolio_return_memory\n","        #print(len(date_list))\n","        #print(len(asset_list))\n","        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n","        return df_account_value\n","\n","    def save_action_memory(self):\n","        # date and close price length must match actions length\n","        date_list = self.date_memory\n","        df_date = pd.DataFrame(date_list)\n","        df_date.columns = ['date']\n","        \n","        action_list = self.actions_memory\n","        df_actions = pd.DataFrame(action_list)\n","        df_actions.columns = self.data.tic.values\n","        df_actions.index = df_date.date\n","        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n","        return df_actions\n","\n","    def _seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def get_sb_env(self):\n","        e = DummyVecEnv([lambda: self])\n","        obs = e.reset()\n","        return e, obs"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzD06X0CbU43","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46146e6e-51ef-4de2-f2c8-6ad5aefaa71e","executionInfo":{"status":"ok","timestamp":1651693888303,"user_tz":300,"elapsed":57,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["stock_dimension = len(train.tic.unique())\n","state_space = stock_dimension\n","print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Stock Dimension: 30, State Space: 30\n"]}]},{"cell_type":"code","metadata":{"id":"jyg0_ZuVEVQ5","executionInfo":{"status":"ok","timestamp":1651693888304,"user_tz":300,"elapsed":50,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["env_kwargs = {\n","    \"hmax\": 100, \n","    \"initial_amount\": 1000000, \n","    \"transaction_cost_pct\": 0.001, \n","    \"state_space\": state_space, \n","    \"stock_dim\": stock_dimension, \n","    \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n","    \"action_space\": stock_dimension, \n","    \"reward_scaling\": 1e-4\n","    \n","}\n","\n","e_train_gym = StockPortfolioEnv(df = train, **env_kwargs)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTlOf8SJGdkl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e8de7c9-6f67-45ce-88da-9b75d835dd86","executionInfo":{"status":"ok","timestamp":1651693888304,"user_tz":300,"elapsed":49,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["env_train, _ = e_train_gym.get_sb_env()\n","print(type(env_train))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"2eKIu5UPlPnk"},"source":["### Part 6: Implement DRL Algorithms\n","* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n","* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n","Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n","design their own DRL algorithms by adapting these DRL algorithms."]},{"cell_type":"code","metadata":{"id":"VDxU0iCEGdnb","executionInfo":{"status":"ok","timestamp":1651693888305,"user_tz":300,"elapsed":42,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# initialize\n","agent = DRLAgent(env = env_train)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdPe8uzflbXe"},"source":["### Model 1: **A2C** (Actor Critic)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1tORf1fIcQ2","outputId":"3f9968b2-0554-463c-e4b1-ef049e008860","executionInfo":{"status":"ok","timestamp":1651693888306,"user_tz":300,"elapsed":42,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["agent = DRLAgent(env = env_train)\n","\n","A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n","model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n","Using cpu device\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DazEdrMpIdyz","outputId":"dcbdefcb-35c9-4751-fbe4-a382e41b60cd","executionInfo":{"status":"ok","timestamp":1651694092412,"user_tz":300,"elapsed":204141,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["trained_a2c = agent.train_model(model=model_a2c, \n","                                tb_log_name='a2c',\n","                                total_timesteps=60000)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to tensorboard_log/a2c/a2c_1\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 174      |\n","|    iterations         | 100      |\n","|    time_elapsed       | 2        |\n","|    total_timesteps    | 500      |\n","| train/                |          |\n","|    entropy_loss       | -42.5    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 99       |\n","|    policy_loss        | 2.03e+08 |\n","|    std                | 0.997    |\n","|    value_loss         | 2.48e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 224      |\n","|    iterations         | 200      |\n","|    time_elapsed       | 4        |\n","|    total_timesteps    | 1000     |\n","| train/                |          |\n","|    entropy_loss       | -42.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 199      |\n","|    policy_loss        | 2.5e+08  |\n","|    std                | 0.997    |\n","|    value_loss         | 4.08e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 247      |\n","|    iterations         | 300      |\n","|    time_elapsed       | 6        |\n","|    total_timesteps    | 1500     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 299      |\n","|    policy_loss        | 3.54e+08 |\n","|    std                | 0.996    |\n","|    value_loss         | 9.13e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 262      |\n","|    iterations         | 400      |\n","|    time_elapsed       | 7        |\n","|    total_timesteps    | 2000     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 1.79e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 399      |\n","|    policy_loss        | 4.28e+08 |\n","|    std                | 0.996    |\n","|    value_loss         | 1.23e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 272      |\n","|    iterations         | 500      |\n","|    time_elapsed       | 9        |\n","|    total_timesteps    | 2500     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 499      |\n","|    policy_loss        | 6.31e+08 |\n","|    std                | 0.995    |\n","|    value_loss         | 2.49e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4563857.487294036\n","Sharpe:  1.0298489859080961\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 273      |\n","|    iterations         | 600      |\n","|    time_elapsed       | 10       |\n","|    total_timesteps    | 3000     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 599      |\n","|    policy_loss        | 1.8e+08  |\n","|    std                | 0.994    |\n","|    value_loss         | 2.35e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 278      |\n","|    iterations         | 700      |\n","|    time_elapsed       | 12       |\n","|    total_timesteps    | 3500     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 699      |\n","|    policy_loss        | 2.61e+08 |\n","|    std                | 0.994    |\n","|    value_loss         | 3.91e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 282       |\n","|    iterations         | 800       |\n","|    time_elapsed       | 14        |\n","|    total_timesteps    | 4000      |\n","| train/                |           |\n","|    entropy_loss       | -42.4     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 799       |\n","|    policy_loss        | 3.87e+08  |\n","|    std                | 0.994     |\n","|    value_loss         | 9.79e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 285      |\n","|    iterations         | 900      |\n","|    time_elapsed       | 15       |\n","|    total_timesteps    | 4500     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 899      |\n","|    policy_loss        | 4.35e+08 |\n","|    std                | 0.993    |\n","|    value_loss         | 1.26e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 287      |\n","|    iterations         | 1000     |\n","|    time_elapsed       | 17       |\n","|    total_timesteps    | 5000     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 999      |\n","|    policy_loss        | 6.09e+08 |\n","|    std                | 0.993    |\n","|    value_loss         | 2.44e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4489309.757719453\n","Sharpe:  1.0213511920217921\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 286      |\n","|    iterations         | 1100     |\n","|    time_elapsed       | 19       |\n","|    total_timesteps    | 5500     |\n","| train/                |          |\n","|    entropy_loss       | -42.4    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1099     |\n","|    policy_loss        | 1.94e+08 |\n","|    std                | 0.993    |\n","|    value_loss         | 2.53e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 289      |\n","|    iterations         | 1200     |\n","|    time_elapsed       | 20       |\n","|    total_timesteps    | 6000     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1199     |\n","|    policy_loss        | 2.75e+08 |\n","|    std                | 0.992    |\n","|    value_loss         | 4.21e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 291      |\n","|    iterations         | 1300     |\n","|    time_elapsed       | 22       |\n","|    total_timesteps    | 6500     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1299     |\n","|    policy_loss        | 3.49e+08 |\n","|    std                | 0.992    |\n","|    value_loss         | 8.77e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 293      |\n","|    iterations         | 1400     |\n","|    time_elapsed       | 23       |\n","|    total_timesteps    | 7000     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1399     |\n","|    policy_loss        | 4.14e+08 |\n","|    std                | 0.991    |\n","|    value_loss         | 1.21e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 294      |\n","|    iterations         | 1500     |\n","|    time_elapsed       | 25       |\n","|    total_timesteps    | 7500     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 1.79e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1499     |\n","|    policy_loss        | 5.71e+08 |\n","|    std                | 0.991    |\n","|    value_loss         | 2.51e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4555702.450616166\n","Sharpe:  1.0274096420325012\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 294      |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 27       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | 1.74e+08 |\n","|    std                | 0.99     |\n","|    value_loss         | 2.14e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 295      |\n","|    iterations         | 1700     |\n","|    time_elapsed       | 28       |\n","|    total_timesteps    | 8500     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1699     |\n","|    policy_loss        | 2.4e+08  |\n","|    std                | 0.99     |\n","|    value_loss         | 3.83e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 1800     |\n","|    time_elapsed       | 30       |\n","|    total_timesteps    | 9000     |\n","| train/                |          |\n","|    entropy_loss       | -42.3    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1799     |\n","|    policy_loss        | 3.41e+08 |\n","|    std                | 0.99     |\n","|    value_loss         | 7.91e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 298      |\n","|    iterations         | 1900     |\n","|    time_elapsed       | 31       |\n","|    total_timesteps    | 9500     |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1899     |\n","|    policy_loss        | 3.86e+08 |\n","|    std                | 0.989    |\n","|    value_loss         | 1.1e+14  |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 299      |\n","|    iterations         | 2000     |\n","|    time_elapsed       | 33       |\n","|    total_timesteps    | 10000    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 1999     |\n","|    policy_loss        | 5.97e+08 |\n","|    std                | 0.989    |\n","|    value_loss         | 2.46e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4381278.908766086\n","Sharpe:  1.0040397666586192\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 2100     |\n","|    time_elapsed       | 35       |\n","|    total_timesteps    | 10500    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2099     |\n","|    policy_loss        | 1.79e+08 |\n","|    std                | 0.989    |\n","|    value_loss         | 2.2e+13  |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 298      |\n","|    iterations         | 2200     |\n","|    time_elapsed       | 36       |\n","|    total_timesteps    | 11000    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2199     |\n","|    policy_loss        | 2.54e+08 |\n","|    std                | 0.989    |\n","|    value_loss         | 4.24e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 299      |\n","|    iterations         | 2300     |\n","|    time_elapsed       | 38       |\n","|    total_timesteps    | 11500    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2299     |\n","|    policy_loss        | 3.9e+08  |\n","|    std                | 0.988    |\n","|    value_loss         | 9.32e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 300      |\n","|    iterations         | 2400     |\n","|    time_elapsed       | 39       |\n","|    total_timesteps    | 12000    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2399     |\n","|    policy_loss        | 4.55e+08 |\n","|    std                | 0.988    |\n","|    value_loss         | 1.44e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 301      |\n","|    iterations         | 2500     |\n","|    time_elapsed       | 41       |\n","|    total_timesteps    | 12500    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2499     |\n","|    policy_loss        | 6.97e+08 |\n","|    std                | 0.987    |\n","|    value_loss         | 2.99e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4945435.185485299\n","Sharpe:  1.0795813390130042\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 300      |\n","|    iterations         | 2600     |\n","|    time_elapsed       | 43       |\n","|    total_timesteps    | 13000    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2599     |\n","|    policy_loss        | 1.45e+08 |\n","|    std                | 0.987    |\n","|    value_loss         | 1.98e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 301      |\n","|    iterations         | 2700     |\n","|    time_elapsed       | 44       |\n","|    total_timesteps    | 13500    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2699     |\n","|    policy_loss        | 2.42e+08 |\n","|    std                | 0.987    |\n","|    value_loss         | 4.19e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 301      |\n","|    iterations         | 2800     |\n","|    time_elapsed       | 46       |\n","|    total_timesteps    | 14000    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2799     |\n","|    policy_loss        | 3.84e+08 |\n","|    std                | 0.987    |\n","|    value_loss         | 8.78e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 302      |\n","|    iterations         | 2900     |\n","|    time_elapsed       | 47       |\n","|    total_timesteps    | 14500    |\n","| train/                |          |\n","|    entropy_loss       | -42.2    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2899     |\n","|    policy_loss        | 4.58e+08 |\n","|    std                | 0.987    |\n","|    value_loss         | 1.32e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 3000     |\n","|    time_elapsed       | 49       |\n","|    total_timesteps    | 15000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 2999     |\n","|    policy_loss        | 5.87e+08 |\n","|    std                | 0.986    |\n","|    value_loss         | 2.41e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4491956.875813438\n","Sharpe:  1.020252855153643\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 302       |\n","|    iterations         | 3100      |\n","|    time_elapsed       | 51        |\n","|    total_timesteps    | 15500     |\n","| train/                |           |\n","|    entropy_loss       | -42.1     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 3099      |\n","|    policy_loss        | 1.52e+08  |\n","|    std                | 0.986     |\n","|    value_loss         | 1.98e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 302      |\n","|    iterations         | 3200     |\n","|    time_elapsed       | 52       |\n","|    total_timesteps    | 16000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 3199     |\n","|    policy_loss        | 2.27e+08 |\n","|    std                | 0.986    |\n","|    value_loss         | 3.44e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 3300      |\n","|    time_elapsed       | 54        |\n","|    total_timesteps    | 16500     |\n","| train/                |           |\n","|    entropy_loss       | -42.1     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 3299      |\n","|    policy_loss        | 3.44e+08  |\n","|    std                | 0.985     |\n","|    value_loss         | 8.12e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 3400     |\n","|    time_elapsed       | 55       |\n","|    total_timesteps    | 17000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 3399     |\n","|    policy_loss        | 4.13e+08 |\n","|    std                | 0.985    |\n","|    value_loss         | 1.16e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 3500     |\n","|    time_elapsed       | 57       |\n","|    total_timesteps    | 17500    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 3499     |\n","|    policy_loss        | 6.18e+08 |\n","|    std                | 0.985    |\n","|    value_loss         | 2.2e+14  |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4386156.052562262\n","Sharpe:  1.0111429977882227\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 3600     |\n","|    time_elapsed       | 59       |\n","|    total_timesteps    | 18000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 3599     |\n","|    policy_loss        | 1.8e+08  |\n","|    std                | 0.985    |\n","|    value_loss         | 2.08e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 3700      |\n","|    time_elapsed       | 60        |\n","|    total_timesteps    | 18500     |\n","| train/                |           |\n","|    entropy_loss       | -42.1     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 3699      |\n","|    policy_loss        | 2.43e+08  |\n","|    std                | 0.985     |\n","|    value_loss         | 4.18e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 3800     |\n","|    time_elapsed       | 62       |\n","|    total_timesteps    | 19000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 3799     |\n","|    policy_loss        | 3.7e+08  |\n","|    std                | 0.984    |\n","|    value_loss         | 1.02e+14 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 3900      |\n","|    time_elapsed       | 64        |\n","|    total_timesteps    | 19500     |\n","| train/                |           |\n","|    entropy_loss       | -42.1     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 3899      |\n","|    policy_loss        | 4.18e+08  |\n","|    std                | 0.984     |\n","|    value_loss         | 1.29e+14  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 4000     |\n","|    time_elapsed       | 65       |\n","|    total_timesteps    | 20000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 2.98e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 3999     |\n","|    policy_loss        | 6.28e+08 |\n","|    std                | 0.984    |\n","|    value_loss         | 2.56e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4894163.478799538\n","Sharpe:  1.0777462296313467\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 4100     |\n","|    time_elapsed       | 67       |\n","|    total_timesteps    | 20500    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4099     |\n","|    policy_loss        | 1.66e+08 |\n","|    std                | 0.984    |\n","|    value_loss         | 2.13e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 4200     |\n","|    time_elapsed       | 69       |\n","|    total_timesteps    | 21000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4199     |\n","|    policy_loss        | 2.31e+08 |\n","|    std                | 0.984    |\n","|    value_loss         | 3.96e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 4300     |\n","|    time_elapsed       | 70       |\n","|    total_timesteps    | 21500    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4299     |\n","|    policy_loss        | 3.5e+08  |\n","|    std                | 0.984    |\n","|    value_loss         | 9.51e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 4400     |\n","|    time_elapsed       | 72       |\n","|    total_timesteps    | 22000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4399     |\n","|    policy_loss        | 4.61e+08 |\n","|    std                | 0.984    |\n","|    value_loss         | 1.27e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 4500     |\n","|    time_elapsed       | 73       |\n","|    total_timesteps    | 22500    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 1.79e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4499     |\n","|    policy_loss        | 5.86e+08 |\n","|    std                | 0.983    |\n","|    value_loss         | 2.51e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4843195.825759301\n","Sharpe:  1.0756222483542104\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 4600     |\n","|    time_elapsed       | 75       |\n","|    total_timesteps    | 23000    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 2.38e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4599     |\n","|    policy_loss        | 1.79e+08 |\n","|    std                | 0.983    |\n","|    value_loss         | 2.07e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 4700     |\n","|    time_elapsed       | 77       |\n","|    total_timesteps    | 23500    |\n","| train/                |          |\n","|    entropy_loss       | -42.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4699     |\n","|    policy_loss        | 2.38e+08 |\n","|    std                | 0.983    |\n","|    value_loss         | 3.85e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 304       |\n","|    iterations         | 4800      |\n","|    time_elapsed       | 78        |\n","|    total_timesteps    | 24000     |\n","| train/                |           |\n","|    entropy_loss       | -42       |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 4799      |\n","|    policy_loss        | 3.84e+08  |\n","|    std                | 0.983     |\n","|    value_loss         | 9e+13     |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 4900     |\n","|    time_elapsed       | 80       |\n","|    total_timesteps    | 24500    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 1.79e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4899     |\n","|    policy_loss        | 4.39e+08 |\n","|    std                | 0.982    |\n","|    value_loss         | 1.25e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5000     |\n","|    time_elapsed       | 81       |\n","|    total_timesteps    | 25000    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 4999     |\n","|    policy_loss        | 5.7e+08  |\n","|    std                | 0.982    |\n","|    value_loss         | 2.33e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4730064.17159162\n","Sharpe:  1.053494037939329\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 304       |\n","|    iterations         | 5100      |\n","|    time_elapsed       | 83        |\n","|    total_timesteps    | 25500     |\n","| train/                |           |\n","|    entropy_loss       | -42       |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 5099      |\n","|    policy_loss        | 1.91e+08  |\n","|    std                | 0.981     |\n","|    value_loss         | 2.26e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5200     |\n","|    time_elapsed       | 85       |\n","|    total_timesteps    | 26000    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5199     |\n","|    policy_loss        | 2.34e+08 |\n","|    std                | 0.981    |\n","|    value_loss         | 3.87e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5300     |\n","|    time_elapsed       | 86       |\n","|    total_timesteps    | 26500    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5299     |\n","|    policy_loss        | 3.43e+08 |\n","|    std                | 0.981    |\n","|    value_loss         | 8.09e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5400     |\n","|    time_elapsed       | 88       |\n","|    total_timesteps    | 27000    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5399     |\n","|    policy_loss        | 4.18e+08 |\n","|    std                | 0.98     |\n","|    value_loss         | 1.16e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 305      |\n","|    iterations         | 5500     |\n","|    time_elapsed       | 90       |\n","|    total_timesteps    | 27500    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5499     |\n","|    policy_loss        | 5.55e+08 |\n","|    std                | 0.98     |\n","|    value_loss         | 2.13e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4605649.954963044\n","Sharpe:  1.0412018074925624\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5600     |\n","|    time_elapsed       | 91       |\n","|    total_timesteps    | 28000    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5599     |\n","|    policy_loss        | 1.9e+08  |\n","|    std                | 0.981    |\n","|    value_loss         | 2.52e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5700     |\n","|    time_elapsed       | 93       |\n","|    total_timesteps    | 28500    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5699     |\n","|    policy_loss        | 2.62e+08 |\n","|    std                | 0.98     |\n","|    value_loss         | 4.51e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5800     |\n","|    time_elapsed       | 95       |\n","|    total_timesteps    | 29000    |\n","| train/                |          |\n","|    entropy_loss       | -42      |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5799     |\n","|    policy_loss        | 3.5e+08  |\n","|    std                | 0.98     |\n","|    value_loss         | 8.99e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 304      |\n","|    iterations         | 5900     |\n","|    time_elapsed       | 96       |\n","|    total_timesteps    | 29500    |\n","| train/                |          |\n","|    entropy_loss       | -41.9    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 5899     |\n","|    policy_loss        | 4.45e+08 |\n","|    std                | 0.979    |\n","|    value_loss         | 1.28e+14 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 304       |\n","|    iterations         | 6000      |\n","|    time_elapsed       | 98        |\n","|    total_timesteps    | 30000     |\n","| train/                |           |\n","|    entropy_loss       | -41.9     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 5999      |\n","|    policy_loss        | 6e+08     |\n","|    std                | 0.979     |\n","|    value_loss         | 2.63e+14  |\n","-------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4947783.558600557\n","Sharpe:  1.0809895995222034\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 6100     |\n","|    time_elapsed       | 100      |\n","|    total_timesteps    | 30500    |\n","| train/                |          |\n","|    entropy_loss       | -41.9    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 6099     |\n","|    policy_loss        | 1.67e+08 |\n","|    std                | 0.979    |\n","|    value_loss         | 2.16e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 6200      |\n","|    time_elapsed       | 102       |\n","|    total_timesteps    | 31000     |\n","| train/                |           |\n","|    entropy_loss       | -41.9     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 6199      |\n","|    policy_loss        | 2.42e+08  |\n","|    std                | 0.979     |\n","|    value_loss         | 3.83e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 6300     |\n","|    time_elapsed       | 103      |\n","|    total_timesteps    | 31500    |\n","| train/                |          |\n","|    entropy_loss       | -41.9    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 6299     |\n","|    policy_loss        | 3.49e+08 |\n","|    std                | 0.978    |\n","|    value_loss         | 8.16e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 6400     |\n","|    time_elapsed       | 105      |\n","|    total_timesteps    | 32000    |\n","| train/                |          |\n","|    entropy_loss       | -41.9    |\n","|    explained_variance | 2.38e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 6399     |\n","|    policy_loss        | 3.83e+08 |\n","|    std                | 0.978    |\n","|    value_loss         | 1.11e+14 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 6500      |\n","|    time_elapsed       | 106       |\n","|    total_timesteps    | 32500     |\n","| train/                |           |\n","|    entropy_loss       | -41.9     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 6499      |\n","|    policy_loss        | 6.13e+08  |\n","|    std                | 0.977     |\n","|    value_loss         | 2.76e+14  |\n","-------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:5043150.156631937\n","Sharpe:  1.0941178998471104\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 6600      |\n","|    time_elapsed       | 108       |\n","|    total_timesteps    | 33000     |\n","| train/                |           |\n","|    entropy_loss       | -41.9     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 6599      |\n","|    policy_loss        | 1.75e+08  |\n","|    std                | 0.977     |\n","|    value_loss         | 2.16e+13  |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 303       |\n","|    iterations         | 6700      |\n","|    time_elapsed       | 110       |\n","|    total_timesteps    | 33500     |\n","| train/                |           |\n","|    entropy_loss       | -41.9     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 6699      |\n","|    policy_loss        | 2.42e+08  |\n","|    std                | 0.977     |\n","|    value_loss         | 3.93e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 6800     |\n","|    time_elapsed       | 112      |\n","|    total_timesteps    | 34000    |\n","| train/                |          |\n","|    entropy_loss       | -41.9    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 6799     |\n","|    policy_loss        | 3.36e+08 |\n","|    std                | 0.977    |\n","|    value_loss         | 7.78e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 6900     |\n","|    time_elapsed       | 113      |\n","|    total_timesteps    | 34500    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 6899     |\n","|    policy_loss        | 3.93e+08 |\n","|    std                | 0.976    |\n","|    value_loss         | 1.03e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 303      |\n","|    iterations         | 7000     |\n","|    time_elapsed       | 115      |\n","|    total_timesteps    | 35000    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 1.79e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 6999     |\n","|    policy_loss        | 7.1e+08  |\n","|    std                | 0.976    |\n","|    value_loss         | 2.7e+14  |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4776296.1552950945\n","Sharpe:  1.0643387250864105\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 302      |\n","|    iterations         | 7100     |\n","|    time_elapsed       | 117      |\n","|    total_timesteps    | 35500    |\n","| train/                |          |\n","|    entropy_loss       | -41.9    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7099     |\n","|    policy_loss        | 1.6e+08  |\n","|    std                | 0.976    |\n","|    value_loss         | 1.94e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 302      |\n","|    iterations         | 7200     |\n","|    time_elapsed       | 118      |\n","|    total_timesteps    | 36000    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7199     |\n","|    policy_loss        | 2.27e+08 |\n","|    std                | 0.976    |\n","|    value_loss         | 3.6e+13  |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 302      |\n","|    iterations         | 7300     |\n","|    time_elapsed       | 120      |\n","|    total_timesteps    | 36500    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7299     |\n","|    policy_loss        | 3.29e+08 |\n","|    std                | 0.976    |\n","|    value_loss         | 8.1e+13  |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 302       |\n","|    iterations         | 7400      |\n","|    time_elapsed       | 122       |\n","|    total_timesteps    | 37000     |\n","| train/                |           |\n","|    entropy_loss       | -41.8     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 7399      |\n","|    policy_loss        | 3.61e+08  |\n","|    std                | 0.975     |\n","|    value_loss         | 1.01e+14  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 302      |\n","|    iterations         | 7500     |\n","|    time_elapsed       | 124      |\n","|    total_timesteps    | 37500    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7499     |\n","|    policy_loss        | 5.93e+08 |\n","|    std                | 0.975    |\n","|    value_loss         | 2.55e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4857029.71470587\n","Sharpe:  1.0748247428396498\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 301      |\n","|    iterations         | 7600     |\n","|    time_elapsed       | 126      |\n","|    total_timesteps    | 38000    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7599     |\n","|    policy_loss        | 1.74e+08 |\n","|    std                | 0.974    |\n","|    value_loss         | 2.15e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 301      |\n","|    iterations         | 7700     |\n","|    time_elapsed       | 127      |\n","|    total_timesteps    | 38500    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7699     |\n","|    policy_loss        | 2.3e+08  |\n","|    std                | 0.975    |\n","|    value_loss         | 3.3e+13  |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 300       |\n","|    iterations         | 7800      |\n","|    time_elapsed       | 129       |\n","|    total_timesteps    | 39000     |\n","| train/                |           |\n","|    entropy_loss       | -41.8     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 7799      |\n","|    policy_loss        | 3.55e+08  |\n","|    std                | 0.974     |\n","|    value_loss         | 8.14e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 300      |\n","|    iterations         | 7900     |\n","|    time_elapsed       | 131      |\n","|    total_timesteps    | 39500    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 7899     |\n","|    policy_loss        | 3.96e+08 |\n","|    std                | 0.974    |\n","|    value_loss         | 1.14e+14 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 300       |\n","|    iterations         | 8000      |\n","|    time_elapsed       | 132       |\n","|    total_timesteps    | 40000     |\n","| train/                |           |\n","|    entropy_loss       | -41.8     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 7999      |\n","|    policy_loss        | 6.47e+08  |\n","|    std                | 0.974     |\n","|    value_loss         | 2.56e+14  |\n","-------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:5048118.375909526\n","Sharpe:  1.1004832676023253\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 300       |\n","|    iterations         | 8100      |\n","|    time_elapsed       | 134       |\n","|    total_timesteps    | 40500     |\n","| train/                |           |\n","|    entropy_loss       | -41.8     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 8099      |\n","|    policy_loss        | 1.66e+08  |\n","|    std                | 0.973     |\n","|    value_loss         | 2.06e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 300      |\n","|    iterations         | 8200     |\n","|    time_elapsed       | 136      |\n","|    total_timesteps    | 41000    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 8199     |\n","|    policy_loss        | 2.12e+08 |\n","|    std                | 0.973    |\n","|    value_loss         | 3.03e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 299      |\n","|    iterations         | 8300     |\n","|    time_elapsed       | 138      |\n","|    total_timesteps    | 41500    |\n","| train/                |          |\n","|    entropy_loss       | -41.8    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 8299     |\n","|    policy_loss        | 3.5e+08  |\n","|    std                | 0.973    |\n","|    value_loss         | 7.85e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 299      |\n","|    iterations         | 8400     |\n","|    time_elapsed       | 140      |\n","|    total_timesteps    | 42000    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 8399     |\n","|    policy_loss        | 3.93e+08 |\n","|    std                | 0.973    |\n","|    value_loss         | 1.17e+14 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 299       |\n","|    iterations         | 8500      |\n","|    time_elapsed       | 141       |\n","|    total_timesteps    | 42500     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 8499      |\n","|    policy_loss        | 5.54e+08  |\n","|    std                | 0.972     |\n","|    value_loss         | 2.23e+14  |\n","-------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4825804.636791511\n","Sharpe:  1.0630940700848848\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 299       |\n","|    iterations         | 8600      |\n","|    time_elapsed       | 143       |\n","|    total_timesteps    | 43000     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 8599      |\n","|    policy_loss        | 1.61e+08  |\n","|    std                | 0.972     |\n","|    value_loss         | 1.93e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 299      |\n","|    iterations         | 8700     |\n","|    time_elapsed       | 145      |\n","|    total_timesteps    | 43500    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 8699     |\n","|    policy_loss        | 2.23e+08 |\n","|    std                | 0.971    |\n","|    value_loss         | 3.01e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 298      |\n","|    iterations         | 8800     |\n","|    time_elapsed       | 147      |\n","|    total_timesteps    | 44000    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 8799     |\n","|    policy_loss        | 3.09e+08 |\n","|    std                | 0.971    |\n","|    value_loss         | 6.96e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 298      |\n","|    iterations         | 8900     |\n","|    time_elapsed       | 148      |\n","|    total_timesteps    | 44500    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 8899     |\n","|    policy_loss        | 4.04e+08 |\n","|    std                | 0.971    |\n","|    value_loss         | 1.21e+14 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 298       |\n","|    iterations         | 9000      |\n","|    time_elapsed       | 150       |\n","|    total_timesteps    | 45000     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 8999      |\n","|    policy_loss        | 5.61e+08  |\n","|    std                | 0.971     |\n","|    value_loss         | 2.19e+14  |\n","-------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4901719.112293877\n","Sharpe:  1.0813720639726352\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 298      |\n","|    iterations         | 9100     |\n","|    time_elapsed       | 152      |\n","|    total_timesteps    | 45500    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9099     |\n","|    policy_loss        | 1.63e+08 |\n","|    std                | 0.97     |\n","|    value_loss         | 1.81e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 298      |\n","|    iterations         | 9200     |\n","|    time_elapsed       | 154      |\n","|    total_timesteps    | 46000    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9199     |\n","|    policy_loss        | 1.95e+08 |\n","|    std                | 0.97     |\n","|    value_loss         | 2.95e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 298       |\n","|    iterations         | 9300      |\n","|    time_elapsed       | 155       |\n","|    total_timesteps    | 46500     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 9299      |\n","|    policy_loss        | 3.06e+08  |\n","|    std                | 0.97      |\n","|    value_loss         | 6.8e+13   |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 9400     |\n","|    time_elapsed       | 157      |\n","|    total_timesteps    | 47000    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9399     |\n","|    policy_loss        | 3.51e+08 |\n","|    std                | 0.97     |\n","|    value_loss         | 9.97e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 297       |\n","|    iterations         | 9500      |\n","|    time_elapsed       | 159       |\n","|    total_timesteps    | 47500     |\n","| train/                |           |\n","|    entropy_loss       | -41.6     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 9499      |\n","|    policy_loss        | 5.21e+08  |\n","|    std                | 0.97      |\n","|    value_loss         | 1.92e+14  |\n","-------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4639709.390753235\n","Sharpe:  1.0450531425325826\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 297       |\n","|    iterations         | 9600      |\n","|    time_elapsed       | 161       |\n","|    total_timesteps    | 48000     |\n","| train/                |           |\n","|    entropy_loss       | -41.6     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 9599      |\n","|    policy_loss        | 1.55e+08  |\n","|    std                | 0.969     |\n","|    value_loss         | 1.63e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 9700     |\n","|    time_elapsed       | 163      |\n","|    total_timesteps    | 48500    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9699     |\n","|    policy_loss        | 2.02e+08 |\n","|    std                | 0.968    |\n","|    value_loss         | 2.43e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 9800     |\n","|    time_elapsed       | 164      |\n","|    total_timesteps    | 49000    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9799     |\n","|    policy_loss        | 2.84e+08 |\n","|    std                | 0.968    |\n","|    value_loss         | 6.06e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 9900     |\n","|    time_elapsed       | 166      |\n","|    total_timesteps    | 49500    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9899     |\n","|    policy_loss        | 3.39e+08 |\n","|    std                | 0.968    |\n","|    value_loss         | 8.95e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 297      |\n","|    iterations         | 10000    |\n","|    time_elapsed       | 168      |\n","|    total_timesteps    | 50000    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 9999     |\n","|    policy_loss        | 5.15e+08 |\n","|    std                | 0.968    |\n","|    value_loss         | 1.83e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4589812.145139493\n","Sharpe:  1.035844799049011\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 296      |\n","|    iterations         | 10100    |\n","|    time_elapsed       | 170      |\n","|    total_timesteps    | 50500    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10099    |\n","|    policy_loss        | 1.46e+08 |\n","|    std                | 0.968    |\n","|    value_loss         | 1.55e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 296      |\n","|    iterations         | 10200    |\n","|    time_elapsed       | 171      |\n","|    total_timesteps    | 51000    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10199    |\n","|    policy_loss        | 1.94e+08 |\n","|    std                | 0.967    |\n","|    value_loss         | 2.48e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 296       |\n","|    iterations         | 10300     |\n","|    time_elapsed       | 173       |\n","|    total_timesteps    | 51500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 10299     |\n","|    policy_loss        | 2.87e+08  |\n","|    std                | 0.967     |\n","|    value_loss         | 5.87e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 296      |\n","|    iterations         | 10400    |\n","|    time_elapsed       | 175      |\n","|    total_timesteps    | 52000    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10399    |\n","|    policy_loss        | 3.57e+08 |\n","|    std                | 0.967    |\n","|    value_loss         | 8.65e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 296      |\n","|    iterations         | 10500    |\n","|    time_elapsed       | 177      |\n","|    total_timesteps    | 52500    |\n","| train/                |          |\n","|    entropy_loss       | -41.6    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10499    |\n","|    policy_loss        | 4.62e+08 |\n","|    std                | 0.967    |\n","|    value_loss         | 1.75e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4531884.558269173\n","Sharpe:  1.0213629561213728\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 295      |\n","|    iterations         | 10600    |\n","|    time_elapsed       | 179      |\n","|    total_timesteps    | 53000    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10599    |\n","|    policy_loss        | 1.54e+08 |\n","|    std                | 0.967    |\n","|    value_loss         | 1.52e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 295      |\n","|    iterations         | 10700    |\n","|    time_elapsed       | 181      |\n","|    total_timesteps    | 53500    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10699    |\n","|    policy_loss        | 1.93e+08 |\n","|    std                | 0.967    |\n","|    value_loss         | 2.66e+13 |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 295       |\n","|    iterations         | 10800     |\n","|    time_elapsed       | 182       |\n","|    total_timesteps    | 54000     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 10799     |\n","|    policy_loss        | 2.83e+08  |\n","|    std                | 0.966     |\n","|    value_loss         | 6.66e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 295      |\n","|    iterations         | 10900    |\n","|    time_elapsed       | 184      |\n","|    total_timesteps    | 54500    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10899    |\n","|    policy_loss        | 3.93e+08 |\n","|    std                | 0.965    |\n","|    value_loss         | 1.11e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 295      |\n","|    iterations         | 11000    |\n","|    time_elapsed       | 186      |\n","|    total_timesteps    | 55000    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 10999    |\n","|    policy_loss        | 5.15e+08 |\n","|    std                | 0.966    |\n","|    value_loss         | 1.83e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4750498.874344095\n","Sharpe:  1.058503747202285\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 294       |\n","|    iterations         | 11100     |\n","|    time_elapsed       | 188       |\n","|    total_timesteps    | 55500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0002    |\n","|    n_updates          | 11099     |\n","|    policy_loss        | 1.34e+08  |\n","|    std                | 0.965     |\n","|    value_loss         | 1.41e+13  |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 294      |\n","|    iterations         | 11200    |\n","|    time_elapsed       | 190      |\n","|    total_timesteps    | 56000    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11199    |\n","|    policy_loss        | 2.04e+08 |\n","|    std                | 0.965    |\n","|    value_loss         | 3.04e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 294      |\n","|    iterations         | 11300    |\n","|    time_elapsed       | 191      |\n","|    total_timesteps    | 56500    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11299    |\n","|    policy_loss        | 2.94e+08 |\n","|    std                | 0.964    |\n","|    value_loss         | 6.4e+13  |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 294      |\n","|    iterations         | 11400    |\n","|    time_elapsed       | 193      |\n","|    total_timesteps    | 57000    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11399    |\n","|    policy_loss        | 4.03e+08 |\n","|    std                | 0.965    |\n","|    value_loss         | 1.07e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 294      |\n","|    iterations         | 11500    |\n","|    time_elapsed       | 195      |\n","|    total_timesteps    | 57500    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11499    |\n","|    policy_loss        | 5.01e+08 |\n","|    std                | 0.965    |\n","|    value_loss         | 1.63e+14 |\n","------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4444039.964760215\n","Sharpe:  1.0184426418048476\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 293      |\n","|    iterations         | 11600    |\n","|    time_elapsed       | 197      |\n","|    total_timesteps    | 58000    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11599    |\n","|    policy_loss        | 1.26e+08 |\n","|    std                | 0.964    |\n","|    value_loss         | 1.21e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 293      |\n","|    iterations         | 11700    |\n","|    time_elapsed       | 199      |\n","|    total_timesteps    | 58500    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11699    |\n","|    policy_loss        | 2.11e+08 |\n","|    std                | 0.964    |\n","|    value_loss         | 3.1e+13  |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 293      |\n","|    iterations         | 11800    |\n","|    time_elapsed       | 200      |\n","|    total_timesteps    | 59000    |\n","| train/                |          |\n","|    entropy_loss       | -41.4    |\n","|    explained_variance | 1.19e-07 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11799    |\n","|    policy_loss        | 3e+08    |\n","|    std                | 0.963    |\n","|    value_loss         | 6.05e+13 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 293      |\n","|    iterations         | 11900    |\n","|    time_elapsed       | 202      |\n","|    total_timesteps    | 59500    |\n","| train/                |          |\n","|    entropy_loss       | -41.4    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11899    |\n","|    policy_loss        | 3.99e+08 |\n","|    std                | 0.962    |\n","|    value_loss         | 1.12e+14 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 293      |\n","|    iterations         | 12000    |\n","|    time_elapsed       | 204      |\n","|    total_timesteps    | 60000    |\n","| train/                |          |\n","|    entropy_loss       | -41.4    |\n","|    explained_variance | 5.96e-08 |\n","|    learning_rate      | 0.0002   |\n","|    n_updates          | 11999    |\n","|    policy_loss        | 4.62e+08 |\n","|    std                | 0.962    |\n","|    value_loss         | 1.72e+14 |\n","------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"lvrqTro3lhAh"},"source":["### Model 2: **PPO** (Proximal Policy Optimization)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVXta7jVKMhV","outputId":"d1d36631-619d-4fb9-f7ff-f6d399446bf8","executionInfo":{"status":"ok","timestamp":1651694092414,"user_tz":300,"elapsed":90,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["agent = DRLAgent(env = env_train)\n","PPO_PARAMS = {\n","    \"n_steps\": 2048,\n","    \"ent_coef\": 0.005,\n","    \"learning_rate\": 0.0001,\n","    \"batch_size\": 128,\n","}\n","model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n","Using cpu device\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5XlUIszKUGx","outputId":"453f3823-c842-4f6d-c2dd-e5d75a791694","executionInfo":{"status":"ok","timestamp":1651694324053,"user_tz":300,"elapsed":231670,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["trained_ppo = agent.train_model(model=model_ppo, \n","                             tb_log_name='ppo',\n","                             total_timesteps=80000)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to tensorboard_log/ppo/ppo_1\n","-----------------------------\n","| time/              |      |\n","|    fps             | 430  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 4    |\n","|    total_timesteps | 2048 |\n","-----------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4693911.617073888\n","Sharpe:  1.0449456411900337\n","=================================\n","--------------------------------------\n","| time/                   |          |\n","|    fps                  | 382      |\n","|    iterations           | 2        |\n","|    time_elapsed         | 10       |\n","|    total_timesteps      | 4096     |\n","| train/                  |          |\n","|    approx_kl            | 0.0      |\n","|    clip_fraction        | 0        |\n","|    clip_range           | 0.2      |\n","|    entropy_loss         | -42.6    |\n","|    explained_variance   | 0        |\n","|    learning_rate        | 0.0001   |\n","|    loss                 | 7.65e+14 |\n","|    n_updates            | 10       |\n","|    policy_gradient_loss | -5e-07   |\n","|    std                  | 1        |\n","|    value_loss           | 1.54e+15 |\n","--------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4425025.024117169\n","Sharpe:  1.0125027055956939\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 368       |\n","|    iterations           | 3         |\n","|    time_elapsed         | 16        |\n","|    total_timesteps      | 6144      |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 5.96e-08  |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.18e+15  |\n","|    n_updates            | 20        |\n","|    policy_gradient_loss | -5.86e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.1e+15   |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:5018280.563119376\n","Sharpe:  1.0883162163117448\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 364       |\n","|    iterations           | 4         |\n","|    time_elapsed         | 22        |\n","|    total_timesteps      | 8192      |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.08e+15  |\n","|    n_updates            | 30        |\n","|    policy_gradient_loss | -5.19e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.27e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4436510.404973225\n","Sharpe:  1.0156482352745297\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 362       |\n","|    iterations           | 5         |\n","|    time_elapsed         | 28        |\n","|    total_timesteps      | 10240     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.47e+15  |\n","|    n_updates            | 40        |\n","|    policy_gradient_loss | -4.03e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 3.01e+15  |\n","---------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 362       |\n","|    iterations           | 6         |\n","|    time_elapsed         | 33        |\n","|    total_timesteps      | 12288     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -2.38e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.33e+15  |\n","|    n_updates            | 50        |\n","|    policy_gradient_loss | -3.7e-07  |\n","|    std                  | 1         |\n","|    value_loss           | 2.56e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4298141.26847823\n","Sharpe:  0.9935804541617326\n","=================================\n","--------------------------------------\n","| time/                   |          |\n","|    fps                  | 360      |\n","|    iterations           | 7        |\n","|    time_elapsed         | 39       |\n","|    total_timesteps      | 14336    |\n","| train/                  |          |\n","|    approx_kl            | 0.0      |\n","|    clip_fraction        | 0        |\n","|    clip_range           | 0.2      |\n","|    entropy_loss         | -42.6    |\n","|    explained_variance   | 0        |\n","|    learning_rate        | 0.0001   |\n","|    loss                 | 8.83e+14 |\n","|    n_updates            | 60       |\n","|    policy_gradient_loss | -5.4e-07 |\n","|    std                  | 1        |\n","|    value_loss           | 1.78e+15 |\n","--------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4650307.623131712\n","Sharpe:  1.0428043602708785\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 358       |\n","|    iterations           | 8         |\n","|    time_elapsed         | 45        |\n","|    total_timesteps      | 16384     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 8.83e+14  |\n","|    n_updates            | 70        |\n","|    policy_gradient_loss | -3.34e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 1.9e+15   |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:5123425.165633802\n","Sharpe:  1.1065516525289887\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 356       |\n","|    iterations           | 9         |\n","|    time_elapsed         | 51        |\n","|    total_timesteps      | 18432     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.2e+15   |\n","|    n_updates            | 80        |\n","|    policy_gradient_loss | -3.98e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.37e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4923449.445666635\n","Sharpe:  1.0738780619135557\n","=================================\n","--------------------------------------\n","| time/                   |          |\n","|    fps                  | 356      |\n","|    iterations           | 10       |\n","|    time_elapsed         | 57       |\n","|    total_timesteps      | 20480    |\n","| train/                  |          |\n","|    approx_kl            | 0.0      |\n","|    clip_fraction        | 0        |\n","|    clip_range           | 0.2      |\n","|    entropy_loss         | -42.6    |\n","|    explained_variance   | 0        |\n","|    learning_rate        | 0.0001   |\n","|    loss                 | 1.68e+15 |\n","|    n_updates            | 90       |\n","|    policy_gradient_loss | -4.7e-07 |\n","|    std                  | 1        |\n","|    value_loss           | 3.03e+15 |\n","--------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 356       |\n","|    iterations           | 11        |\n","|    time_elapsed         | 63        |\n","|    total_timesteps      | 22528     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.64e+15  |\n","|    n_updates            | 100       |\n","|    policy_gradient_loss | -3.82e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.85e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4617193.303393039\n","Sharpe:  1.0387200603458142\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 12        |\n","|    time_elapsed         | 69        |\n","|    total_timesteps      | 24576     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.26e+15  |\n","|    n_updates            | 110       |\n","|    policy_gradient_loss | -5.43e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.48e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4508953.776153343\n","Sharpe:  1.0212143364679827\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 13        |\n","|    time_elapsed         | 74        |\n","|    total_timesteps      | 26624     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 7.17e+14  |\n","|    n_updates            | 120       |\n","|    policy_gradient_loss | -6.18e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 1.58e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4417942.075707689\n","Sharpe:  1.0121092370737308\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 14        |\n","|    time_elapsed         | 80        |\n","|    total_timesteps      | 28672     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 9.92e+14  |\n","|    n_updates            | 130       |\n","|    policy_gradient_loss | -4.48e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.08e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4781687.99892205\n","Sharpe:  1.056535803939103\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 15        |\n","|    time_elapsed         | 86        |\n","|    total_timesteps      | 30720     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 5.96e-08  |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.07e+15  |\n","|    n_updates            | 140       |\n","|    policy_gradient_loss | -4.54e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.3e+15   |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4723014.7852396555\n","Sharpe:  1.0530432427546155\n","=================================\n","--------------------------------------\n","| time/                   |          |\n","|    fps                  | 355      |\n","|    iterations           | 16       |\n","|    time_elapsed         | 92       |\n","|    total_timesteps      | 32768    |\n","| train/                  |          |\n","|    approx_kl            | 0.0      |\n","|    clip_fraction        | 0        |\n","|    clip_range           | 0.2      |\n","|    entropy_loss         | -42.6    |\n","|    explained_variance   | 5.96e-08 |\n","|    learning_rate        | 0.0001   |\n","|    loss                 | 1.2e+15  |\n","|    n_updates            | 150      |\n","|    policy_gradient_loss | -4.6e-07 |\n","|    std                  | 1        |\n","|    value_loss           | 2.8e+15  |\n","--------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 17        |\n","|    time_elapsed         | 97        |\n","|    total_timesteps      | 34816     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.34e+15  |\n","|    n_updates            | 160       |\n","|    policy_gradient_loss | -3.83e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.86e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4663685.646141227\n","Sharpe:  1.043799340312458\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 18        |\n","|    time_elapsed         | 103       |\n","|    total_timesteps      | 36864     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 9.15e+14  |\n","|    n_updates            | 170       |\n","|    policy_gradient_loss | -6.06e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 1.74e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4900613.054364892\n","Sharpe:  1.071430477808924\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 19        |\n","|    time_elapsed         | 109       |\n","|    total_timesteps      | 38912     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.01e+15  |\n","|    n_updates            | 180       |\n","|    policy_gradient_loss | -4.52e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.13e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4802300.4822891615\n","Sharpe:  1.060053606511407\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 20        |\n","|    time_elapsed         | 115       |\n","|    total_timesteps      | 40960     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.29e+15  |\n","|    n_updates            | 190       |\n","|    policy_gradient_loss | -5.06e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.55e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4400029.671328842\n","Sharpe:  1.006282592160993\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 21        |\n","|    time_elapsed         | 121       |\n","|    total_timesteps      | 43008     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.5e+15   |\n","|    n_updates            | 200       |\n","|    policy_gradient_loss | -4.79e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.83e+15  |\n","---------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 22        |\n","|    time_elapsed         | 126       |\n","|    total_timesteps      | 45056     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 1.19e-07  |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.26e+15  |\n","|    n_updates            | 210       |\n","|    policy_gradient_loss | -4.03e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.62e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4590821.681267747\n","Sharpe:  1.029451997526058\n","=================================\n","--------------------------------------\n","| time/                   |          |\n","|    fps                  | 355      |\n","|    iterations           | 23       |\n","|    time_elapsed         | 132      |\n","|    total_timesteps      | 47104    |\n","| train/                  |          |\n","|    approx_kl            | 0.0      |\n","|    clip_fraction        | 0        |\n","|    clip_range           | 0.2      |\n","|    entropy_loss         | -42.6    |\n","|    explained_variance   | 0        |\n","|    learning_rate        | 0.0001   |\n","|    loss                 | 1.09e+15 |\n","|    n_updates            | 220      |\n","|    policy_gradient_loss | -5.2e-07 |\n","|    std                  | 1        |\n","|    value_loss           | 2.05e+15 |\n","--------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4696365.768897861\n","Sharpe:  1.0483637211144714\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 24        |\n","|    time_elapsed         | 138       |\n","|    total_timesteps      | 49152     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 1.19e-07  |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 9.73e+14  |\n","|    n_updates            | 230       |\n","|    policy_gradient_loss | -4.83e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 1.87e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4448933.853955421\n","Sharpe:  1.011707074991097\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 25        |\n","|    time_elapsed         | 144       |\n","|    total_timesteps      | 51200     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.18e+15  |\n","|    n_updates            | 240       |\n","|    policy_gradient_loss | -4.45e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.3e+15   |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4390644.170455098\n","Sharpe:  1.0057131311001648\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 26        |\n","|    time_elapsed         | 149       |\n","|    total_timesteps      | 53248     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.16e+15  |\n","|    n_updates            | 250       |\n","|    policy_gradient_loss | -4.28e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.39e+15  |\n","---------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 27        |\n","|    time_elapsed         | 155       |\n","|    total_timesteps      | 55296     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.08e+15  |\n","|    n_updates            | 260       |\n","|    policy_gradient_loss | -4.14e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.49e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4729645.74770008\n","Sharpe:  1.057816466961063\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 28        |\n","|    time_elapsed         | 161       |\n","|    total_timesteps      | 57344     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.2e+15   |\n","|    n_updates            | 270       |\n","|    policy_gradient_loss | -5.55e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.59e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4399748.180248028\n","Sharpe:  1.0078136120314563\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 29        |\n","|    time_elapsed         | 167       |\n","|    total_timesteps      | 59392     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 8.14e+14  |\n","|    n_updates            | 280       |\n","|    policy_gradient_loss | -6.29e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 1.52e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4853490.933303672\n","Sharpe:  1.065360180948387\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 30        |\n","|    time_elapsed         | 172       |\n","|    total_timesteps      | 61440     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 1.19e-07  |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 9.85e+14  |\n","|    n_updates            | 290       |\n","|    policy_gradient_loss | -5.56e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.08e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4650953.133164322\n","Sharpe:  1.0377371763301988\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 31        |\n","|    time_elapsed         | 178       |\n","|    total_timesteps      | 63488     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.1e+15   |\n","|    n_updates            | 300       |\n","|    policy_gradient_loss | -5.08e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.46e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:5023003.751397484\n","Sharpe:  1.0874707304388649\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 32        |\n","|    time_elapsed         | 184       |\n","|    total_timesteps      | 65536     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.4e+15   |\n","|    n_updates            | 310       |\n","|    policy_gradient_loss | -5.42e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.6e+15   |\n","---------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 33        |\n","|    time_elapsed         | 190       |\n","|    total_timesteps      | 67584     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.55e+15  |\n","|    n_updates            | 320       |\n","|    policy_gradient_loss | -5.86e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 3.19e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:5028972.482850097\n","Sharpe:  1.092230304405696\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 34        |\n","|    time_elapsed         | 196       |\n","|    total_timesteps      | 69632     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 9.78e+14  |\n","|    n_updates            | 330       |\n","|    policy_gradient_loss | -5.15e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2e+15     |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4730724.810175407\n","Sharpe:  1.0507890165310076\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 35        |\n","|    time_elapsed         | 201       |\n","|    total_timesteps      | 71680     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.04e+15  |\n","|    n_updates            | 340       |\n","|    policy_gradient_loss | -5.36e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.21e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4745365.552397562\n","Sharpe:  1.0543169907276826\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 36        |\n","|    time_elapsed         | 207       |\n","|    total_timesteps      | 73728     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.2e+15   |\n","|    n_updates            | 350       |\n","|    policy_gradient_loss | -3.68e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.4e+15   |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4587784.110679447\n","Sharpe:  1.028575980830444\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 354       |\n","|    iterations           | 37        |\n","|    time_elapsed         | 213       |\n","|    total_timesteps      | 75776     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | -1.19e-07 |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.52e+15  |\n","|    n_updates            | 360       |\n","|    policy_gradient_loss | -5.93e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.81e+15  |\n","---------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 38        |\n","|    time_elapsed         | 219       |\n","|    total_timesteps      | 77824     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.41e+15  |\n","|    n_updates            | 370       |\n","|    policy_gradient_loss | -5.17e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.66e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4447731.592438138\n","Sharpe:  1.013970879078777\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 39        |\n","|    time_elapsed         | 224       |\n","|    total_timesteps      | 79872     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 1.1e+15   |\n","|    n_updates            | 380       |\n","|    policy_gradient_loss | -3.74e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 2.22e+15  |\n","---------------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4864624.707304226\n","Sharpe:  1.0689777202344088\n","=================================\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 355       |\n","|    iterations           | 40        |\n","|    time_elapsed         | 230       |\n","|    total_timesteps      | 81920     |\n","| train/                  |           |\n","|    approx_kl            | 0.0       |\n","|    clip_fraction        | 0         |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -42.6     |\n","|    explained_variance   | 0         |\n","|    learning_rate        | 0.0001    |\n","|    loss                 | 8.3e+14   |\n","|    n_updates            | 390       |\n","|    policy_gradient_loss | -5.65e-07 |\n","|    std                  | 1         |\n","|    value_loss           | 1.78e+15  |\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"a3Iuv554xYFH"},"source":["### Model 3: **DDPG** (Deep Deterministic Policy Gradients)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ojmppgo4LPLz","outputId":"80609d2e-83b8-472b-9816-03388a2003d9","executionInfo":{"status":"ok","timestamp":1651694324382,"user_tz":300,"elapsed":387,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["agent = DRLAgent(env = env_train)\n","DDPG_PARAMS = {\"batch_size\": 128, \"buffer_size\": 50000, \"learning_rate\": 0.001}\n","\n","\n","model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n","Using cpu device\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWt6BIR0LT25","outputId":"d4b1f8f9-cf66-4c6d-b161-ece79257aa55","executionInfo":{"status":"ok","timestamp":1651695766915,"user_tz":300,"elapsed":1442553,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["trained_ddpg = agent.train_model(model=model_ddpg, \n","                             tb_log_name='ddpg',\n","                             total_timesteps=50000)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to tensorboard_log/ddpg/ddpg_1\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4637487.096051112\n","Sharpe:  1.0327553915353764\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 4         |\n","|    fps             | 49        |\n","|    time_elapsed    | 204       |\n","|    total_timesteps | 10064     |\n","| train/             |           |\n","|    actor_loss      | -6.61e+07 |\n","|    critic_loss     | 1.93e+13  |\n","|    learning_rate   | 0.001     |\n","|    n_updates       | 7548      |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 8         |\n","|    fps             | 42        |\n","|    time_elapsed    | 472       |\n","|    total_timesteps | 20128     |\n","| train/             |           |\n","|    actor_loss      | -1.39e+08 |\n","|    critic_loss     | 6.25e+13  |\n","|    learning_rate   | 0.001     |\n","|    n_updates       | 17612     |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 12        |\n","|    fps             | 40        |\n","|    time_elapsed    | 749       |\n","|    total_timesteps | 30192     |\n","| train/             |           |\n","|    actor_loss      | -1.82e+08 |\n","|    critic_loss     | 8.98e+13  |\n","|    learning_rate   | 0.001     |\n","|    n_updates       | 27676     |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 16        |\n","|    fps             | 38        |\n","|    time_elapsed    | 1033      |\n","|    total_timesteps | 40256     |\n","| train/             |           |\n","|    actor_loss      | -2.09e+08 |\n","|    critic_loss     | 1.04e+14  |\n","|    learning_rate   | 0.001     |\n","|    n_updates       | 37740     |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4522294.923808918\n","Sharpe:  1.056520737385653\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 20        |\n","|    fps             | 36        |\n","|    time_elapsed    | 1360      |\n","|    total_timesteps | 50320     |\n","| train/             |           |\n","|    actor_loss      | -2.24e+08 |\n","|    critic_loss     | 1.06e+14  |\n","|    learning_rate   | 0.001     |\n","|    n_updates       | 47804     |\n","----------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"SPEXBcm-uBJo"},"source":["### Model 4: **SAC** (Soft Actor Critic)\n","\n","Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic policy in an off-policy way, forming a bridge between stochastic policy optimization and DDPG-style approaches. It incorporates the clipped double-Q trick, and due to the inherent stochasticity of the policy in SAC, it benefits from target policy smoothing.\n","\n","A central feature of SAC is entropy regularization. The policy is trained to maximize a trade-off between expected return and entropy, a measure of randomness in the policy. This has a close connection to the exploration-exploitation trade-off: increasing entropy results in more exploration, which can accelerate learning later on. It can also prevent the policy from prematurely converging to a bad local optimum.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaWf2QeiLqyO","outputId":"7447c506-dcd5-432c-d615-77b9d722bafc","executionInfo":{"status":"ok","timestamp":1651695767219,"user_tz":300,"elapsed":337,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["agent = DRLAgent(env = env_train)\n","SAC_PARAMS = {\n","    \"batch_size\": 128,\n","    \"buffer_size\": 100000,\n","    \"learning_rate\": 0.0003,\n","    \"learning_starts\": 100,\n","    \"ent_coef\": \"auto_0.1\",\n","}\n","\n","model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0003, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n","Using cpu device\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZgYVPqtKLvi3","outputId":"dde6ba02-2327-4ff8-dd51-d7fc42678f49","executionInfo":{"status":"ok","timestamp":1651697233386,"user_tz":300,"elapsed":1466192,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["trained_sac = agent.train_model(model=model_sac, \n","                             tb_log_name='sac',\n","                             total_timesteps=50000)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to tensorboard_log/sac/sac_1\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4690616.743585024\n","Sharpe:  1.068653893929399\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523758.263730788\n","Sharpe:  1.055628504530759\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523755.111236704\n","Sharpe:  1.05562822200797\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 4         |\n","|    fps             | 34        |\n","|    time_elapsed    | 290       |\n","|    total_timesteps | 10064     |\n","| train/             |           |\n","|    actor_loss      | -8.22e+07 |\n","|    critic_loss     | 3.18e+13  |\n","|    ent_coef        | 2.33      |\n","|    ent_coef_loss   | -239      |\n","|    learning_rate   | 0.0003    |\n","|    n_updates       | 9963      |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 8         |\n","|    fps             | 34        |\n","|    time_elapsed    | 584       |\n","|    total_timesteps | 20128     |\n","| train/             |           |\n","|    actor_loss      | -1.3e+08  |\n","|    critic_loss     | 5.73e+13  |\n","|    ent_coef        | 47.8      |\n","|    ent_coef_loss   | -1.09e+03 |\n","|    learning_rate   | 0.0003    |\n","|    n_updates       | 20027     |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 12        |\n","|    fps             | 34        |\n","|    time_elapsed    | 878       |\n","|    total_timesteps | 30192     |\n","| train/             |           |\n","|    actor_loss      | -1.54e+08 |\n","|    critic_loss     | 5.78e+13  |\n","|    ent_coef        | 978       |\n","|    ent_coef_loss   | -1.94e+03 |\n","|    learning_rate   | 0.0003    |\n","|    n_updates       | 30091     |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 16        |\n","|    fps             | 34        |\n","|    time_elapsed    | 1174      |\n","|    total_timesteps | 40256     |\n","| train/             |           |\n","|    actor_loss      | -7.85e+07 |\n","|    critic_loss     | 1.93e+13  |\n","|    ent_coef        | 2e+04     |\n","|    ent_coef_loss   | -2.79e+03 |\n","|    learning_rate   | 0.0003    |\n","|    n_updates       | 40155     |\n","----------------------------------\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n","=================================\n","begin_total_asset:1000000\n","end_total_asset:4523753.046577074\n","Sharpe:  1.0556281240347065\n","=================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"E2Ma6YpTlnuZ"},"source":["### Trading\n","Assume that we have $1,000,000 initial capital at 2019-01-01. We use the DDPG model to trade Dow jones 30 stocks."]},{"cell_type":"code","metadata":{"id":"uas8U6k455sI","executionInfo":{"status":"ok","timestamp":1651697233388,"user_tz":300,"elapsed":26,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["trade = data_split(df,'2019-01-01', '2021-01-01')\n","e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)\n"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcGYlhyal205","outputId":"f4ae4e08-7a35-48cd-aa2a-ede0f3bca8c9","executionInfo":{"status":"ok","timestamp":1651697233390,"user_tz":300,"elapsed":25,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["trade.shape"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15150, 17)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qq4W9FbSstT7","outputId":"b7f89511-82af-44a5-d030-7615d5c40caa","executionInfo":{"status":"ok","timestamp":1651697234582,"user_tz":300,"elapsed":1203,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df_daily_return, df_actions = DRLAgent.DRL_prediction(model=trained_a2c,\n","                        environment = e_trade_gym)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["=================================\n","begin_total_asset:1000000\n","end_total_asset:1389028.9302316473\n","Sharpe:  0.7396188290160893\n","=================================\n","hit end!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uJvj3pXt_Ukg","outputId":"91c8ba65-e6c4-4fff-adc4-d036560eb8aa","executionInfo":{"status":"ok","timestamp":1651697234583,"user_tz":300,"elapsed":59,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df_daily_return.head()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date  daily_return\n","0  2019-01-02      0.000000\n","1  2019-01-03     -0.026016\n","2  2019-01-04      0.031910\n","3  2019-01-07      0.003951\n","4  2019-01-08      0.009409"],"text/html":["\n","  <div id=\"df-f9e57c29-19cc-45e7-9579-cf8d0d292e42\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>daily_return</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2019-01-02</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2019-01-03</td>\n","      <td>-0.026016</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2019-01-04</td>\n","      <td>0.031910</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2019-01-07</td>\n","      <td>0.003951</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2019-01-08</td>\n","      <td>0.009409</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9e57c29-19cc-45e7-9579-cf8d0d292e42')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f9e57c29-19cc-45e7-9579-cf8d0d292e42 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f9e57c29-19cc-45e7-9579-cf8d0d292e42');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"tByVcZ2L9TAJ","outputId":"7efb56c0-f5b8-4a5a-81f1-23ea67b9873b","executionInfo":{"status":"ok","timestamp":1651697234584,"user_tz":300,"elapsed":55,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df_actions.head()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                AAPL       AXP        BA       CAT      CSCO       CVX  \\\n","date                                                                     \n","2019-01-02  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n","2019-01-03  0.058167  0.021398  0.021764  0.048239  0.021398  0.058167   \n","2019-01-04  0.017181  0.043305  0.043178  0.017181  0.017181  0.046702   \n","2019-01-07  0.060639  0.022308  0.058616  0.022308  0.022308  0.053711   \n","2019-01-08  0.059519  0.031454  0.021896  0.036060  0.059519  0.021896   \n","\n","                  DD       DIS        GS        HD  ...       PFE        PG  \\\n","date                                                ...                       \n","2019-01-02  0.033333  0.033333  0.033333  0.033333  ...  0.033333  0.033333   \n","2019-01-03  0.021398  0.021398  0.038364  0.045761  ...  0.021398  0.028547   \n","2019-01-04  0.046702  0.042161  0.046702  0.017181  ...  0.017181  0.024537   \n","2019-01-07  0.022308  0.022308  0.022308  0.040096  ...  0.060639  0.041304   \n","2019-01-08  0.030097  0.021896  0.059519  0.024551  ...  0.021896  0.021896   \n","\n","                 RTX       TRV       UNH         V        VZ       WBA  \\\n","date                                                                     \n","2019-01-02  0.033333  0.033333  0.033333  0.033333  0.033333  0.033333   \n","2019-01-03  0.021398  0.021398  0.028699  0.028470  0.058167  0.021398   \n","2019-01-04  0.046702  0.046702  0.017181  0.046702  0.046702  0.046702   \n","2019-01-07  0.022308  0.022308  0.022308  0.022308  0.060639  0.022308   \n","2019-01-08  0.037538  0.021896  0.053625  0.029183  0.021896  0.023527   \n","\n","                 WMT       XOM  \n","date                            \n","2019-01-02  0.033333  0.033333  \n","2019-01-03  0.021398  0.021398  \n","2019-01-04  0.046702  0.035099  \n","2019-01-07  0.022308  0.051915  \n","2019-01-08  0.021896  0.059519  \n","\n","[5 rows x 30 columns]"],"text/html":["\n","  <div id=\"df-35dac060-1393-4b01-985e-27ed9ea6fed1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AAPL</th>\n","      <th>AXP</th>\n","      <th>BA</th>\n","      <th>CAT</th>\n","      <th>CSCO</th>\n","      <th>CVX</th>\n","      <th>DD</th>\n","      <th>DIS</th>\n","      <th>GS</th>\n","      <th>HD</th>\n","      <th>...</th>\n","      <th>PFE</th>\n","      <th>PG</th>\n","      <th>RTX</th>\n","      <th>TRV</th>\n","      <th>UNH</th>\n","      <th>V</th>\n","      <th>VZ</th>\n","      <th>WBA</th>\n","      <th>WMT</th>\n","      <th>XOM</th>\n","    </tr>\n","    <tr>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2019-01-02</th>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>...</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","      <td>0.033333</td>\n","    </tr>\n","    <tr>\n","      <th>2019-01-03</th>\n","      <td>0.058167</td>\n","      <td>0.021398</td>\n","      <td>0.021764</td>\n","      <td>0.048239</td>\n","      <td>0.021398</td>\n","      <td>0.058167</td>\n","      <td>0.021398</td>\n","      <td>0.021398</td>\n","      <td>0.038364</td>\n","      <td>0.045761</td>\n","      <td>...</td>\n","      <td>0.021398</td>\n","      <td>0.028547</td>\n","      <td>0.021398</td>\n","      <td>0.021398</td>\n","      <td>0.028699</td>\n","      <td>0.028470</td>\n","      <td>0.058167</td>\n","      <td>0.021398</td>\n","      <td>0.021398</td>\n","      <td>0.021398</td>\n","    </tr>\n","    <tr>\n","      <th>2019-01-04</th>\n","      <td>0.017181</td>\n","      <td>0.043305</td>\n","      <td>0.043178</td>\n","      <td>0.017181</td>\n","      <td>0.017181</td>\n","      <td>0.046702</td>\n","      <td>0.046702</td>\n","      <td>0.042161</td>\n","      <td>0.046702</td>\n","      <td>0.017181</td>\n","      <td>...</td>\n","      <td>0.017181</td>\n","      <td>0.024537</td>\n","      <td>0.046702</td>\n","      <td>0.046702</td>\n","      <td>0.017181</td>\n","      <td>0.046702</td>\n","      <td>0.046702</td>\n","      <td>0.046702</td>\n","      <td>0.046702</td>\n","      <td>0.035099</td>\n","    </tr>\n","    <tr>\n","      <th>2019-01-07</th>\n","      <td>0.060639</td>\n","      <td>0.022308</td>\n","      <td>0.058616</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.053711</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.040096</td>\n","      <td>...</td>\n","      <td>0.060639</td>\n","      <td>0.041304</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.060639</td>\n","      <td>0.022308</td>\n","      <td>0.022308</td>\n","      <td>0.051915</td>\n","    </tr>\n","    <tr>\n","      <th>2019-01-08</th>\n","      <td>0.059519</td>\n","      <td>0.031454</td>\n","      <td>0.021896</td>\n","      <td>0.036060</td>\n","      <td>0.059519</td>\n","      <td>0.021896</td>\n","      <td>0.030097</td>\n","      <td>0.021896</td>\n","      <td>0.059519</td>\n","      <td>0.024551</td>\n","      <td>...</td>\n","      <td>0.021896</td>\n","      <td>0.021896</td>\n","      <td>0.037538</td>\n","      <td>0.021896</td>\n","      <td>0.053625</td>\n","      <td>0.029183</td>\n","      <td>0.021896</td>\n","      <td>0.023527</td>\n","      <td>0.021896</td>\n","      <td>0.059519</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 30 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35dac060-1393-4b01-985e-27ed9ea6fed1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35dac060-1393-4b01-985e-27ed9ea6fed1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35dac060-1393-4b01-985e-27ed9ea6fed1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"xBX3Y68o1vRG","executionInfo":{"status":"ok","timestamp":1651697234585,"user_tz":300,"elapsed":54,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df_actions.to_csv('df_actions.csv')"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFO42LcomPUT"},"source":["### Part 7: Backtest Our Strategy\n","Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."]},{"cell_type":"code","metadata":{"id":"-2DgsIW-fh0s","executionInfo":{"status":"aborted","timestamp":1651697237533,"user_tz":300,"elapsed":62,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXaoZs2lh1hi"},"source":["## Now let's ensemble over Reinforcement Learned agents!\n","\n","## Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using an Ensemble Strategy\n","\n","Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy | Based on a version resented at ICAIF 2020\n","\n","* This notebook is the reimplementation of the paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n","* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n","* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n","* **Pytorch Version** \n","\n"]},{"cell_type":"markdown","metadata":{"id":"sApkDlD9LIZv"},"source":["### Part 1. Problem Definition"]},{"cell_type":"markdown","metadata":{"id":"HjLD2TZSLKZ-"},"source":["This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n","\n","The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n","\n","\n","* Action: The action space describes the allowed actions that the agent interacts with the\n","environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n","selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n","an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n","10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n","\n","* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n","values at state s′ and s, respectively\n","\n","* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n","our trading agent observes many different features to better learn in an interactive environment.\n","\n","* Environment: Dow 30 consituents\n","\n","\n","The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"]},{"cell_type":"markdown","metadata":{"id":"Ffsre789LY08"},"source":["### Part 2. Getting Started- Load Python Packages"]},{"cell_type":"markdown","metadata":{"id":"Uy5_PTmOh1hj"},"source":["### 2.1. Install all the packages through FinRL library\n"]},{"cell_type":"code","metadata":{"id":"mPT0ipYE28wL","executionInfo":{"status":"aborted","timestamp":1651697237535,"user_tz":300,"elapsed":64,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# ## install finrl library\n","# !pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"osBHhVysOEzi"},"source":["\n","### 2.2. Check if the additional packages needed are present, if not install them. \n","* Yahoo Finance API\n","* pandas\n","* numpy\n","* matplotlib\n","* stockstats\n","* OpenAI gym\n","* stable-baselines\n","* tensorflow\n","* pyfolio"]},{"cell_type":"markdown","metadata":{"id":"nGv01K8Sh1hn"},"source":["### 2.3. Import Packages"]},{"cell_type":"code","metadata":{"id":"EeMK7Uentj1V","executionInfo":{"status":"ok","timestamp":1651701155343,"user_tz":300,"elapsed":328,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"lPqeTTwoh1hn","executionInfo":{"status":"ok","timestamp":1651701164637,"user_tz":300,"elapsed":13,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","# matplotlib.use('Agg')\n","import datetime\n","\n","%matplotlib inline\n","from finrl.config import config\n","from finrl.marketdata.yahoodownloader import YahooDownloader\n","from finrl.preprocessing.preprocessors import FeatureEngineer\n","from finrl.preprocessing.data import data_split\n","from finrl.env.env_stocktrading import StockTradingEnv\n","from finrl.model.models import DRLAgent,DRLEnsembleAgent\n","# from finrl.trade.backtest import BackTestStats, BaselineStats, BackTestPlot\n","\n","from pprint import pprint\n","\n","import sys\n","sys.path.append(\"../FinRL-Library\")\n","\n","import itertools"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T2owTj985RW4"},"source":["### 2.4. Create Folders"]},{"cell_type":"code","metadata":{"id":"w9A8CN5R5PuZ","executionInfo":{"status":"ok","timestamp":1651701165232,"user_tz":300,"elapsed":27,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["import os\n","if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n","    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n","if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n","    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n","if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n","    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n","if not os.path.exists(\"./\" + config.RESULTS_DIR):\n","    os.makedirs(\"./\" + config.RESULTS_DIR)"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A289rQWMh1hq"},"source":["### Part 3. Download Data\n","Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n","* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n","* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"]},{"cell_type":"markdown","metadata":{"id":"NPeQ7iS-LoMm"},"source":["\n","\n","-----\n","class YahooDownloader:\n","    Provides methods for retrieving daily stock data from\n","    Yahoo Finance API\n","\n","    Attributes\n","    ----------\n","        start_date : str\n","            start date of the data (modified from config.py)\n","        end_date : str\n","            end date of the data (modified from config.py)\n","        ticker_list : list\n","            a list of stock tickers (modified from config.py)\n","\n","    Methods\n","    -------\n","    fetch_data()\n","        Fetches data from yahoo API\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"h3XJnvrbLp-C","outputId":"dfcc45c4-323d-420c-9a5d-deea76de4d94","executionInfo":{"status":"ok","timestamp":1651701165242,"user_tz":300,"elapsed":36,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["# from config.py start_date is a string\n","config.START_DATE"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2000-01-01'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzqRRTOX6aFu","outputId":"6c9820e0-17e9-492d-eaf5-214a7567fc30","executionInfo":{"status":"ok","timestamp":1651701165244,"user_tz":300,"elapsed":35,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["print(config.SRI_KEHATI_TICKER)"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["['AALI.JK', 'ADHI.JK', 'ASII.JK', 'BBCA.JK', 'BBNI.JK', 'BBRI.JK', 'BBTN.JK', 'BMRI.JK', 'BSDE.JK', 'INDF.JK', 'JPFA.JK', 'JSMR.JK', 'KLBF.JK', 'PGAS.JK', 'PJAA.JK', 'PPRO.JK', 'SIDO.JK', 'SMGR.JK', 'TINS.JK', 'TLKM.JK', 'UNTR.JK', 'UNVR.JK', 'WIKA.JK', 'WSKT.JK', 'WTON.JK']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCKm4om-s9kE","outputId":"03118df0-a87c-47db-b633-9c3e8bd69f7c","executionInfo":{"status":"ok","timestamp":1651701177336,"user_tz":300,"elapsed":12119,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df = YahooDownloader(start_date = config.START_DATE,\n","                     end_date = '2021-01-19',\n","                     ticker_list = config.SRI_KEHATI_TICKER).fetch_data()"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","Shape of DataFrame:  (95418, 8)\n"]}]},{"cell_type":"code","metadata":{"id":"GiRuFOTOtj1Y","outputId":"eadc83e3-586e-4683-9466-203e72e1e78c","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1651701177337,"user_tz":300,"elapsed":63,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.head()"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date        open        high         low       close     volume  \\\n","0  2000-09-05  455.352478  455.352478  441.553925  249.921371  3712343.0   \n","1  2000-09-06  441.553925  464.551514  436.954376  252.471680  6793100.0   \n","2  2000-09-07  450.752960  459.951996  446.153442  252.471680  5011392.0   \n","3  2000-09-08  450.752960  455.352478  446.153442  249.921371  2277955.0   \n","4  2000-09-11  441.553925  441.553925  427.755341  239.720566   978362.0   \n","\n","       tic  day  \n","0  UNTR.JK    1  \n","1  UNTR.JK    2  \n","2  UNTR.JK    3  \n","3  UNTR.JK    4  \n","4  UNTR.JK    0  "],"text/html":["\n","  <div id=\"df-b25dce88-f1c1-4bd2-8355-350eda027d04\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-09-05</td>\n","      <td>455.352478</td>\n","      <td>455.352478</td>\n","      <td>441.553925</td>\n","      <td>249.921371</td>\n","      <td>3712343.0</td>\n","      <td>UNTR.JK</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-09-06</td>\n","      <td>441.553925</td>\n","      <td>464.551514</td>\n","      <td>436.954376</td>\n","      <td>252.471680</td>\n","      <td>6793100.0</td>\n","      <td>UNTR.JK</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-09-07</td>\n","      <td>450.752960</td>\n","      <td>459.951996</td>\n","      <td>446.153442</td>\n","      <td>252.471680</td>\n","      <td>5011392.0</td>\n","      <td>UNTR.JK</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-09-08</td>\n","      <td>450.752960</td>\n","      <td>455.352478</td>\n","      <td>446.153442</td>\n","      <td>249.921371</td>\n","      <td>2277955.0</td>\n","      <td>UNTR.JK</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-09-11</td>\n","      <td>441.553925</td>\n","      <td>441.553925</td>\n","      <td>427.755341</td>\n","      <td>239.720566</td>\n","      <td>978362.0</td>\n","      <td>UNTR.JK</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b25dce88-f1c1-4bd2-8355-350eda027d04')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b25dce88-f1c1-4bd2-8355-350eda027d04 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b25dce88-f1c1-4bd2-8355-350eda027d04');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"DSw4ZEzVtj1Z","outputId":"feaab791-0a86-466e-ecd9-29101632eb87","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1651701177338,"user_tz":300,"elapsed":59,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.tail()"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             date     open     high      low         close       volume  \\\n","95413  2021-01-18  26300.0  26850.0  25950.0  25012.246094    4282500.0   \n","95414  2021-01-18   6950.0   7525.0   6900.0   7261.655762   44943900.0   \n","95415  2021-01-18   2410.0   2450.0   2220.0   2250.000000  234257600.0   \n","95416  2021-01-18   1960.0   2080.0   1905.0   1920.000000  727533400.0   \n","95417  2021-01-18    430.0    525.0    430.0    441.445801  418011900.0   \n","\n","           tic  day  \n","95413  UNTR.JK    0  \n","95414  UNVR.JK    0  \n","95415  WIKA.JK    0  \n","95416  WSKT.JK    0  \n","95417  WTON.JK    0  "],"text/html":["\n","  <div id=\"df-db919c81-6578-4138-99df-92cfc62fa809\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>95413</th>\n","      <td>2021-01-18</td>\n","      <td>26300.0</td>\n","      <td>26850.0</td>\n","      <td>25950.0</td>\n","      <td>25012.246094</td>\n","      <td>4282500.0</td>\n","      <td>UNTR.JK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>95414</th>\n","      <td>2021-01-18</td>\n","      <td>6950.0</td>\n","      <td>7525.0</td>\n","      <td>6900.0</td>\n","      <td>7261.655762</td>\n","      <td>44943900.0</td>\n","      <td>UNVR.JK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>95415</th>\n","      <td>2021-01-18</td>\n","      <td>2410.0</td>\n","      <td>2450.0</td>\n","      <td>2220.0</td>\n","      <td>2250.000000</td>\n","      <td>234257600.0</td>\n","      <td>WIKA.JK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>95416</th>\n","      <td>2021-01-18</td>\n","      <td>1960.0</td>\n","      <td>2080.0</td>\n","      <td>1905.0</td>\n","      <td>1920.000000</td>\n","      <td>727533400.0</td>\n","      <td>WSKT.JK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>95417</th>\n","      <td>2021-01-18</td>\n","      <td>430.0</td>\n","      <td>525.0</td>\n","      <td>430.0</td>\n","      <td>441.445801</td>\n","      <td>418011900.0</td>\n","      <td>WTON.JK</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db919c81-6578-4138-99df-92cfc62fa809')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db919c81-6578-4138-99df-92cfc62fa809 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db919c81-6578-4138-99df-92cfc62fa809');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CV3HrZHLh1hy","outputId":"8ccccdc5-a807-47dd-e764-c7caa7659a1d","executionInfo":{"status":"ok","timestamp":1651701177339,"user_tz":300,"elapsed":58,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.shape"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(95418, 8)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"4hYkeaPiICHS","outputId":"56858f81-facc-48c9-f7ee-f3c68738d88a","executionInfo":{"status":"ok","timestamp":1651701177343,"user_tz":300,"elapsed":53,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df.sort_values(['date','tic']).head()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date        open        high         low       close     volume  \\\n","0  2000-09-05  455.352478  455.352478  441.553925  249.921371  3712343.0   \n","1  2000-09-06  441.553925  464.551514  436.954376  252.471680  6793100.0   \n","2  2000-09-07  450.752960  459.951996  446.153442  252.471680  5011392.0   \n","3  2000-09-08  450.752960  455.352478  446.153442  249.921371  2277955.0   \n","4  2000-09-11  441.553925  441.553925  427.755341  239.720566   978362.0   \n","\n","       tic  day  \n","0  UNTR.JK    1  \n","1  UNTR.JK    2  \n","2  UNTR.JK    3  \n","3  UNTR.JK    4  \n","4  UNTR.JK    0  "],"text/html":["\n","  <div id=\"df-67b2f231-ad3f-4fe2-9d07-186132ace164\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>tic</th>\n","      <th>day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-09-05</td>\n","      <td>455.352478</td>\n","      <td>455.352478</td>\n","      <td>441.553925</td>\n","      <td>249.921371</td>\n","      <td>3712343.0</td>\n","      <td>UNTR.JK</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-09-06</td>\n","      <td>441.553925</td>\n","      <td>464.551514</td>\n","      <td>436.954376</td>\n","      <td>252.471680</td>\n","      <td>6793100.0</td>\n","      <td>UNTR.JK</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-09-07</td>\n","      <td>450.752960</td>\n","      <td>459.951996</td>\n","      <td>446.153442</td>\n","      <td>252.471680</td>\n","      <td>5011392.0</td>\n","      <td>UNTR.JK</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-09-08</td>\n","      <td>450.752960</td>\n","      <td>455.352478</td>\n","      <td>446.153442</td>\n","      <td>249.921371</td>\n","      <td>2277955.0</td>\n","      <td>UNTR.JK</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-09-11</td>\n","      <td>441.553925</td>\n","      <td>441.553925</td>\n","      <td>427.755341</td>\n","      <td>239.720566</td>\n","      <td>978362.0</td>\n","      <td>UNTR.JK</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67b2f231-ad3f-4fe2-9d07-186132ace164')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-67b2f231-ad3f-4fe2-9d07-186132ace164 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-67b2f231-ad3f-4fe2-9d07-186132ace164');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"uqC6c40Zh1iH"},"source":["### Part 4: Preprocess Data\n","Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n","* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n","* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"jgXfBcjxtj1a","outputId":"8eae9754-5e7b-4f03-d764-bc8a521867b5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651701240387,"user_tz":300,"elapsed":63095,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["fe = FeatureEngineer(\n","                    use_technical_indicator=True,\n","                    tech_indicator_list = config.TECHNICAL_INDICATORS_LIST,\n","                    use_turbulence=True,\n","                    user_defined_feature = False)\n","\n","processed = fe.preprocess_data(df)"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully added technical indicators\n","Successfully added turbulence index\n"]}]},{"cell_type":"code","metadata":{"id":"R_3v-ycktj1b","executionInfo":{"status":"ok","timestamp":1651701240913,"user_tz":300,"elapsed":568,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["list_ticker = processed[\"tic\"].unique().tolist()\n","list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n","combination = list(itertools.product(list_date,list_ticker))\n","\n","processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n","processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n","processed_full = processed_full.sort_values(['date','tic'])\n","\n","processed_full = processed_full.fillna(0)"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"grvhGJJII3Xn","outputId":"5a5e29ab-6581-454e-8c99-f8fd3fda8e92","executionInfo":{"status":"ok","timestamp":1651701240922,"user_tz":300,"elapsed":209,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["processed_full.sample(5)"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              date      tic          open          high           low  \\\n","2652    2000-12-20  ASII.JK    156.493439    160.357468    156.493439   \n","135951  2015-07-27  INDF.JK   6075.000000   6100.000000   5975.000000   \n","115750  2013-05-10  UNTR.JK  17250.000000  17450.000000  17050.000000   \n","178063  2020-03-06  BBCA.JK   6350.000000   6350.000000   6200.000000   \n","112416  2012-12-27  SMGR.JK  15700.000000  15950.000000  15700.000000   \n","\n","               close      volume  day       macd      boll_ub      boll_lb  \\\n","2652       86.115128  74494658.0  2.0  -6.388314   169.018972   128.336480   \n","135951   4875.154297   4689900.0  0.0  17.230692   586.181160   514.530101   \n","115750  11713.349609   4754500.0  4.0  81.060380  1832.036405  1559.841122   \n","178063   5882.049316  77835500.0  4.0 -34.509436  1669.971531  1455.455337   \n","112416  12531.427734   3249500.0  3.0  -8.837877  1374.157602  1324.075100   \n","\n","           rsi_30      cci_30      dx_30  close_30_sma  close_60_sma  \\\n","2652    36.258109  -57.858829  26.656233    157.008448    182.510641   \n","135951  60.087186   76.143222  24.911368    532.723528    496.875892   \n","115750  67.728202   98.386789  34.746956   1614.393433   1433.158978   \n","178063  45.549677   -9.855733  12.291197   1614.215055   1709.007890   \n","112416  44.504408 -109.199855   8.417592   1358.402783   1372.789132   \n","\n","        turbulence  \n","2652      0.000000  \n","135951   51.788713  \n","115750    9.360066  \n","178063   54.842110  \n","112416   21.774045  "],"text/html":["\n","  <div id=\"df-9ccf4594-5644-4c52-8cc6-dec196ae569e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>tic</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>boll_ub</th>\n","      <th>boll_lb</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>close_30_sma</th>\n","      <th>close_60_sma</th>\n","      <th>turbulence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2652</th>\n","      <td>2000-12-20</td>\n","      <td>ASII.JK</td>\n","      <td>156.493439</td>\n","      <td>160.357468</td>\n","      <td>156.493439</td>\n","      <td>86.115128</td>\n","      <td>74494658.0</td>\n","      <td>2.0</td>\n","      <td>-6.388314</td>\n","      <td>169.018972</td>\n","      <td>128.336480</td>\n","      <td>36.258109</td>\n","      <td>-57.858829</td>\n","      <td>26.656233</td>\n","      <td>157.008448</td>\n","      <td>182.510641</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>135951</th>\n","      <td>2015-07-27</td>\n","      <td>INDF.JK</td>\n","      <td>6075.000000</td>\n","      <td>6100.000000</td>\n","      <td>5975.000000</td>\n","      <td>4875.154297</td>\n","      <td>4689900.0</td>\n","      <td>0.0</td>\n","      <td>17.230692</td>\n","      <td>586.181160</td>\n","      <td>514.530101</td>\n","      <td>60.087186</td>\n","      <td>76.143222</td>\n","      <td>24.911368</td>\n","      <td>532.723528</td>\n","      <td>496.875892</td>\n","      <td>51.788713</td>\n","    </tr>\n","    <tr>\n","      <th>115750</th>\n","      <td>2013-05-10</td>\n","      <td>UNTR.JK</td>\n","      <td>17250.000000</td>\n","      <td>17450.000000</td>\n","      <td>17050.000000</td>\n","      <td>11713.349609</td>\n","      <td>4754500.0</td>\n","      <td>4.0</td>\n","      <td>81.060380</td>\n","      <td>1832.036405</td>\n","      <td>1559.841122</td>\n","      <td>67.728202</td>\n","      <td>98.386789</td>\n","      <td>34.746956</td>\n","      <td>1614.393433</td>\n","      <td>1433.158978</td>\n","      <td>9.360066</td>\n","    </tr>\n","    <tr>\n","      <th>178063</th>\n","      <td>2020-03-06</td>\n","      <td>BBCA.JK</td>\n","      <td>6350.000000</td>\n","      <td>6350.000000</td>\n","      <td>6200.000000</td>\n","      <td>5882.049316</td>\n","      <td>77835500.0</td>\n","      <td>4.0</td>\n","      <td>-34.509436</td>\n","      <td>1669.971531</td>\n","      <td>1455.455337</td>\n","      <td>45.549677</td>\n","      <td>-9.855733</td>\n","      <td>12.291197</td>\n","      <td>1614.215055</td>\n","      <td>1709.007890</td>\n","      <td>54.842110</td>\n","    </tr>\n","    <tr>\n","      <th>112416</th>\n","      <td>2012-12-27</td>\n","      <td>SMGR.JK</td>\n","      <td>15700.000000</td>\n","      <td>15950.000000</td>\n","      <td>15700.000000</td>\n","      <td>12531.427734</td>\n","      <td>3249500.0</td>\n","      <td>3.0</td>\n","      <td>-8.837877</td>\n","      <td>1374.157602</td>\n","      <td>1324.075100</td>\n","      <td>44.504408</td>\n","      <td>-109.199855</td>\n","      <td>8.417592</td>\n","      <td>1358.402783</td>\n","      <td>1372.789132</td>\n","      <td>21.774045</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ccf4594-5644-4c52-8cc6-dec196ae569e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9ccf4594-5644-4c52-8cc6-dec196ae569e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9ccf4594-5644-4c52-8cc6-dec196ae569e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"-QsYaY0Dh1iw"},"source":["### 5. Design Environment\n","Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n","\n","Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n","\n","The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], because the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYN573SOHhxG","outputId":"ce638e91-00b7-4038-e7d2-e968500f016b","executionInfo":{"status":"ok","timestamp":1651701240923,"user_tz":300,"elapsed":208,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["config.TECHNICAL_INDICATORS_LIST"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['macd',\n"," 'boll_ub',\n"," 'boll_lb',\n"," 'rsi_30',\n"," 'cci_30',\n"," 'dx_30',\n"," 'close_30_sma',\n"," 'close_60_sma']"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2zqII8rMIqn","outputId":"0969afd7-0e23-40ea-9d3a-0c0fa30d5621","executionInfo":{"status":"ok","timestamp":1651701240934,"user_tz":300,"elapsed":189,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["stock_dimension = len(processed_full.tic.unique())\n","state_space = 1 + 2*stock_dimension + len(config.TECHNICAL_INDICATORS_LIST)*stock_dimension\n","print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Stock Dimension: 25, State Space: 251\n"]}]},{"cell_type":"code","metadata":{"id":"AWyp84Ltto19","executionInfo":{"status":"ok","timestamp":1651701240935,"user_tz":300,"elapsed":132,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["env_kwargs = {\n","    \"hmax\": 100, \n","    \"initial_amount\": 50_000_000/100, #Since in Indonesia the minimum number of shares per trx is 100, then we scaled the initial amount by dividing it with 100 \n","    \"buy_cost_pct\": 0.0019, #IPOT has 0.19% buy cost\n","    \"sell_cost_pct\": 0.0029, #IPOT has 0.29% sell cost\n","    \"state_space\": state_space, \n","    \"stock_dim\": stock_dimension, \n","    \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n","    \"action_space\": stock_dimension, \n","    \"reward_scaling\": 1e-4,\n","    \"print_verbosity\":5\n","    \n","}"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMNR5nHjh1iz"},"source":["### 6: Implement DRL Algorithms\n","* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n","* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n","Multi-Agent DDPG, PPO, SAC, A2C and TD3 (we've seen all of these but Multi-Agent DDPG). We also allow users to\n","design their own DRL algorithms by adapting these DRL algorithms.\n","\n","* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"]},{"cell_type":"code","metadata":{"id":"v-gthCxMtj1d","executionInfo":{"status":"ok","timestamp":1651701240936,"user_tz":300,"elapsed":132,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n","validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n","train_start = '2000-01-01'\n","train_end = '2019-01-01'\n","val_test_start = '2019-01-01'\n","val_test_end = '2021-01-18'\n","\n","ensemble_agent = DRLEnsembleAgent(df=processed_full,\n","                 train_period=(train_start,train_end),\n","                 val_test_period=(val_test_start,val_test_end),\n","                 rebalance_window=rebalance_window, \n","                 validation_window=validation_window, \n","                 **env_kwargs)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"KsfEHa_Etj1d","executionInfo":{"status":"ok","timestamp":1651701240937,"user_tz":300,"elapsed":131,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["A2C_model_kwargs = {\n","                    'n_steps': 5,\n","                    'ent_coef': 0.01,\n","                    'learning_rate': 0.0005\n","                    }\n","\n","PPO_model_kwargs = {\n","                    \"ent_coef\":0.01,\n","                    \"n_steps\": 2048,\n","                    \"learning_rate\": 0.00025,\n","                    \"batch_size\": 128\n","                    }\n","\n","DDPG_model_kwargs = {\n","                      \"action_noise\":\"ornstein_uhlenbeck\",\n","                      \"buffer_size\": 50_000,\n","                      \"learning_rate\": 0.000005,\n","                      \"batch_size\": 128\n","                    }\n","\n","timesteps_dict = {'a2c' : 100_000, \n","                 'ppo' : 100_000, \n","                 'ddpg' : 50_000\n","                 }"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"_1lyCECstj1e"},"source":["df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n","                                                 PPO_model_kwargs,\n","                                                 DDPG_model_kwargs,\n","                                                 timesteps_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0qd8acMtj1f","executionInfo":{"status":"aborted","timestamp":1651701262657,"user_tz":300,"elapsed":21,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":["df_summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pymWC-kKEklP"},"source":["# Advanced Reinforcement Learning Material\n","\n","This section contains links to advanced and nonstandard RL material that go beyond those described above. "]},{"cell_type":"markdown","metadata":{"id":"8pl62NJR0mNg"},"source":["## Multi-Agent Reinforcement Learning (MARL)\n","\n","Many problems involve more than one player/agent (e.g., a basketball team) and it is likely that you can improve performance if you systematically model the problem with multiple agents in mind.\n","\n","MARL addresses the sequential decision-making problem of multiple autonomous agents operating in a common environment. Each attempts to optimize retruns by interacting with *both* the environment and other agents.\n","\n","MARL approaches tend to be cooperative, competitive, or a mixture of both. In cooperative MARL, agents collaborate to optimize a long-term shared goal. In competitive MARL, the reward sums to zerio, common in domains like price bidding and trading. Mixed MARL might include a social media platform seeking to use ads to both maximize global revenue while also maintaining user-engagement.\n","\n","MARL faces several challenges:\n","\n","1) there are more things to consider (and more potential solutions to any problem);\n","\n","2) the goal of one agent may be different enough from another that it can be challenging to share policies\n","\n","3) sharing information between agents imposes communication costs, which decrease model efficiency\n","\n","4) the environment is nonstationary from the perspective of one agent if acted upon by another agent, which breaks some of the basic assumptions of MDP and can lead to a lack of stability.\n","\n","5) its is hard to define optimality for real-world examples and so there is no convergence guarantees for nontrivial environments.\n","\n","With those caveats, here are a few repositories with working code:\n","\n","https://github.com/koulanurag/marl-pytorch\n","\n","\n","Multi-agent DDPG:\n","\n","https://github.com/starry-sky6688/MADDPG\n","\n","Multi-agent PPO:\n","\n","https://pythonrepo.com/repo/marlbenchmark-on-policy"]},{"cell_type":"markdown","metadata":{"id":"MIEALBSwkgZM"},"source":["## Inverse Reinforcement Learning\n","\n","This approach takes an agent's actions and environment to infer the rewards to which they are responding. The following paper tries to infer the rewards to physicians ordering tests, but there are many institutions and individuals whose actions and environment can be inverted to reveal the rewards to which they are responding. \n","\n","[LEARNING \"WHAT-IF\" EXPLANATIONS\n","FOR SEQUENTIAL DECISION-MAKING](https://arxiv.org/pdf/2007.13531.pdf)"]},{"cell_type":"code","metadata":{"id":"Nc0zFNjxlFN_","executionInfo":{"status":"aborted","timestamp":1651701262666,"user_tz":300,"elapsed":26,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1uaFXeBqtm3W"},"source":["## Reinforcement Learning for Text\n","\n","[Microsoft TextWorld](https://github.com/microsoft/TextWorld)"]},{"cell_type":"code","metadata":{"id":"kLJjajEcto2S","executionInfo":{"status":"aborted","timestamp":1651701262667,"user_tz":300,"elapsed":27,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AC5E1jrtpQB"},"source":["## Distributional Reinforcement Learning\n","\n","[A Distributional Perspective on Reinforcement Learning](http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf)\n","\n","[A distributional code for value in dopamine-based reinforcement learning\n","](https://www.nature.com/articles/s41586-019-1924-6)"]},{"cell_type":"code","metadata":{"id":"95ZZLP4CMPcB","executionInfo":{"status":"aborted","timestamp":1651701262669,"user_tz":300,"elapsed":29,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47yiu44MtwlY"},"source":["## Reinforcement Learning for Neuroscience and Social Robots\n","\n","[Deep Reinforcement Learning and\n","its Neuroscientific Implications](https://arxiv.org/pdf/2007.03750.pdf)\n","\n","[Social robot learning with deep\n","reinforcement learning and realistic\n","reward shaping](https://uu.diva-portal.org/smash/get/diva2:1365641/FULLTEXT01.pdf)"]},{"cell_type":"code","metadata":{"id":"-ffON9ZOtyxg","executionInfo":{"status":"aborted","timestamp":1651701262672,"user_tz":300,"elapsed":32,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7W-GJd5YFXXp"},"source":["# Homework\n","\n","In the last homework, we brainstormed how we would include RL in a social scientific context, ideally related to our final project. In this homework, you can either:\n","\n","**a)** Use any Deep RL based method on any social scientific context or dataset, ideally related to your final project.\n","\n","**b)** Use a Deep RL method on one of the environments shared with the OpenAI gym beyond those directly implemented here."]},{"cell_type":"code","metadata":{"id":"sxigInhSFsbL","executionInfo":{"status":"aborted","timestamp":1651701262673,"user_tz":300,"elapsed":33,"user":{"displayName":"Likun Cao","userId":"01917344510473464948"}}},"source":[""],"execution_count":null,"outputs":[]}]}